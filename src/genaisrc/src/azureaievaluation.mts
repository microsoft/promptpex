import type { PromptPexContext } from "./types.mts"
import type {
    DatasetVersion,
    Evaluation,
    EvaluationWithOptionalName,
} from "@azure/ai-projects"
const { generator } = env
const dbg = host.logger("promptpex:eval:azure")

/**
 * Creates an evaluation run using Azure AI Evaluations.
 *
 * @see https://github.com/Azure/azure-sdk-for-js/tree/main/sdk/ai/ai-projects#evaluation
 */
export async function azureAIEvaluate(files: PromptPexContext) {
    const { name } = files
    const { AIProjectClient, EvaluatorIds } = await import("@azure/ai-projects")
    const { DefaultAzureCredential } = await import("@azure/identity")

    const endpoint = process.env["AZURE_AI_PROJECT_ENDPOINT_STRING"]
    if (!endpoint)
        throw new Error(
            "AZURE_AI_PROJECT_ENDPOINT_STRING environment variable is not set"
        )
    const project = new AIProjectClient(endpoint, new DefaultAzureCredential())
    dbg(`project: %s`, project.getEndpointUrl())

    const dataset: DatasetVersion = await project.datasets.uploadFile(
        name,
        "1",
        "./samples_folder/sample_data_evaluation.jsonl"
    )
    dbg(`dataset: %s`, dataset.id)

    const newEvaluation: EvaluationWithOptionalName = {
        displayName: name,
        description: "Evaluation generated by PromptPex",
        data: {
            type: "dataset",
            id: dataset.id,
        },
        evaluators: {
            relevance: {
                id: EvaluatorIds.BLUE_SCORE,
                initParams: {
                    deploymentName: "gpt-4o-mini", // TODO: make it configurable
                },
                dataMapping: {
                    query: "${data.query}", // TODO: figure out mapping
                    response: "${data.response}",
                },
            },
        },
    }
    const evalResp = await project.evaluations.create(newEvaluation)
    dbg("eval: %O", evalResp)
}
