import type { PromptPexContext } from "./types.mts"
import type {
    DatasetVersion,
    Evaluation,
    EvaluationWithOptionalName,
} from "@azure/ai-projects"
const { generator } = env
const dbg = host.logger("promptpex:eval:azure")

/**
 * Type of an entry in the dataset JSONL file
 * https://github.com/Azure/azure-sdk-for-js/blob/9ee6f429a4bd699c946b5dabc94be4b1c213c4d1/sdk/ai/ai-projects/samples-dev/evaluations/samples_folder/sample_data_evaluation.jsonl
 * https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/evaluate-sdk#conversation-support-for-text
s */

/**
 * Creates an evaluation run using Azure AI Evaluations.
 *
 * @see https://github.com/Azure/azure-sdk-for-js/tree/main/sdk/ai/ai-projects#evaluation
 */
export async function azureAIProjectEvaluationCreate(files: PromptPexContext) {
    const { name, dir, promptPexTests } = files
    const { AIProjectClient, EvaluatorIds } = await import("@azure/ai-projects")
    const { DefaultAzureCredential } = await import("@azure/identity")

    const endpoint = process.env["AZURE_AI_PROJECT_ENDPOINT_STRING"]
    if (!endpoint)
        throw new Error(
            "AZURE_AI_PROJECT_ENDPOINT_STRING environment variable is not set"
        )
    const deploymentName = process.env["AZURE_AI_PROJECT_DEPLOYMENT_NAME"]
    if (!deploymentName)
        throw new Error(
            "AZURE_AI_PROJECT_DEPLOYMENT_NAME environment variable is not set"
        )

    const project = new AIProjectClient(endpoint, new DefaultAzureCredential())
    dbg(`project: %s`, project.getEndpointUrl())
    const version = await findNextVersion()

    // convert tests to DatasetRow[]
    const data = promptPexTests.map((test) => {
        return {
            query: "What is the capital of france?",
            response: "paris",
        }
    })
    const datasetFile = path.join(dir, `azure-ai-eval-dataset.jsonl`)
    await workspace.writeText(datasetFile, JSONL.stringify(data))
    dbg(`dataset file: %s`, datasetFile)
    const dataset: DatasetVersion = await project.datasets.uploadFile(
        name,
        version.toString(),
        datasetFile
    )
    dbg(`dataset: %s`, dataset.id)

    const newEvaluation: EvaluationWithOptionalName = {
        displayName: name,
        description: "Evaluation generated by PromptPex",
        data: {
            type: "dataset",
            id: dataset.id,
        },
        evaluators: {
            relevance: {
                id: EvaluatorIds.RELEVANCE,
                initParams: {
                    deploymentName: "gpt-4o-mini", // TODO: make it configurable
                },
                dataMapping: {
                    query: "${data.query}", // TODO: figure out mapping
                    response: "${data.response}",
                },
            },
        },
    }
    const evalResp = await project.evaluations.create(newEvaluation)
    dbg("eval: %O", evalResp)

    async function findNextVersion() {
        let nextVersion = 0
        const versions = await project.datasets.listVersions(name)
        for await (const version of versions) {
            dbg("version: %s", version.version)
            const v = parseFloat(version.version) || 0
            if (v > nextVersion) {
                nextVersion = v
            }
        }
        nextVersion = nextVersion + 1
        dbg("next version: %s", nextVersion)
        return nextVersion
    }
}
