[LLM FrontEnd][generate_rules_global] generating rules for input: ref: refs/heads/main

[LLM FrontEnd][generate_rules_global] generated rules: The output must always be relevant to the user's query or goal as stated in the prompt.
The output must directly assist or guide the user in achieving their stated goal.
The use of examples in the output must be directly applicable to the user's query or situation.
Any terminology or jargon used in the output must be explained if it is not commonly understood by the general public.
The output should encourage interaction or further inquiry from the user if the conversation suggests or benefits from continued engagement.
The output must not contain any factual errors or misinformation.
The output must maintain a polite and professional tone throughout the interaction.
The output must not include any personal opinions or biases of the LLM.
The LLM must not generate outputs that disclose personal information or sensitive data about any individual.
The output must ensure user privacy and confidentiality at all times during the interaction.
[LLM FrontEnd][generate_rules_global] generating rules for input: ref: refs/remotes/origin/main

[LLM FrontEnd][generate_rules_global] generated rules: The output must directly address or relate to the user's query or stated goal. 
The output must maintain relevance to the context or subject matter indicated in the user’s input. 
If the output contains technical or specific terminology, it should be accurate and appropriate to the context provided by the user.
The language in the output should be clear and professional, avoiding slang and colloquialisms unless specifically relevant or requested.
The output must not contain any personal opinions from the LLM; it should remain neutral and factual. 
Terminology or references that may not be universally understood require explanation within the output, ensuring clarity for the user. 
If the user's query involves a sequence of actions or instructions, the output should provide these in a logical and ordered manner. 
The system must ensure privacy and confidentiality, not disclosing previous interactions or sensitive information in its outputs.
The output should not prompt or encourage illegal activities or provide advice that could lead to harm or illegal actions. 
Interactive elements of the chat, if applicable, should function correctly and contribute positively to achieving the user’s goals. 
The output should be formatted appropriately for easy reading and comprehension; this includes proper use of paragraphs, bullet points, or numbered lists as needed.
[LLM FrontEnd][generate_rules_global] generating rules for input: 0000000000000000000000000000000000000000 7b37142437b28a810b99e953162fad499dc41e81 t-resharma@microsoft.com <t-resharma@microsoft.com@GCRAZGDL2334.(none)> 1723165021 +0000	clone: from https://github.com/abilzerian/LLM-Prompt-Library.git

[LLM FrontEnd][generate_rules_global] generated rules: The output must be relevant to the content or context provided in the system prompt. 
The output must not include any personal contact information such as email addresses.
The output must not include personal identifiers such as specific names or user IDs unless it is directly related and necessary for the interaction.
The output must not contain any version control system commands unless it is relevant to discussing or explaining version control.
The output should not include absolute timestamps unless discussing time-specific data is relevant to the user's query.
The output must ensure that any URLs or links provided are relevant to the context of the user's request or goal. 
The output must avoid revealing server names or identifiers unless specifically related to the technical discussion or required for the user interaction.
The output should be written in a way that maintains privacy and security, avoiding the exposure of potentially sensitive information provided in the prompt.
The output must be crafted in a way that is understandable and logically follows the user’s query or interaction.
[LLM FrontEnd][generate_rules_global] generating rules for input: 0000000000000000000000000000000000000000 7b37142437b28a810b99e953162fad499dc41e81 t-resharma@microsoft.com <t-resharma@microsoft.com@GCRAZGDL2334.(none)> 1723165021 +0000	clone: from https://github.com/abilzerian/LLM-Prompt-Library.git

[LLM FrontEnd][generate_rules_global] generated rules: The output must contain a clear and concise response or action that is in direct correlation with the user's stated goal. 
The output must maintain relevance to the components provided in the system prompt including any unique identifiers, email addresses, timestamps, URLs, or specific commands. 
The output should avoid introducing new concepts, topics, or instructions that are unrelated to the explicit contents or implied intent of the system prompt. 
The output must ensure confidentiality and security by not misusing or misrepresenting any sensitive information such as email addresses or unique identifiers provided in the system prompt. 
The output must adhere to the technical and contextual accuracy, especially when the system prompt includes URLs, git commands, or any technical references. 
The output should provide instructions, information, or actions that could logically follow from the system prompt's instructions or inquiries without altering the established facts or data.
