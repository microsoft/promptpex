[
  {
    "input": "We present FoobarNet, a lightweight convolutional architecture for mobile vision tasks. Code and preprint available."
  },
  {
    "input": "We introduce AlphaNet and BetaNet for multi-task learning; both models outperform baselines on four benchmarks."
  },
  {
    "input": ""
  },
  {
    "input": "We propose mBERT-Adapt, adapting multilingual BERT for cross-lingual transfer."
  },
  {
    "input": "We introduce GraphFormer, a Transformer-based graph learning model achieving state-of-the-art on OGB benchmarks. The architecture scales efficiently."
  },
  {
    "input": "We compare GraphSAGE and GIN for node classification across citation networks, detailing inductive biases and training setups."
  },
  {
    "input": "We propose ResNet-50-TR and compare against BERT and GPT-2; models are described in detail with comprehensive ablations."
  },
  {
    "input": "We release a variant called DeepFusionNet that excels on multimodal tasks such as VQA and captioning."
  },
  {
    "input": "We present SequenceLab v1, a simple sequence labeling toolkit with CRFs and transformers backends."
  },
  {
    "input": "This survey synthesizes trends in self-supervised learning for audio and vision; no new model is introduced or named."
  },
  {
    "input": "We conduct controlled ablations on architecture depth and width without proposing a new model; baselines are standard."
  },
  {
    "input": ""
  },
  {
    "input": "No specific model is proposed; methods are generic and naming is intentionally omitted throughout."
  },
  {
    "input": "We evaluate BERT against XLNet on GLUE and SQuAD; our contribution does not introduce a new model."
  },
  {
    "input": "We introduce LiteViT, a lightweight Vision Transformer optimized for mobile hardware with distillation and quantization-aware training."
  },
  {
    "input": "We fine-tune T5 and BART for summarization, detailing datasets, metrics, and training schedules."
  },
  {
    "input": "We present RoBERTa-XS, a compact pre-trained encoder tailored for latency-critical NLU on-device applications."
  },
  {
    "input": "We introduce three models—BERT, GPT-2, and RoBERTa—and show complementary strengths across GLUE, SuperGLUE, and WMT tasks."
  }
]