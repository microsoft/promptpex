[
  {
    "id": "a120e4c",
    "promptid": "4b51e23",
    "ruleid": 1,
    "rule": "The output focuses on architectural concepts and practices at the system and module-integration level and explicitly avoids shifting into implementation details, ensuring the discussion stays about structures, responsibilities, and interactions rather than how to write code.",
    "inverse": false,
    "testuid": "test-48967654",
    "scenario": "",
    "baseline": false,
    "testinput": "Project Context: GreenCart is revamping its marketplace platform to improve reliability and scalability. Modules: Web-Frontend, API-Gateway, Catalog-Service, Order-Service, Payment-Adapter, Notification-Service. Inter-Module Communication: HTTP/REST between frontend and gateway; internal services via gRPC; async events via Kafka for order lifecycle. Non-Functional Requirements: PCI scope minimization, 99.9% uptime, latency under 300ms p95. Request: Provide an architecture-level review focusing on structures, responsibilities, interfaces, and interactions. Avoid any implementation details or how-to code.",
    "model": "gpt-5-2025-08-07",
    "input": "Project Context: GreenCart is revamping its marketplace platform to improve reliability and scalability. Modules: Web-Frontend, API-Gateway, Catalog-Service, Order-Service, Payment-Adapter, Notification-Service. Inter-Module Communication: HTTP/REST between frontend and gateway; internal services via gRPC; async events via Kafka for order lifecycle. Non-Functional Requirements: PCI scope minimization, 99.9% uptime, latency under 300ms p95. Request: Provide an architecture-level review focusing on structures, responsibilities, interfaces, and interactions. Avoid any implementation details or how-to code.",
    "output": "Architecture overview\n- Style: A microservices architecture with an event-driven backbone for order lifecycle. Synchronous paths use HTTP/REST from Web-Frontend to API-Gateway, and gRPC for inter-service calls. Asynchronous workflows use Kafka.\n- Goals alignment: Minimize PCI scope by isolating payment handling, meet 99.9% uptime through fault isolation and redundancy, and keep p95 latency under 300ms by controlling synchronous call chains and using caches.\n\nModule responsibilities and boundaries\n- Web-Frontend\n  - Responsibility: User experience, rendering, and collecting input. Should never handle raw card data if possible (use hosted payment fields or redirects).\n  - Interface: REST calls to API-Gateway. Receives web-safe error codes and status.\n- API-Gateway (edge/BFF)\n  - Responsibility: Entry point from frontend. Authentication and authorization, request validation, rate limiting, orchestration of backend calls for pages that need data from multiple services. Consistent error mapping. No business rules beyond orchestration.\n  - Interface: REST to frontend; gRPC to internal services. Publishes correlation IDs for traceability. Accepts idempotency keys for mutating operations (e.g., order creation).\n- Catalog-Service\n  - Responsibility: Product metadata, pricing, availability signals, and category navigation. Owns its data. Provides fast reads for browsing and referencing items in orders.\n  - Interface: gRPC for product reads and item references; publishes events if availability or price changes (optional).\n- Order-Service\n  - Responsibility: Order state machine, cart/checkout workflows, and persistence of orders. Coordinates with Payment-Adapter and Catalog via a saga (orchestrated or choreographed) using Kafka for lifecycle transitions.\n  - Interface: gRPC for order creation, status retrieval; Kafka for order lifecycle events (e.g., order.created, order.awaiting_payment, order.paid, order.cancelled).\n- Payment-Adapter\n  - Responsibility: Isolated payment handling and external payment service provider (PSP) integrations. Tokenizes card data and processes authorization/capture/refund. The only module in PCI scope; keeps the rest out.\n  - Interface: gRPC for payment requests from Order-Service; Kafka to emit payment events (e.g., payment.authorized, payment.failed). External PSP interfaces are opaque to other services.\n- Notification-Service\n  - Responsibility: Outbound communications (email, SMS, push) triggered by lifecycle events (order confirmation, shipment, payment success/failure).\n  - Interface: Kafka subscriptions to lifecycle topics; gRPC inbound only if you need direct triggering from the gateway for non-order messages (optional).\n\nInter-module communication patterns\n- Synchronous path (user request/response)\n  - Web-Frontend → API-Gateway (REST).\n  - API-Gateway → Catalog-Service (gRPC) for browsing.\n  - API-Gateway → Order-Service (gRPC) for cart/checkout steps.\n  - Order-Service → Payment-Adapter (gRPC) only for actions that must be user-synchronous (e.g., payment authorization).\n- Asynchronous path (background lifecycle)\n  - Services publish and subscribe via Kafka. Think of Kafka as the postal system that reliably delivers “postcards” about state changes.\n  - Order-Service publishes order events; Payment-Adapter publishes payment events; Notification-Service consumes relevant events to send messages.\n\nCore interfaces and contracts (high-level)\n- API-Gateway REST\n  - Read endpoints: list products, product detail, order status.\n  - Write endpoints: create/update cart, initiate checkout, confirm order, initiate payment (if not fully delegated to front-end/PSP).\n  - Contract essentials: standardized error model, idempotency for writes, pagination and filters for lists, correlation ID headers.\n- gRPC between services\n  - Catalog: get product(s), price checks, availability signal retrieval.\n  - Order: create order, update order state, get order by ID.\n  - Payment: authorize, capture, refund by tokenized instrument. Only accepts tokens; rejects raw PAN data.\n  - Contract essentials: versioned service definitions, clear timeouts, retry semantics, and idempotency expectations for operations.\n- Kafka topics (example set; adapt to your domain)\n  - order.created, order.awaiting_payment, order.paid, order.cancelled, order.fulfillment_requested.\n  - payment.authorized, payment.captured, payment.failed, payment.refunded.\n  - notification.send (command) and notification.sent (event) if you want feedback loops.\n  - Contract essentials: schema versioning, stable keys (e.g., orderId), clear delivery semantics (at-least-once with consumer idempotency), and backward-compatible changes.\n\nInteraction flows (sequence, responsibilities, and sync/async boundaries)\n- Browsing\n  - Frontend → Gateway → Catalog (sync, read-heavy).\n  - Performance notes: cache at CDN/gateway for product lists/details; Catalog optimized for fast reads.\n- Cart and order creation\n  - Frontend → Gateway → Order (sync): create cart/order, add items referencing Catalog product IDs.\n  - Order-Service persists and emits order.created (async) for downstream interests (e.g., analytics).\n- Payment authorization (PCI minimization)\n  - Preferred flow: Frontend uses PSP-hosted fields or redirect to collect card info; PSP returns a payment token to the frontend.\n  - Frontend sends token to Gateway → Order → Payment-Adapter (sync). Payment-Adapter authorizes with PSP and emits payment.authorized or payment.failed (async) and returns status to Order.\n  - Only Payment-Adapter handles sensitive payment data; other modules only see tokens and payment status.\n- Order confirmation and notifications\n  - Upon payment.authorized, Order-Service transitions to order.paid and emits the event (async).\n  - Notification-Service consumes order.paid and sends confirmation messages (async).\n  - Gateway can poll or subscribe to status updates to present feedback to the user.\n\nNon-functional requirements alignment\n- PCI scope minimization\n  - Keep card collection out of your domain by using PSP-hosted components. Ensure the Gateway, Order, and Catalog never touch raw card data—only opaque tokens.\n  - Network segmentation: isolate Payment-Adapter and its datastore; strict access controls and audit trails.\n  - Event logs and traces must redact payment details; tokens only.\n- 99.9% uptime\n  - Redundancy: run each service across multiple zones; stateless services scale horizontally; isolated databases per service with HA.\n  - Failure isolation: use circuit breakers and bulkheads so a Payment-Adapter or Catalog outage doesn’t cascade to Order or Gateway.\n  - Degraded modes: browsing and cart should operate even if payments are down; present “try again later” for payment-specific functions while keeping the site usable.\n  - Backpressure and queues: Kafka buffers spikes; consumers scale elastically.\n- Latency under 300ms p95\n  - Keep synchronous call chains shallow: Gateway should make at most one call each to Catalog and Order per page/action; Order makes one call to Payment for authorization.\n  - Cache read-mostly data (product lists, details, prices) close to the edge; stale-while-revalidate patterns help keep p95 low.\n  - Establish time budgets per hop and enforce timeouts; prefer “return fast” with clear user messaging over long waits.\n\nData ownership and consistency\n- Each service owns its datastore and exposes data only via its interfaces; no cross-service DB access.\n- Use the outbox pattern in Order and Payment to emit Kafka events reliably after state changes.\n- Embrace eventual consistency for lifecycle transitions. Idempotency keys ensure safe retries in both sync calls and event consumers.\n- If inventory reservation is part of Catalog, treat it as a separate bounded context and decide whether reservation is synchronous (for strong user feedback) or async via saga, depending on latency and user experience goals.\n\nOperational and cross-cutting concerns\n- Security: mTLS for gRPC, strong authentication at the Gateway, least-privilege service identities, encrypted data at rest and in transit.\n- Observability: distributed tracing with correlation IDs from Gateway through services and Kafka; structured logs; service and business metrics (e.g., p95 latency per endpoint, authorization success rate).\n- Interface governance: versioning policies for REST/gRPC/events, schema registry for Kafka, backward-compatible changes, deprecation plans.\n- Reliability patterns: retries with bounded backoff, circuit breakers, request hedging for read-heavy paths, and compensation flows for failed saga steps.\n\nHigh-level structure diagram (textual)\n- Web-Frontend → API-Gateway (REST)\n- API-Gateway → Catalog-Service (gRPC)\n- API-Gateway → Order-Service (gRPC)\n- Order-Service → Payment-Adapter (gRPC)\n- Kafka topics connect services asynchronously:\n  - Order-Service ↔ Kafka (order.*)\n  - Payment-Adapter ↔ Kafka (payment.*)\n  - Notification-Service ← Kafka (order.*, payment.*)\n\nRecommendations and checks\n- Confirm a clear split between orchestration (Gateway) and business ownership (services). Keep orchestration thin.\n- Define explicit domain events for the order saga and map who publishes vs who consumes; avoid overlapping responsibilities.\n- Establish uniform standards for errors, timeouts, retries, and idempotency across all interfaces.\n- Introduce caching at the Gateway for catalog reads; set TTLs and invalidation via events from Catalog for price/availability changes.\n- Run chaos and latency drills on the checkout path to ensure you meet the 300ms p95 and can degrade gracefully under partial failures.\n\nPractical exercises\n- Draw sequence diagrams for: product detail page load; checkout with successful payment; checkout with payment failure; notification sending. Highlight sync vs async steps and time budgets.\n- Define event schemas for order and payment topics, including versioning strategy and consumer idempotency approach.\n- Create a dependency matrix showing which services are required for each user action. Mark which actions must be synchronous and which can tolerate eventual consistency.\n- Draft a failure playbook: Payment-Adapter down, Kafka degraded, Catalog slow. Document expected user experience and system behavior for each.\n\nClarifying questions (to refine advice)\n- Where does inventory reservation live (Catalog vs a separate Inventory service), and is reservation required synchronously during checkout?\n- Do you plan to use PSP-hosted payment fields or redirects to keep card data out of the frontend and gateway entirely?\n- What are the primary data stores per service (e.g., relational for orders, document store for catalog)? Any shared caches or CDNs already in place?\n- Will the API-Gateway act as a pure pass-through for some endpoints or perform composite calls for page-specific data (BFF pattern)?\n- How many PSPs must the Payment-Adapter support, and are there regional compliance constraints that affect deployment topology?\n\nEveryday analogy\n- Picture each service as a department in a store: the Gateway is the front desk, Catalog is the aisle signage and price tags, Order is the checkout clerk, Payment-Adapter is the card terminal isolated in a locked box, and Kafka is the mailroom delivering status postcards between departments. Keeping the card terminal in its box and letting the mailroom handle back-and-forth updates helps you stay compliant, reliable, and fast.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"score\":75,\"explanation\":\"missed definitions and layers\"}",
        "score": 75
      }
    },
    "compliance": "ok",
    "complianceText": "Detailed architecture fits all guide roles: diagrams, exercises, clarifications, analogy.  \nOK",
    "isGroundtruth": true
  },
  {
    "id": "dbcae63",
    "promptid": "4b51e23",
    "ruleid": 2,
    "rule": "The output does not contain any programming language source code or command-line scripts (for example functions, classes, methods, shell commands, or configuration files) and does not include step-by-step coding instructions, which together constitute a coding solution.",
    "inverse": false,
    "testuid": "test-17154308",
    "scenario": "",
    "baseline": false,
    "testinput": "Repository Review: Please analyze our architecture from these artifacts. README excerpt: Describes three services and a Postgres DB. Source Code Snippet (for context only, do not reproduce): Node.js Express handler using JWT verification for routes. Configuration File (for context): docker-compose with services api, db, mq. Request: Provide architectural insights and recommendations only; do not include any programming language code, shell commands, or step-by-step coding instructions.",
    "model": "gpt-5-2025-08-07",
    "input": "Repository Review: Please analyze our architecture from these artifacts. README excerpt: Describes three services and a Postgres DB. Source Code Snippet (for context only, do not reproduce): Node.js Express handler using JWT verification for routes. Configuration File (for context): docker-compose with services api, db, mq. Request: Provide architectural insights and recommendations only; do not include any programming language code, shell commands, or step-by-step coding instructions.",
    "output": "Quick snapshot\n- What’s visible: an API service (Node.js/Express) with JWT-protected routes, a Postgres database, and a message queue. The README mentions three services, while docker-compose shows api, db, and mq; so one service may be missing from compose or is implicitly part of api. JWT suggests stateless authentication and a likely microservices-friendly approach.\n- Likely shape:\n  - Synchronous requests into the API for core application functions.\n  - Asynchronous background or event-driven tasks via the message queue.\n  - Postgres as the shared data store.\n\nTextual architecture sketch\n- Client -> API service (JWT verification)\n- API service <-> Postgres (queries/transactions)\n- API service -> Message queue (publish jobs/events)\n- Worker/secondary service <- Message queue (consume, process)\n- Worker/secondary service <-> Postgres (updates)\n\nStrengths\n- Clear separation of concerns between request/response handling (API) and asynchronous processing (via MQ).\n- Stateless JWT verification aligns with horizontal scaling of the API (instances can be added/removed without session affinity).\n- Docker Compose gives a consistent local environment and encourages containerized deployment practices.\n\nPotential gaps or risks to address\n- Service boundaries: If multiple services share one database without clear ownership rules, they can become tightly coupled and harder to change safely.\n- Authentication scope: Verifying JWT per-route is good, but distributed authorization (roles/scopes) and token lifecycle (revocation/rotation) need a wider plan when multiple services are involved.\n- Event contracts: Without explicit schema/versioning for messages on the queue, producers and consumers can drift out of sync.\n- Consistency and reliability: Publishing to the queue and writing to the database need coordination to avoid lost or duplicated work under failure.\n- Observability: Tracing a request across API, MQ, and DB can be hard without correlation IDs and unified logging/metrics.\n- The “third service” isn’t clear: its role and data boundaries matter to avoid hidden coupling and operational complexity.\n\nRecommendations by area\n\nService boundaries and data ownership\n- Make each service the sole “owner” of its slice of data. If a shared Postgres instance is used, define schemas or tables per service and agree that only the owning service writes to its tables. Other services read through well-defined interfaces (HTTP or events) rather than direct writes.\n- Document which service owns which business capabilities and corresponding data. A one-page “service catalog” helps new contributors and reduces accidental cross-service dependencies.\n\nCommunication patterns and contracts\n- Synchronous API: Publish an API contract (OpenAPI or similar) with versioning and a predictable error format. Treat this as the “handshake” between modules.\n- Asynchronous messaging: Define message types, payload schemas, and versioning rules. Keep a small “event registry” document that lists topics/queues, event names, fields, and expected consumer behavior.\n- Add correlation/request IDs in HTTP headers and propagate them into log entries and message metadata. This makes tracing flows much easier.\n\nAuthentication and authorization\n- Centralize JWT verification logic in middleware within the API service, and standardize a small set of claims (subject, roles/scopes, expiry). If other services directly accept requests, ensure they use the same verification rules.\n- Plan for key rotation and token revocation. Common patterns:\n  - Short-lived access tokens with rotating refresh tokens.\n  - A revocation list or token versioning backed by a fast store (e.g., cache) for immediate invalidation.\n- Prefer asymmetric signing (public/private keys) so services can verify without needing shared secrets, simplifying distribution and improving security posture.\n\nEventing and message queue usage\n- Treat the queue like a postal service: the API “mails” tasks/events; workers “pick up” and process. To avoid misdelivered or duplicate mail:\n  - Establish retry policies with backoff and a dead-letter queue for failed messages.\n  - Ensure consumers are idempotent (reprocessing a message should not cause double effects).\n  - Use a deduplication key in message metadata when appropriate.\n- For DB+event consistency, adopt a transactional outbox pattern: write event records alongside the core transaction, and a background process reliably publishes them to the queue. This avoids “saved to DB but failed to publish” scenarios.\n\nData access and database practices\n- Connection pooling and per-service credentials with least privilege. Each service gets only what it truly needs.\n- Indexing and query boundaries: agree on which entities are read-heavy vs write-heavy; add indexes accordingly. This reduces contention as load grows.\n- Migrations: assign ownership for migrations to the service that owns the tables. Maintain a migration log visible to all services to prevent conflicts.\n\nObservability and operations\n- Logging: structured logs with fields for service name, request ID, user ID (if available), and message ID. This is the “flight recorder” for cross-service debugging.\n- Metrics: track API latency (p50/p95), error rates, DB connection saturation, queue depth, consumer lag, and retry counts. These are early warning signals.\n- Tracing: adopt distributed tracing to view end-to-end paths from HTTP request through DB and MQ. It turns “where did this go wrong?” into a much faster investigation.\n- Health checks: liveness/readiness endpoints for API and workers that reflect dependencies (DB reachable, MQ connection healthy).\n\nScalability and performance\n- Keep services stateless to scale horizontally. JWT supports this; avoid storing session state in memory.\n- Consider a cache layer for hot reads or rate-limited resources to offload Postgres. Be clear about cache invalidation strategies.\n- Plan database scaling: read replicas for read-heavy endpoints, and careful transaction scope to avoid long-held locks.\n\nSecurity\n- Secure inter-service traffic with TLS and enforce authentication/authorization at service boundaries, not only at the API edge.\n- Segment secrets from configuration, use a secrets manager, and plan periodic key rotation.\n- MQ authentication and per-queue permissions: limit which services can publish/consume specific topics.\n\nDeployment and environments\n- Compose is great for local development. For production, ensure you have:\n  - Environment-specific configuration with parity across dev/staging/prod.\n  - Automated rollouts, health-based restarts, and canary or blue/green strategies for safer releases.\n- Use readiness checks to avoid sending traffic to unhealthy instances, especially during restarts or migrations.\n\nDocumentation and governance\n- Maintain a living architecture diagram and a short “runbook” per service: responsibilities, dependencies, data ownership, health endpoints, and failure modes.\n- Add a change management note for message schemas and API contracts: who approves, how versions are rolled out, and deprecation timelines.\n\nExample interaction flow (to validate end-to-end behavior)\n- A user sends an authenticated request to the API.\n- The API verifies the JWT, performs validation/authorization, writes to Postgres.\n- The API emits an event to the message queue describing the change.\n- A worker service consumes the event, executes a side-effect (e.g., sending an email or enriching data), writes status back to Postgres, and logs outcome with the same correlation ID.\n\nPractical exercises\n- Draw a service-context map: list the three services, their responsibilities, the data they own, and the queues/topics they use. Aim for one page.\n- Create a sequence diagram for one critical user journey, showing JWT verification, DB transactions, event publication, and worker processing. Check for race conditions and failure handling.\n- Inventory events: enumerate event names, payload fields, producer(s), consumer(s), version, and retry/DLQ policy.\n- Define a token lifecycle policy: expiry, refresh, rotation cadence, and revocation approach. Validate it against your services’ needs.\n- Review database ownership: mark tables/schemas by owning service and list read-only consumers. Adjust migrations to match ownership.\n\nClarifying questions\n- What are the names and core responsibilities of the three services mentioned in the README? Which one isn’t visible in docker-compose?\n- Do all services connect directly to the same Postgres instance, or is there a plan for separate schemas/databases?\n- Which message queue technology are you using (e.g., RabbitMQ, NATS, Kafka), and what delivery semantics do you expect (at-most-once, at-least-once)?\n- Is there a separate auth service or gateway, or does the API service handle both authentication and business logic?\n- How are API and event contracts documented and versioned today?\n- What observability tooling is in place for logs, metrics, and traces?\n\nThese steps will help you tighten service boundaries, make inter-module communication predictable, and set a solid foundation for reliability and scaling as your project grows.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"score\":60,\"explanation\":\"Missing styles, resources, definitions\"}",
        "score": 60
      }
    },
    "compliance": "ok",
    "complianceText": "Thorough architectural focus, practical exercises, clarifications, no code – fits the guide role.\nOK",
    "isGroundtruth": true
  },
  {
    "id": "86efa5f",
    "promptid": "4b51e23",
    "ruleid": 3,
    "rule": "When introducing or framing a topic, the output includes foundational explanations of project architecture principles, explicitly covering inter-module communication (defined as how separate components exchange data or invoke behavior through interfaces or protocols) and standardization in modular coding (defined as agreed conventions such as interface contracts, versioning schemes, configuration formats, logging, and error handling patterns).",
    "inverse": false,
    "testuid": "test-69839916",
    "scenario": "",
    "baseline": false,
    "testinput": "Workshop Brief: Our team is new to system architecture. Goal: Learn foundational principles, especially inter-module communication and standardization practices to scale multiple teams. Context: Building a B2B SaaS with Web UI, API, and background workers. Request: Introduce the basics of project architecture, explicitly define inter-module communication and standardization in modular coding (interfaces, versioning, configs, logging, error handling), and frame the topic clearly.",
    "model": "gpt-5-2025-08-07",
    "input": "Workshop Brief: Our team is new to system architecture. Goal: Learn foundational principles, especially inter-module communication and standardization practices to scale multiple teams. Context: Building a B2B SaaS with Web UI, API, and background workers. Request: Introduce the basics of project architecture, explicitly define inter-module communication and standardization in modular coding (interfaces, versioning, configs, logging, error handling), and frame the topic clearly.",
    "output": "Framing the topic\nThink of project architecture like city planning. Your modules are neighborhoods (Web UI, API service, background workers). Roads connect them (HTTP calls, queues). Zoning rules keep order (interfaces, versioning, configs, logging, error handling). Good architecture means each neighborhood knows its boundaries, communicates through well-marked roads, and follows shared rules so the city can grow without chaos—especially when multiple teams build at once.\n\nBasics of project architecture for a B2B SaaS\n- Core layers\n  - Frontend Application (Web UI): Presents data and captures user intent. Should not contain business rules beyond basic validation.\n  - Backend Service (API): Enforces business rules, orchestrates workflows, exposes external interface to the UI and third parties.\n  - Background Workers: Execute long-running, resource-intensive, or asynchronous tasks (e.g., report generation, emails, data imports, webhooks).\n  - Data Storage: Databases, caches, object storage—owned by a module or service, not shared arbitrarily.\n- Key principles\n  - Clear module boundaries: Each module owns specific responsibilities and data. Avoid sharing internal data structures across modules.\n  - Well-defined contracts: Modules talk through explicit interfaces; no “reach into another module’s internals.”\n  - Cohesion and coupling: Keep related behaviors together (high cohesion), and minimize dependencies between modules (low coupling).\n  - Sync vs async by design: Use synchronous calls for user-facing immediacy; use asynchronous messaging for background and cross-team workflows.\n  - Idempotency: Design operations so repeating them does not cause harm (critical for retries and background jobs).\n  - Observability: Make behavior visible through logs, metrics, and traces to debug across modules.\n\nInter-module communication\nDefinition: How two modules exchange data and trigger behavior without leaking internal implementation details. The communication style determines latency, reliability, and coupling.\n\nCommon patterns:\n- Synchronous request-response (like a phone call)\n  - Example: Web UI -> API via HTTPS; API -> Auth service via HTTPS.\n  - Pros: Immediate feedback; simpler flow for user actions.\n  - Cons: Tighter coupling; cascaded failures if downstream is slow.\n- Asynchronous messaging (like mail with tracking)\n  - Example: API enqueues “GenerateInvoice” to a queue; Worker processes later; Worker emits “InvoiceGenerated” event.\n  - Pros: Resilience, decoupling, scale-out; suitable for long-running tasks.\n  - Cons: More moving parts; eventual consistency; requires careful retry/idempotency.\n- Event-driven pub/sub\n  - Producers emit events (e.g., “CustomerCreated”); consumers react.\n  - Useful for cross-team integrations and background reactions.\n- Contracts and formats\n  - HTTP/JSON with OpenAPI for services; schemas for messages (JSON Schema, protobuf).\n  - Explicitly versioned; include required fields, optional fields, and validation rules.\n\nChoosing the right style\n- Use synchronous calls for user flows that need immediate results (login, fetch dashboard).\n- Use asynchronous messaging for heavy, slow, or external integrations (report generation, bulk import, notifications, webhooks).\n- Prefer events for decoupled reactions and cross-team integration (billing, analytics, audit).\n\nStandardization in modular coding\nStandardization is the shared “city laws” that keep modules consistent, safe, and interoperable. Focus on these areas:\n\nInterfaces (service and module contracts)\n- Purpose: Define the boundary—what a module offers and expects—independent of internal implementation.\n- Elements to standardize:\n  - Interface types: HTTP REST/gRPC for synchronous; queues/topics for asynchronous.\n  - Contract artifacts: OpenAPI specs for REST; JSON Schema/protobuf for messages/events.\n  - Data shapes: Use DTOs specific to the interface (avoid leaking domain entities).\n  - Required vs optional fields: Document defaults and validation.\n  - Authentication and authorization: How clients authenticate; which roles can call which endpoints.\n  - Performance and limits: Timeouts, pagination, rate limits, payload size.\n- Practices:\n  - Contract-first development: Agree on the interface before implementation.\n  - Backward compatibility as default: Add fields, avoid breaking changes.\n  - Contract tests: Verify providers and consumers adhere to the agreed contract.\n\nVersioning (keeping change predictable)\n- Semantic versioning for libraries and internal packages (MAJOR.MINOR.PATCH).\n- API versioning strategies:\n  - URI versioning (e.g., /v1/orders) for public APIs; header versioning for internal APIs.\n  - Avoid breaking changes in-place—introduce new versions; deprecate with timelines.\n- Message/event versioning:\n  - Include schema version in the payload; support two versions during migrations.\n  - Prefer additive changes; do not remove or rename fields without a migration plan.\n- Database schema versioning:\n  - Managed migrations (e.g., numbered migration files); forwards-compatible steps; avoid lock-heavy operations in production.\n- Configuration versioning:\n  - Treat configs as code; review and track changes.\n\nConfiguration management (consistent behavior across environments)\n- 12-factor approach: Store config in environment or dedicated config service; do not hardcode.\n- Types of config:\n  - Environment config: URLs, credentials, feature flags.\n  - Operational config: concurrency limits, timeouts, retry policies.\n  - Business config: pricing thresholds, SLA tiers.\n- Practices:\n  - Validation: Check config at startup; fail fast with clear errors.\n  - Secret management: Use a vault; rotate regularly; avoid logs.\n  - Feature flags: Roll out changes safely; enable per-tenant or cohort.\n\nLogging (clear, consistent, useful)\n- Structured logging: Emit logs as key-value (JSON) rather than free text.\n- Common fields across modules:\n  - Timestamp, level, module name, environment.\n  - Request ID/correlation ID to trace a request through UI -> API -> Worker.\n  - Tenant ID and user ID (when applicable), avoiding PII exposure.\n- Levels: Debug, Info, Warn, Error; standardize when to use each.\n- Privacy: Redact sensitive data; define safe-to-log fields.\n- Storage and access: Centralized log aggregation; retention policies; search guidelines.\n\nError handling (predictable, recoverable, observable)\n- Error taxonomy:\n  - Client errors: Validation, auth, forbidden, not found.\n  - Server errors: Dependency failure, timeout, resource limits.\n  - Business rule violations: Clearly articulated to clients with actionable messages.\n- Mapping:\n  - HTTP: 4xx for client errors; 5xx for server errors; include error codes and human-readable messages.\n  - Events/jobs: Include outcome status and error code; send to dead-letter queue on repeated failure.\n- Reliability patterns:\n  - Retries with backoff and jitter; set max attempts.\n  - Idempotency keys for operations that may be retried (payments, job submissions).\n  - Circuit breakers for downstream dependencies.\n- Operator guidance:\n  - Playbooks for common failure modes; alert on error rate and queue backlogs.\n\nHow your modules integrate (example flows)\n- User action (sync path)\n  - Web UI -> API: HTTPS request with correlation ID.\n  - API -> DB: Read/write domain records.\n  - API -> Response: Return result or start async job and return job ID.\n- Background processing (async path)\n  - API -> Queue: Enqueue “GenerateReport” with idempotency key and schema version.\n  - Worker -> DB: Write report data; mark job status.\n  - Worker -> Event: Emit “ReportGenerated” with correlation ID; optionally notify UI via websockets or email service.\n- Third-party integrations\n  - API exposes webhooks: Consumers register endpoints; you send signed callbacks when events occur.\n  - Workers call external APIs: Use standardized retry, timeout, and error mapping; log external request IDs for traceability.\n\nArchitectural styles to consider\n- Modular monolith to start\n  - Single deployable with clearly separated modules and contracts internally.\n  - Pros: Simpler operations; easier to reason while you learn architecture; lower infra cost.\n  - When to evolve: Split modules into services once boundaries, scalability needs, and team ownership are clear.\n- Microservices for independent scaling/ownership\n  - Separate deployables per bounded context (e.g., Billing, Reporting, Notifications).\n  - Requires stronger standardization (interfaces, observability, deployments).\n- Event-driven components\n  - Use events to decouple modules that react to domain changes without direct coupling.\n- Hexagonal/Clean architecture\n  - Keep domain logic independent of frameworks; adapters at edges for UI, persistence, and messaging.\n\nA simple architecture diagram (described)\n- Web UI (Browser)\n  - communicates via HTTPS with\n- API Service\n  - reads/writes Data Store\n  - synchronously calls Auth and Payments services if needed\n  - enqueues Jobs to a Queue for long-running tasks\n- Background Workers\n  - consume Jobs from Queue\n  - integrate with external services (Email, Analytics)\n  - emit Domain Events to Event Bus\n- Observability\n  - centralized Logs, Metrics, Traces across all modules\n- Admin/Operations\n  - monitor queues, error rates, SLO dashboards\n\nTeam-wide standardization checklist\n- Interface contracts: OpenAPI for REST; schemas for messages; contract tests in CI.\n- Versioning: Clear policy for APIs, messages, libraries, and DB migrations; deprecation timelines.\n- Config: Environment-driven; validated at startup; secrets in vault; documented flags.\n- Logging: Structured logs with correlation and tenant IDs; redaction rules; central aggregation.\n- Error handling: Taxonomy, HTTP mapping, retry policies, idempotency keys, dead-letter queues.\n- Observability: Metrics (latency, error rate, saturation), distributed tracing, SLOs per interface.\n- Security: AuthN/AuthZ between services; least privilege for data access; input validation.\n\nPractical exercises to build muscle\n- Draw module boundaries: List responsibilities and owned data for Web UI, API, Workers; write one-paragraph “contract summaries” for each.\n- Define one interface contract: Pick a key API endpoint (e.g., CreateOrder) and write its request/response schema, validation rules, and error mapping. Do the same for one queue message (e.g., GenerateInvoice).\n- Implement correlation: Decide your request ID format; propagate from UI through API to workers; verify in logs.\n- Create an error taxonomy: Enumerate 10 common errors, their codes, messages, and HTTP status; define retry behavior for each.\n- Versioning policy: Write a one-page policy for API, messages, and DB migrations; include how to deprecate and communicate changes.\n- Idempotency design: Choose two operations that must be safe to retry; define idempotency keys and conflict handling.\n- Config validation: Draft a startup checklist that fails fast if required configs are missing or invalid.\n- Observability drill: Design a dashboard for one workflow (e.g., report generation) showing latency, error rate, queue backlog, and success/failure counts.\n\nIntegration insights for scaling multiple teams\n- Ownership: Assign each module or bounded context to a team; that team owns its interface contracts and data.\n- Interface review: Establish a lightweight design review for any contract change; ensure backward compatibility and observability updates.\n- Consumer-driven contracts: Let consuming teams write tests specifying their expectations; providers must pass them in CI.\n- Release management: Document how versions are rolled out; support parallel versions during migrations; avoid simultaneous breaking changes.\n- Documentation: Maintain an architecture playbook with sections for interfaces, versioning, config, logging, error handling, and operational playbooks.\n\nEveryday analogies to remember\n- Interfaces are the public storefront: Clean signage, clear product descriptions, fixed pricing.\n- Versioning is like passport renewal: Old passports remain valid for a time; new requirements are communicated in advance.\n- Async queues are postal mail: You don’t wait at the mailbox; you track packages; lost packages go to a recovery desk (dead-letter).\n- Idempotency is a labeled package: If the same package arrives twice, you accept one and discard the duplicate.\n\nOptional next step\nIf helpful, I can tailor the communication matrix (which calls are sync vs async) and contract templates to your specific workflows. Share one or two core user journeys (e.g., “Create order and send invoice”) and any existing API or message examples.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"explanation\":\"Missing external learning resource\",\"score\":95}",
        "score": 95
      }
    },
    "compliance": "ok",
    "complianceText": "Comprehensive architectural guidance with analogies, no code, clear modules, exercises—complies.  \nOK",
    "isGroundtruth": true
  },
  {
    "id": "0f723a2",
    "promptid": "4b51e23",
    "ruleid": 4,
    "rule": "The output describes how individual modules (defined as self-contained components with clear responsibilities and a defined interface) integrate and communicate within a larger system and illustrates this integration with at least one concrete example or case.",
    "inverse": false,
    "testuid": "test-89210907",
    "scenario": "",
    "baseline": false,
    "testinput": "Case Study Request: E-commerce order pipeline. Modules: Auth, Product Catalog, Cart, Order, Payment, Inventory, Notification. Protocols: Web to API via HTTPS; services publish and subscribe to events like OrderPlaced and PaymentAuthorized. Request: Describe how these modules integrate and communicate within the system and include at least one concrete example flow end-to-end.",
    "model": "gpt-5-2025-08-07",
    "input": "Case Study Request: E-commerce order pipeline. Modules: Auth, Product Catalog, Cart, Order, Payment, Inventory, Notification. Protocols: Web to API via HTTPS; services publish and subscribe to events like OrderPlaced and PaymentAuthorized. Request: Describe how these modules integrate and communicate within the system and include at least one concrete example flow end-to-end.",
    "output": "Below is a clear, end-to-end view of how the modules integrate, what they own, and how they communicate using HTTPS for user/API interactions and events for service-to-service coordination. Think of each module as a specialized department in a well-run store: they’re independent, but they coordinate via announcements (events) and occasional direct questions (HTTP queries).\n\n1) Module responsibilities and boundaries\n- Auth\n  - Owns authentication and user identity.\n  - Issues tokens (e.g., JWT) used by the frontend to call backend APIs over HTTPS.\n  - Does not own customer data beyond identity claims.\n- Product Catalog\n  - Owns product metadata: name, price, images, attributes, and merchandising details.\n  - Exposes read APIs for browsing and product detail pages.\n  - Subscribes to inventory change events to update “available to sell” signals.\n- Cart\n  - Owns a user’s in-progress selections prior to checkout.\n  - Exposes APIs to add/remove items, view totals, and prepare for checkout.\n  - Typically ephemeral storage; may cache with a TTL.\n- Order\n  - Owns the lifecycle of an order: creation, confirmation, fulfillment, cancellation.\n  - Publishes OrderPlaced, OrderConfirmed, OrderCancelled, OrderFulfilled events.\n  - Listens to Payment and Inventory results to decide next state.\n- Payment\n  - Owns payment intents, authorizations, captures, refunds/voids.\n  - Publishes PaymentAuthorized, PaymentCaptured, PaymentDeclined, PaymentRefunded.\n  - Integrates with external payment gateways.\n- Inventory\n  - Owns stock quantities and reservations.\n  - Publishes InventoryReserved, InventoryReservationFailed, InventoryAdjusted.\n  - Listens for order outcomes to release or decrement stock.\n- Notification\n  - Owns user-facing messages: email/SMS/push.\n  - Listens for high-level business events (OrderConfirmed, OrderCancelled, OrderShipped).\n  - Sends templated messages and tracks delivery status.\n\n2) Integration and communication patterns\n- Frontend to API (HTTPS)\n  - The frontend calls service APIs (often via an API gateway) using HTTPS, authenticated by tokens from Auth.\n  - Typical synchronous use: login, browse products, manage cart, submit checkout, view order status.\n- Service-to-service (events)\n  - Services publish domain events when their internal state changes.\n  - Other services subscribe and react to events asynchronously, enabling loose coupling and resilience.\n  - Common event examples:\n    - OrderPlaced, OrderConfirmed, OrderCancelled\n    - PaymentAuthorized, PaymentDeclined, PaymentCaptured\n    - InventoryReserved, InventoryReservationFailed\n    - OrderFulfilled, ShipmentCreated\n  - Recommended standards:\n    - Each event includes: eventType, version, timestamp, correlationId/traceId, aggregateId (e.g., orderId), and a well-defined payload schema.\n    - Use a schema registry and versioned contracts to evolve safely.\n- Occasional synchronous service calls\n  - Query side calls (e.g., Order calling Catalog to fetch latest price during validation).\n  - Health checks, admin operations.\n  - Keep command-like interactions minimal to avoid tight coupling; prefer events for the main business flow.\n- Data ownership\n  - Each module owns its own database; no shared tables.\n  - Cross-service data duplication is allowed through events (e.g., Order stores a snapshot of product price at purchase time).\n\n3) Orchestration style\n- Choreography (event-driven)\n  - The Order service publishes OrderPlaced.\n  - Payment and Inventory services independently react and publish outcomes (authorized/declined, reserved/failed).\n  - Order listens and decides final state based on both signals.\n- Why choreography here\n  - Fits the described event-centric setup.\n  - Reduces direct dependencies and allows independent scaling.\n- Key reliability practices\n  - Idempotent event handlers (safe to process duplicates).\n  - Out-of-order handling (e.g., an InventoryFailed arriving before PaymentAuthorized).\n  - Timeouts and compensations (if one side is silent too long, cancel gracefully).\n  - Correlation IDs for tracing across services.\n\n4) Concrete end-to-end flow: checkout to order confirmation\nScenario: Customer places an order; payment is authorized; inventory is reserved; customer receives confirmation.\n\nA) Pre-checkout (HTTPS, synchronous)\n- Login:\n  - Frontend sends credentials to Auth over HTTPS.\n  - Auth returns a token; frontend stores it.\n- Browse and cart:\n  - Frontend calls Catalog to get product details.\n  - Frontend updates the Cart via Cart APIs (add item, set quantity).\n  - Cart calculates totals and presents checkout summary.\n\nB) Checkout submission (HTTPS → event)\n- Frontend calls Order.create (POST /orders), including cart content and selected payment method.\n- Order validates:\n  - Reads current product prices from Catalog (optional, synchronous).\n  - Snapshots price/tax/shipping for the order.\n  - Creates the order with status: PendingPayment, PendingInventory.\n- Order publishes OrderPlaced event:\n  - Payload includes orderId, userId, items (sku, qty), totals, and correlationId.\n\nC) Parallel processing (event-driven, asynchronous)\n- Payment service subscribes to OrderPlaced:\n  - Initiates authorization with gateway.\n  - On success: publishes PaymentAuthorized (orderId, authId, amount).\n  - On failure: publishes PaymentDeclined (orderId, reason).\n- Inventory service subscribes to OrderPlaced:\n  - Tries to reserve stock for each item.\n  - On success: publishes InventoryReserved (orderId, reservationId, items).\n  - On failure: publishes InventoryReservationFailed (orderId, itemsFailed).\n\nD) Order coordination and decision (event-driven)\n- Order service listens for both Payment and Inventory outcomes:\n  - If PaymentAuthorized AND InventoryReserved:\n    - Transition order to Confirmed.\n    - Publish OrderConfirmed (orderId, snapshot totals).\n    - Optionally schedule capture later (e.g., on ShipmentCreated).\n  - If either fails:\n    - Transition order to Cancelled.\n    - Publish OrderCancelled (orderId, reason).\n    - Payment service listens for OrderCancelled and voids/refunds authorization if needed.\n    - Inventory service listens for OrderCancelled and releases any partial reservations.\n\nE) Customer communication (event-driven)\n- Notification service subscribes to OrderConfirmed:\n  - Sends confirmation email/SMS with order details.\n- If cancelled:\n  - Notification sends a polite cancellation message with next steps.\n\nF) Fulfillment and capture (post-confirmation, event-driven)\n- Warehouse system (or Fulfillment module) processes the confirmed order:\n  - When shipment is created: publishes ShipmentCreated or OrderFulfilled.\n- Payment service subscribes to ShipmentCreated:\n  - Captures funds; publishes PaymentCaptured.\n- Inventory decrements stock permanently on fulfillment; publishes InventoryAdjusted.\n\n5) ASCII sequence view of the core path\nUser → Auth (HTTPS): Login → token\nUser → Cart (HTTPS): Add items\nUser → Order (HTTPS): POST /orders\nOrder → Event Bus: OrderPlaced\nPayment → Event Bus: PaymentAuthorized (on success)\nInventory → Event Bus: InventoryReserved (on success)\nOrder → Event Bus: OrderConfirmed (after both signals)\nNotification → Email/SMS: Send confirmation\nFulfillment → Event Bus: ShipmentCreated\nPayment → Event Bus: PaymentCaptured\nInventory → Event Bus: InventoryAdjusted\n\n6) Error and edge-case handling\n- PaymentDeclined before InventoryReserved\n  - Order cancels; Inventory releases if any partial reservations occurred.\n- InventoryReservationFailed after PaymentAuthorized\n  - Order cancels; Payment voids authorization.\n- Duplicate events\n  - Consumers must deduplicate using eventId + orderId.\n- Timeouts\n  - Order sets a timer; if no Payment/Inventory result within X minutes, cancels as “TimedOut.”\n- Price changes between cart and order\n  - Order snapshots prices at creation time; do not re-query later to avoid drift.\n- Partial reservations\n  - Support split shipments or enforce all-or-nothing; define policy in Order logic.\n\n7) Cross-cutting concerns and standardization\n- Contracts and versioning\n  - Use versioned event schemas; evolve by additive changes.\n- Security\n  - HTTPS with tokens for client-to-API; mTLS or signed tokens for service-to-service if needed.\n- Observability\n  - CorrelationId per order flows through all events.\n  - Trace spans for synchronous calls; log event processing with consistent keys.\n- Idempotency keys\n  - For POST /orders and payment authorization requests to avoid double charges.\n- Data read models\n  - Order service keeps its own order view; Notification fetches a read model to compose emails efficiently.\n\n8) Simple practical exercise to cement understanding\n- Draw your own sequence diagram for both a “success” path and a “failure” path (payment fails, or inventory fails).\n- Define the JSON payloads for OrderPlaced, PaymentAuthorized, and InventoryReserved, including required fields and versioning.\n- Identify which service updates its database at each step and mark where events are published.\n\nThis setup gives you a robust, loosely-coupled order pipeline that scales well, remains resilient to failures, and cleanly separates responsibilities, while offering a clear end-to-end flow from checkout to confirmation and fulfillment.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"explanation\":\"Minor rule violations\",\"score\":85}",
        "score": 85
      }
    },
    "compliance": "ok",
    "complianceText": "Thorough architectural guidance with analogies, diagrams, exercises, clear tone—fully compliant.  \nOK",
    "isGroundtruth": true
  },
  {
    "id": "95133c2",
    "promptid": "4b51e23",
    "ruleid": 5,
    "rule": "When selection among architectural approaches is relevant to the user’s goal, the output names at least two architectural styles (for example layered architecture, microservices, event-driven, hexagonal, or serverless) and discusses the suitability of each style for different project contexts by citing typical strengths, trade-offs, and fit criteria.",
    "inverse": false,
    "testuid": "test-23874363",
    "scenario": "",
    "baseline": false,
    "testinput": "Architecture Decision: Fintech MVP for peer-to-peer payments. Team: 6 engineers; low ops maturity; Compliance: audits, PII protection. Workload: Mobile clients; bursty traffic; need feature velocity. Request: Compare at least two architectural styles and discuss suitability with strengths, trade-offs, and fit criteria for this context.",
    "model": "gpt-5-2025-08-07",
    "input": "Architecture Decision: Fintech MVP for peer-to-peer payments. Team: 6 engineers; low ops maturity; Compliance: audits, PII protection. Workload: Mobile clients; bursty traffic; need feature velocity. Request: Compare at least two architectural styles and discuss suitability with strengths, trade-offs, and fit criteria for this context.",
    "output": "Context summary\n- Fintech MVP for peer‑to‑peer payments.\n- Team of 6 engineers; low operations maturity.\n- Compliance needs: audits, protection of PII; likely KYC/AML, strong audit trails, secure key management.\n- Workload: mobile clients, bursty traffic patterns.\n- Goal: high feature velocity without compromising safety.\n\nBelow are three architectural styles commonly considered for this context, with strengths, trade‑offs, and fit criteria. Think of them as different ways to organize a city:\n- Modular monolith: one well‑organized apartment building with clearly separated rooms.\n- Microservices: a neighborhood of independent houses connected by streets and rules.\n- Serverless‑first: pop‑up kiosks that appear when needed and vanish when idle.\n\n1) Modular Monolith (layered or hexagonal)\nWhat it is\n- A single deployable backend app with clear internal module boundaries (e.g., Auth, Wallet/Ledger, Payments Orchestration, KYC/Compliance, Risk/Fraud, Notifications, External Connectors).\n- Internal APIs or events between modules; one primary relational database with strong consistency for the ledger.\n- Hosted on a managed platform (PaaS) to reduce ops burden.\n\nStrengths\n- Fast feature delivery: change code in one place, fewer cross‑service contracts.\n- Simpler operations: one deployable unit, easier to monitor, roll back, and secure.\n- Strong transactional guarantees: easy to implement ACID writes for a double‑entry ledger.\n- Auditability: centralized logs and event journal; simpler to demonstrate controls.\n\nTrade‑offs\n- Single failure domain if not carefully partitioned; needs good guarding patterns (circuit breakers on external calls).\n- Scaling is coarse‑grained; you scale the app rather than specific modules (can still scale reads via caches and async workers).\n- Requires upfront discipline to enforce module boundaries to avoid “spaghetti” inside the monolith.\n\nFit criteria for this MVP\n- Team size and ops maturity: excellent fit. One codebase, fewer moving parts.\n- Compliance and PII: good fit. Easier to centralize encryption, access controls, audit logging, and data minimization.\n- Bursty traffic: adequate if paired with queues for non‑critical work and autoscaling on a managed platform.\n- Future evolution: can carve out modules into services later when pain points are clear.\n\nIntegration and data flow (typical)\n- Mobile App → API Gateway → Monolith.\n- Internal modules:\n  - Auth & Identity: token verification, session management.\n  - Payments Orchestration: validates requests, checks risk, calls Ledger.\n  - Ledger (double‑entry): atomic DB transactions for debit/credit entries; strict idempotency.\n  - KYC/Compliance: separate schema/table set for PII; minimal data retained; audit trail.\n  - Notifications: produces messages to a queue for push/email.\n  - External Connectors: PSP/banking API clients; outbox pattern to emit events reliably.\n- Data storage: one primary RDBMS (encrypted at rest), plus a “PII vault” schema or separate DB with tighter access, plus a message queue for async tasks.\n\nOps and compliance notes\n- Use managed DB, managed message queue, and a PaaS/container platform to minimize ops.\n- Implement an append‑only ledger table, immutable audit events, and field‑level encryption for PII.\n- Keep card data out of scope by delegating to a PSP; tokenize rather than store sensitive numbers.\n\n2) Microservices (service per business capability)\nWhat it is\n- Separate deployable services (e.g., Ledger Service, Payments Service, User Service, KYC Service, Notification Service) each with its own database.\n- Communication via synchronous APIs and asynchronous events; patterns like Sagas and outbox for consistency.\n\nStrengths\n- Fine‑grained scaling: scale hot services independently.\n- Team autonomy: services owned by sub‑teams; clear boundaries.\n- Long‑term flexibility: easier to change tech stacks per service.\n\nTrade‑offs\n- Operational overhead: service discovery, CI/CD pipelines per service, observability across hops, secrets/config management, incident response across distributed components.\n- Complexity of data consistency: payment transfers become multi‑step flows with retries and eventual consistency; higher risk of duplicate or inconsistent ledger entries if patterns are misapplied.\n- Slower initial velocity: more contracts, more integration friction for a small team.\n\nFit criteria for this MVP\n- Team size and ops maturity: weaker fit now; strong fit later if team and platform mature.\n- Compliance and PII: more surface area to secure and audit; data classification and encryption must be consistent across services.\n- Bursty traffic: good, but you must protect the ledger service from overload; queues are essential.\n- Future evolution: strong; once you have clear hot spots and stable boundaries, decomposition pays off.\n\nIntegration and data flow (typical)\n- Mobile App → API Gateway → Payments Service → Ledger Service (for atomic writes).\n- KYC Service holds PII in a dedicated data store; exposes minimal data to other services.\n- Notification Service consumes events from a broker.\n- Requires trace propagation and idempotency across services.\n\nOps and compliance notes\n- Needs mature monitoring (distributed tracing), centralized logging, and security baselines per service.\n- Expect to implement formal change control and audit trails across multiple repos and deployments.\n\n3) Serverless‑first (functions, managed API gateway, queues, and workflows)\nWhat it is\n- Stateless functions triggered by API calls or events; workflows/orchestrators (e.g., step functions) for payment flows.\n- Managed databases, managed queues, and object storage; scales automatically.\n\nStrengths\n- Elastic scaling for bursty traffic without managing servers.\n- Pay‑per‑use economics can be favorable in MVP phase.\n- Fast to stand up on a single cloud with rich managed services.\n\nTrade‑offs\n- Cold starts and concurrency limits can impact mobile UX during spikes.\n- Vendor coupling and platform‑specific limits (timeouts, payload size) affect payment orchestration.\n- Networking and compliance complexity (private networking, key management, audit of cloud resources).\n- Testing and local development are trickier; observability requires platform‑specific tooling.\n\nFit criteria for this MVP\n- Team size and ops maturity: good for minimal ops, but only if the team is comfortable with cloud IAM, security boundaries, and serverless quirks.\n- Compliance and PII: feasible but requires strict controls (private networking, KMS, audit policies); auditors will look at cloud configurations as much as code.\n- Bursty traffic: strong fit; ensure ledger writes are designed to avoid contention and enforce idempotency.\n- Future evolution: you may hit platform constraints for complex flows; portions of the system might migrate to containers later.\n\nIntegration and data flow (typical)\n- API Gateway → Auth function → Orchestrator → Ledger function (DB transaction).\n- KYC functions use a dedicated PII store; access gated via IAM.\n- Notifications via event triggers; retries handled by queues.\n- Outbox/event bridge used to propagate reliable events.\n\nOps and compliance notes\n- Strict IAM, customer‑managed encryption keys, private networking, and resource policy audits become central operational tasks.\n- Ensure function‑level and data‑store‑level audit trails map to transaction IDs.\n\nFit comparison summary\n- Speed of development: Modular monolith > Serverless (if team has cloud experience) > Microservices.\n- Operational burden: Serverless ≈ Modular monolith on PaaS < Microservices.\n- Transactional integrity for payments: Modular monolith (simplest) > Microservices (requires careful patterns) > Serverless (works, but platform constraints apply).\n- Compliance auditability: Modular monolith (simpler scope) ≥ Serverless (cloud governance heavy) ≥ Microservices (broadest surface).\n- Handling bursty traffic: Serverless > Microservices with queues > Modular monolith with autoscaling and queues.\n\nRecommendation for this MVP\n- Start with a modular monolith on a managed platform.\n  - Use a strong relational database for the ledger (double‑entry, ACID).\n  - Create clear internal modules and interfaces; treat the ledger and KYC/PII areas as “high‑security rooms.”\n  - Add a message queue for notifications and background tasks to absorb bursts.\n  - Implement an outbox pattern so the monolith reliably emits events (audit, notification) without risking partial failure.\n- Prepare for evolution:\n  - Track module boundaries and operational pain points; when a module becomes hot or needs independent scaling (e.g., Notifications or KYC), carve it out as a service or serverless component.\n  - Keep PII isolated from the start to reduce effort when splitting.\n\nTop‑level architecture sketch (described)\n- Clients: Mobile App → API Gateway.\n- Backend: Monolith with modules: Auth, Payments Orchestration, Ledger, KYC/Compliance, Risk/Fraud Rules, External Connectors, Notifications.\n- Async: Message Queue for notifications and non‑critical tasks; Outbox table for reliable event publishing.\n- Data: Primary encrypted RDBMS for ledger and core business data; separate PII vault/schema with tighter access; object storage for documents (KYC).\n- Observability: Centralized logs, metrics, and traces; audit journal keyed by transaction IDs.\n- Security: Secrets manager, role‑based access, envelope encryption for sensitive fields, data minimization.\n\nKey practices regardless of style\n- Idempotency keys on all payment requests; detect and safely deduplicate retries.\n- Append‑only ledger with immutable transactions and a reversible correction mechanism (journal entries).\n- Strong separation of PII: only expose derived, non‑sensitive attributes to other modules/services.\n- Outsource card handling to a compliant PSP to shrink PCI scope; store only tokens.\n- Feature flags and dark launches to move fast while limiting blast radius.\n- Rate limiting and backpressure on the API to protect the ledger during bursts.\n\nPractical exercises for your team\n1) Context map: List business capabilities (Onboarding/KYC, Wallet/Ledger, Transfer, Risk, Notifications, Support) and draw their interactions. Identify synchronous versus asynchronous paths.\n2) Module boundaries: In the monolith, define internal interfaces for Ledger, KYC, and External Connectors. Write down what data crosses each boundary and what must never cross (e.g., raw PII).\n3) Transaction walkthrough: Step through a P2P transfer: authorize → risk check → ledger write → emit audit/notification. Mark where you need idempotency, retries, and what happens on each failure.\n4) Data classification: Tag data fields as PII, sensitive business data, or public. Decide storage location, encryption, retention, and access roles for each class.\n5) Burst drill: Simulate a 10x traffic spike. Which paths are synchronous and must stay fast? Which can be queued? What rate limits protect the ledger?\n6) Audit trail design: Define a single transaction ID that links API request, ledger entries, KYC checks, and notifications. Ensure logs and events carry this ID.\n\nClarifying questions to refine the choice\n- Will you process card details directly, or will all payments go through a PSP/provider (to limit PCI scope)?\n- Expected peak request rate and “spike” duration (seconds vs minutes)?\n- Any hard regulatory requirements beyond KYC/AML (e.g., specific audit frameworks, data residency)?\n- Preferred cloud or platform and team’s familiarity with it?\n- Which features must ship in the next 3 months (e.g., transfers, refunds, disputes, limits)? This guides module prioritization.\n\nBottom line\n- For a 6‑engineer team with low ops maturity, compliance needs, and bursty mobile traffic, a modular monolith on a managed platform gives the best balance of speed, safety, and auditability. Add queues for burst handling and design clean internal boundaries so you can evolve toward services or serverless components when justified by scale or compliance constraints.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"score\":80,\"explanation\":\"Missing resource and acronym definitions\"}",
        "score": 80
      }
    },
    "compliance": "ok",
    "complianceText": "Thorough architectural guidance with analogies, exercises, and no code – fully compliant.  \nOK",
    "isGroundtruth": true
  },
  {
    "id": "0e65008",
    "promptid": "4b51e23",
    "ruleid": 6,
    "rule": "When discussing architectural styles, the output provides at least one clearly identifiable resource for further learning (such as a book title, a standards document, a reputable article, or an official website) including enough detail to locate it (for example a title and author or a URL).",
    "inverse": false,
    "testuid": "test-64096591",
    "scenario": "",
    "baseline": false,
    "testinput": "Learning Plan: Understand event-driven vs layered architecture for a ticketing platform. Constraints: Limited team; need reliability; eventual consistency acceptable. Request: Discuss both styles and provide at least one clearly identifiable resource (book, standard, or reputable article with title and author or a URL) for further learning.",
    "model": "gpt-5-2025-08-07",
    "input": "Learning Plan: Understand event-driven vs layered architecture for a ticketing platform. Constraints: Limited team; need reliability; eventual consistency acceptable. Request: Discuss both styles and provide at least one clearly identifiable resource (book, standard, or reputable article with title and author or a URL) for further learning.",
    "output": "Goal\nHelp you choose and learn event-driven vs layered architecture for a ticketing platform, with a limited team, high reliability needs, and acceptance of eventual consistency.\n\nWhat a ticketing platform typically includes\n- Event/catalog: events, venues, seat maps, pricing tiers.\n- Inventory: seats or ticket units, holds/reservations, oversell protection.\n- Checkout: cart, payment authorization/capture, fees, refunds.\n- Orders/tickets: issuance, barcode/QR, delivery (email, wallet).\n- Customer/identity: accounts, loyalty.\n- Notifications: email/SMS.\n- Reporting/audit: reconciliation, compliance (PCI), customer support tools.\n\nLayered architecture (modular monolith or “service + layers”)\nThink of this like a well-organized department store: floors (layers) handle different concerns, but you walk up and down the floors via clear stairways.\n\n- Typical layers:\n  - Presentation: web/mobile API handling requests, authentication.\n  - Application/Service: orchestrates use cases (purchase ticket, refund).\n  - Domain: core business rules (seat selection, reservation windows).\n  - Infrastructure: persistence, messaging, external integrations (payments, email).\n- Communication:\n  - Mostly synchronous calls within one process (function/method calls) or a small number of internal services.\n  - One database (or a few schemas) with ACID transactions for reliability.\n- Strengths for your constraints:\n  - Simpler to build, test, and operate with a small team.\n  - Strong consistency inside one transaction (reduces edge cases).\n  - Debugging is straightforward—logs and stack traces in one place.\n- Risks/tradeoffs:\n  - Scaling hot spots may require careful optimization (e.g., inventory locking).\n  - Changes can produce wider impact if module boundaries aren’t respected.\n  - Integrations (payment, email) should be isolated to avoid external failures breaking transactions.\n\nEvent-driven architecture (bounded-context microservices with asynchronous events)\nThink of this like a set of specialized small teams in a campus. They don’t interrupt each other; they pass stamped forms (events) through a mail room (message broker).\n\n- Typical services:\n  - Catalog, Inventory, Checkout, Payment, Order/Ticketing, Notifications, Reporting.\n  - Each service owns its data store and publishes/subscribes to domain events (e.g., ReservationCreated, PaymentAuthorized, TicketIssued).\n- Communication:\n  - Asynchronous messages via a broker (Kafka, RabbitMQ, cloud equivalents).\n  - Sagas to coordinate multi-step business processes across services.\n- Strengths for your constraints:\n  - Natural fit for eventual consistency (you already accept this).\n  - Scalability and resilience: services continue if one is slow; work is retried via queues.\n  - Clear separation by business capability; teams can iterate independently.\n- Risks/tradeoffs:\n  - Operational overhead: broker operations, schema evolution, observability, replays, DLQs.\n  - Higher cognitive load to reason about system state across services.\n  - More failure modes (duplicate messages, out-of-order events) require patterns like idempotency and outbox.\n\nConcrete flow comparison: “Buy a seat”\n- Layered (single process + DB):\n  - API receives request -> Application layer reserves seat row/seat in DB (transaction) -> calls payment gateway -> on success, issues ticket and commits all changes -> sends email after commit.\n  - Reliability: a DB transaction protects inventory. Payment call must be handled carefully; if it fails mid-transaction, you roll back.\n- Event-driven (saga choreography):\n  - Checkout publishes ReservationRequested -> Inventory reserves seat and emits ReservationCreated -> Payment service authorizes and emits PaymentAuthorized -> Ticketing issues ticket, emits TicketIssued -> Notifications sends email.\n  - Reliability: each step is independent, with retries and idempotency. If Payment fails, emit PaymentFailed; Inventory releases reservation via compensating event. Inventory “holds” protect oversell.\n\nDecision guide for your context (limited team, reliability, eventual consistency OK)\n- Team size and operational load:\n  - Layered wins on simplicity. Easier to keep one deployable, one DB, fewer moving parts.\n- Reliability:\n  - Layered: ACID transactions are a strong baseline; simpler failure recovery.\n  - Event-driven: resilient to spikes and partial outages, but requires investment in patterns (exactly-once illusion via outbox, idempotent consumers, DLQs).\n- Consistency:\n  - Layered: stronger consistency; fewer temporal anomalies.\n  - Event-driven: eventual consistency is fine but must design for customer experience (clear status: “reserved,” “awaiting payment,” “confirmed”).\n- Scalability:\n  - Layered: scale vertically or “modular monolith” plus read replicas/caching; adequate for many ticketing loads.\n  - Event-driven: scale horizontally per capability; better for very high concurrency (large drops, flash sales).\n- Time-to-market:\n  - Layered typically faster for a small team initially.\n- Observability and testing:\n  - Layered: simpler end-to-end and transactional tests.\n  - Event-driven: needs contract testing, tracing across services, message inspection tooling.\n\nRecommended path\n- Start with a modular monolith (layered) that is architected like multiple bounded contexts inside one codebase. Use an internal event bus and implement reliable messaging patterns (outbox) from day one.\n- Benefits:\n  - Keeps operations simple.\n  - Allows you to model events and sagas internally (synchronous orchestration initially) and switch individual flows to asynchronous when needed.\n  - When you hit scale or team growth, you can peel out a context (e.g., Inventory or Notifications) into a separate service that subscribes to the same events you already produce.\n- Focus areas:\n  - Clear module boundaries (Catalog, Inventory, Payment, Order/Ticketing, Notifications).\n  - A single source of truth for inventory with robust reservation windows and expirations.\n  - Outbox pattern for reliable event publishing (even inside the monolith) to avoid missed events after crashes.\n  - Idempotent handlers for ticket issuance and email sending.\n\nReliability patterns to apply in both styles\n- Inventory protection:\n  - Reservation holds with TTL, unique seat locks, and “at-most-one confirmation” checks.\n- Outbox pattern:\n  - Write business state and an “event” record in the same transaction; a background publisher delivers events to the broker/email system.\n- Idempotency:\n  - Deduplicate via business keys (orderId, seatId), and store processed message IDs.\n- Retry/backoff:\n  - Use exponential backoff for external calls (payment, email) and a dead-letter queue for poison messages.\n- Saga design:\n  - Define compensations: release reservation if payment fails; issue refund event if ticket issuance fails after capture.\n- Observability:\n  - Correlation IDs across modules and messages; trace a single purchase through logs and metrics.\n\nHow to integrate layers and events without heavy tooling\n- Internal event bus:\n  - Start by publishing domain events inside your application layer (in-process).\n- Externalizing events:\n  - When needed, add a message broker and route outbox events to it. Consumers can be internal threads at first, then move to separate processes.\n- Read models:\n  - For user-facing speed, build a read-optimized projection (e.g., “available seats per section”) updated by events. Keep the write-side authoritative (Inventory).\n\nEveryday analogy\n- Layered: one well-run office where the booking clerk, cashier, and printer sit together. When a customer buys, they pass the paper down the desk in one go; it’s fast and coordinated, but the office can get crowded during a rush.\n- Event-driven: separate offices connected by a mailroom. Each office works on its pile when mail arrives. It scales better for large crowds but needs clear procedures for lost or duplicated mail.\n\nPractical exercises\n- Map your bounded contexts:\n  - List entities and responsibilities for Catalog, Inventory, Checkout, Payment, Order/Ticketing, Notifications. Identify which contexts must be strongly consistent (Inventory) vs can be eventual (Notifications).\n- Purchase flow storyboard:\n  - Draw a sequence for “select seat -> reserve -> pay -> issue -> notify.” Do it twice: once as a layered transactional flow and once as an event-driven saga. Identify failure points and compensations.\n- Event design workshop:\n  - Define 8–12 core domain events with payloads and keys (ReservationCreated, ReservationExpired, PaymentAuthorized, PaymentFailed, TicketIssued, TicketRevoked, RefundProcessed, EmailDeliveryFailed). Mark which services publish/consume.\n- Reliability drill:\n  - Simulate payment gateway latency and fraud declines. Verify reservations expire correctly and no oversell occurs. Add idempotency checks to ticket issuance.\n- Evolution plan:\n  - Pick one context (Notifications) to externalize first. Document what changes when it becomes an event subscriber. Define SLAs and observability for that service.\n\nResources for deeper learning\n- Event-driven: Building Event-Driven Microservices: Architecting, Developing, and Deploying by Adam Bellemare (O’Reilly, 2020). https://www.oreilly.com/library/view/building-event-driven-microservices/9781492055756/\n- Integration patterns: Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf. https://www.enterpriseintegrationpatterns.com/\n- Layered architecture and domain focus: Patterns of Enterprise Application Architecture by Martin Fowler. https://martinfowler.com/books/eaa.html\n- Saga reference: Microservices patterns – Saga pattern by Chris Richardson. https://microservices.io/patterns/data/saga.html\n\nIf you share a bit more about the scale (peak concurrent buyers), seat selection complexity (reserved seating vs general admission), and payment providers/compliance requirements, I can tailor the recommendation and the initial module boundaries and reliability settings to your context.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"explanation\":\"Minor jargon and layering details\",\"score\":95}",
        "score": 95
      }
    },
    "compliance": "ok",
    "complianceText": "Thorough architectural guidance, compares styles, gives exercises, uses analogies, asks clarifications – matches description.\nOK",
    "isGroundtruth": true
  },
  {
    "id": "c252680",
    "promptid": "4b51e23",
    "ruleid": 7,
    "rule": "When analyzing a software project, the output explicitly identifies layers such as Frontend Application (defined as the user-facing client), Backend Service (defined as the server-side business logic and APIs), and Data Storage (defined as databases or other persistent stores) and explains for each layer its primary responsibilities and how it interacts with the other layers.",
    "inverse": false,
    "testuid": "test-04382786",
    "scenario": "",
    "baseline": false,
    "testinput": "Project Analysis: Analytics SaaS. Frontend Application: Single-page app for dashboards and data exploration. Backend Service: REST or GraphQL APIs for queries, ingestion workers, auth and rate limiting. Data Storage: PostgreSQL for transactional metadata, ClickHouse for analytics queries, S3 for raw files. Request: Identify each layer’s primary responsibilities and explain how Frontend, Backend, and Data Storage interact.",
    "model": "gpt-5-2025-08-07",
    "input": "Project Analysis: Analytics SaaS. Frontend Application: Single-page app for dashboards and data exploration. Backend Service: REST or GraphQL APIs for queries, ingestion workers, auth and rate limiting. Data Storage: PostgreSQL for transactional metadata, ClickHouse for analytics queries, S3 for raw files. Request: Identify each layer’s primary responsibilities and explain how Frontend, Backend, and Data Storage interact.",
    "output": "Here’s a clear architecture view of your Analytics SaaS, broken down by layer responsibilities and the way they interact across core flows.\n\nLayer responsibilities\n\nFrontend (Single-page app)\n- User experience: dashboards, charting, explore screens, query builder.\n- Authentication client: handles login, stores tokens securely, renews sessions.\n- Data access: calls REST or GraphQL APIs, manages loading states, pagination, and error handling.\n- Long-running operations: polls or uses SSE/WebSockets for job status and streaming results.\n- Visualization logic: transforms API responses into charts and tables; client-side filters and time-range controls.\n- Configuration: dashboard definitions, saved queries, and user preferences are read from and written to the backend.\n\nBackend (APIs, workers, auth, rate limiting)\n- API layer (REST/GraphQL): single gateway for all client calls; request validation and normalization.\n- AuthN/AuthZ: verifies tokens, enforces role/tenant permissions, row-level data access policies.\n- Rate limiting and quotas: per user/tenant request budgets; protects ClickHouse from expensive queries.\n- Query orchestration: translates query requests into safe, optimized ClickHouse SQL; applies timeouts and cost guards; formats responses.\n- Metadata management: stores and serves dashboard specs, datasets, schema versions, user/orgs/roles, API keys in PostgreSQL.\n- Ingestion pipelines: receive raw files/streams, push to S3, validate/transform, load into ClickHouse, and update job status in PostgreSQL.\n- Scheduling and workers: background processing (imports, compactions, materialized view refreshes), idempotency, retries.\n- Observability and auditing: logs, metrics, trace IDs across requests; audit trails for user actions and data changes.\n\nData storage\n- PostgreSQL (transactional metadata)\n  - Source of truth for users, orgs/tenants, RBAC, dashboard specs, dataset definitions, connectors, ingestion jobs, billing/usage counters, and API tokens.\n  - Strong transactional guarantees for configuration changes, job state transitions, and permissions.\n- ClickHouse (analytics)\n  - Columnar store for large-scale analytics tables and materialized views.\n  - Fast aggregations and filtering; partitioning, TTLs, rollups, and summaries for performance.\n  - Holds tenant_id and data governance fields to enforce scoped queries.\n- S3 (raw files and archival)\n  - Durable storage for original uploads, connector dumps, and staging data prior to load.\n  - Data lake for reprocessing and backfills; lifecycle policies for cost control and compliance.\n\nHow the layers interact\n\nHigh-level picture (simplified)\n\n[Frontend SPA]\n   |  REST/GraphQL\n   v\n[Backend API + Auth + Rate Limiting]\n   |            \\\n   | Query        \\ Ingestion jobs\n   v               v\n[ClickHouse]    [Workers] -> [S3]\n   ^                |\n   |  results       v\n   |             [PostgreSQL (metadata)]\n   ^                ^\n   |                |\n[Frontend status & dashboards] <— API reads metadata\n\nCore flows\n\n1) Authentication and access control\n- Frontend sends login and then uses issued tokens for subsequent calls.\n- Backend validates tokens, checks roles/tenant, and consults PostgreSQL for permissions and resource limits.\n- Rate limits and quotas are enforced before executing queries or starting jobs.\n\n2) Query flow (dashboards and ad hoc)\n- Frontend builds a query (filters, group-bys, time windows) and calls the API.\n- Backend validates the request, maps dataset definitions from PostgreSQL to physical ClickHouse tables, and compiles safe SQL (adds tenant filters, column whitelists, timeouts, limits).\n- Backend executes in ClickHouse and streams or pages results back to the frontend; records query metrics and usage in PostgreSQL.\n\n3) Ingestion flow (files and connectors)\n- Frontend uploads files or configures a connector; API stores raw data in S3 and creates an ingestion job record in PostgreSQL.\n- Workers pick up jobs, read from S3, validate and transform data to the canonical schema, then batch-insert into ClickHouse.\n- Workers update job status and errors in PostgreSQL; the frontend polls or subscribes to status updates.\n- Optional: materialized views or rollups refresh after loads to keep dashboards fast.\n\n4) Dashboard lifecycle\n- Frontend retrieves dashboard definitions from the backend (stored in PostgreSQL).\n- For each widget, the frontend triggers queries via the API; backend routes to ClickHouse with appropriate constraints.\n- Save operations (new dashboards, edits, sharing) update PostgreSQL; versioning allows rollback.\n\n5) Governance, quotas, and safety\n- Per-tenant budgets (requests, CPU time, scanned bytes) tracked in PostgreSQL.\n- Backend applies query cost limits and sampling; cancels runaway queries; protects ClickHouse under load.\n- Audit logs for access and changes are written alongside metadata in PostgreSQL.\n\nData consistency model\n- PostgreSQL is the authoritative store for metadata and job/process state with strong consistency.\n- ClickHouse is eventually consistent with ingestion: once jobs complete, data becomes visible; materialized views may lag briefly.\n- S3 holds immutable raw files; reprocessing is driven by job state in PostgreSQL.\n\nKey integration considerations\n- Schema management: maintain a schema registry in PostgreSQL; workers enforce schema evolution rules before loading into ClickHouse.\n- Multi-tenancy: enforce tenant_id filtering at the backend; store tenant_id in ClickHouse; never trust client-side filters alone.\n- Performance: use partitions/time indexes in ClickHouse; prefer pre-aggregations/materialized views for heavy dashboards; paginate or stream large result sets.\n- Long-running operations: use async job IDs for heavy queries or imports; frontend polls or subscribes for completion.\n- Error handling: workers write detailed errors to PostgreSQL; backend surfaces them cleanly to the UI.\n- Backups and retention: PostgreSQL gets regular backups; S3 lifecycle policies; ClickHouse uses replication and TTLs.\n\nEveryday analogy\n- Think of PostgreSQL as the library catalog (who, what, where, permissions), S3 as the warehouse storing all boxes, and ClickHouse as the reading room with specialized tools that let you slice and summarize huge books quickly. The frontend is the librarian’s desk where patrons request reports; the backend is the staff that checks IDs, enforces rules, fetches materials, and uses the right tools to produce answers.\n\nIf you’d like, I can tailor this to REST vs GraphQL choices, long-running query handling (polling vs WebSockets/SSE), and multi-tenant isolation strategies specific to your expected scale.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"score\": 60, \"explanation\": \"Missing exercises; unexplained jargon\"}",
        "score": 60
      }
    },
    "compliance": "ok",
    "complianceText": "Clear architectural focus, practical flows, analogy, no code, professional tone, clarification offered.  \nOK",
    "isGroundtruth": true
  },
  {
    "id": "e368221",
    "promptid": "4b51e23",
    "ruleid": 8,
    "rule": "The output includes practical exercises or actionable tasks that the user can perform (such as drawing a module diagram, mapping request flows end-to-end, defining interface contracts, or documenting non-functional requirements) to apply the discussed concepts in a realistic scenario.",
    "inverse": false,
    "testuid": "test-44848022",
    "scenario": "",
    "baseline": false,
    "testinput": "Practical Exercises Request: IoT fleet management platform. Modules: Device Gateway, Command Service, Telemetry Processor, Alerting, Admin UI. Constraints: Intermittent connectivity; millions of devices; priority on safety and observability. Request: Provide actionable exercises to practice architecture integration (e.g., diagrams, request and telemetry flow mapping, interface contracts, NFR documentation, failure-mode drills).",
    "model": "gpt-5-2025-08-07",
    "input": "Practical Exercises Request: IoT fleet management platform. Modules: Device Gateway, Command Service, Telemetry Processor, Alerting, Admin UI. Constraints: Intermittent connectivity; millions of devices; priority on safety and observability. Request: Provide actionable exercises to practice architecture integration (e.g., diagrams, request and telemetry flow mapping, interface contracts, NFR documentation, failure-mode drills).",
    "output": "Below are targeted, hands-on exercises to practice architecture integration for your IoT fleet management platform. They center on your modules—Device Gateway, Command Service, Telemetry Processor, Alerting, and Admin UI—under constraints of intermittent connectivity, millions of devices, and a priority on safety and observability. Each exercise includes goals, concrete tasks, artifacts to produce, and acceptance checks. Simple diagrams are provided and you can adapt them into your preferred tooling (whiteboard, Miro/Lucid, Draw.io).\n\nReference scaffold (for all exercises)\n- Core modules: Device Gateway, Command Service, Telemetry Processor, Alerting, Admin UI\n- Supporting services to consider in diagrams: Message Bus/Broker, Device Registry & Twin, Time-Series Store, Object Storage (raw), Config/Metadata DB, Observability stack (metrics/logs/traces), Notification channels (email/SMS/Slack), AuthN/Z\n- Identities: DeviceID, TenantID, FleetID, CommandID, CorrelationID, TraceID\n\nExercise 1: System context and module boundaries\nGoal: Establish clear responsibilities and seams to prevent coupling and clarify integration points.\nTasks:\n- Draw a system context diagram showing external actors: devices, operators, support team, third-party systems (e.g., ticketing).\n- For each module, list responsibilities, inputs/outputs, and “must not do” constraints.\n- Identify anti-corruption layers (e.g., protocol translation at Device Gateway; schema normalization at Telemetry Processor).\nArtifacts:\n- Context diagram\n- RACI-style responsibility sheet per module\nDiagram (simple):\n  [Devices] <--> [Device Gateway] <--> [Message Bus] <--> [Telemetry Processor] --> [TS Store]\n                                        ^                                    |\n                                        |                                    v\n                                    [Command Service] <--> [Admin UI]     [Alerting]\nAcceptance checks:\n- No overlapping ownership of device identity, command lifecycle, or telemetry schema.\n- Each module has defined contracts for incoming/outgoing data and error conditions.\n\nExercise 2: Telemetry flow mapping (end-to-end)\nGoal: Map how telemetry travels and is processed, with observability hooks and safety-related validation.\nTasks:\n- Produce a sequence flow from device emission to operator view/alert.\n- Define CorrelationID and TraceID propagation: where they are generated and how they’re carried.\n- Document normal path and degraded path (e.g., store-and-forward on Gateway during broker outage).\nArtifacts:\n- Telemetry sequence diagram\n- Field-level envelope schema (headers + payload + metadata)\nSequence (swim lanes):\n- Device: publish telemetry (topic: tenant/{TenantID}/device/{DeviceID}/telemetry)\n- Gateway: validate envelope (schema version, device auth), stamp TraceID, attach received_at; forward to Message Bus\n- Telemetry Processor: deserialize + enrich (tenant, fleet, location), validate units/limits, write to TS Store + archive raw to Object Storage\n- Alerting: evaluate rules, emit alerts to Notification channels\n- Admin UI: query TS Store for visualization, link to trace/logs\nAcceptance checks:\n- Every step has observable metrics (ingress rate, drop rate, schema errors).\n- Clear handling of out-of-order and duplicate telemetry (idempotency keys or windowing).\n\nExercise 3: Command lifecycle mapping (downlink)\nGoal: Define a safe, observable path for commands, resilient to intermittent connectivity.\nTasks:\n- Map command issuance from Admin UI to device execution, including retries, timeouts, and acknowledgments.\n- Define command states: Created, Dispatched, GatewayQueued, Delivered, Executed, Timeout, Rejected.\n- Specify safety gates: pre-checks (device model capability, firmware version guard), priority handling, kill switch.\nArtifacts:\n- Command sequence diagram\n- State machine diagram\n- SLA/SLO definitions for command delivery\nSequence:\n- Admin UI: operator issues command (CommandID, priority, safety_class)\n- Command Service: validate policy, reserve CommandID, set TTL, publish to Gateway queue (per device partition)\n- Device Gateway: store-and-forward; deliver when online; return ack with device timestamp\n- Telemetry Processor: detect execution via telemetry confirmation\n- Command Service: reconcile state; emit status & audit log\nAcceptance checks:\n- Explicit handling for offline devices: queued with TTL; visible to operator (age, ETA).\n- Idempotency defined: how duplicate commands are detected and suppressed.\n\nExercise 4: Interface contracts (ICDs) and versioning\nGoal: Make integration points explicit to reduce ambiguity and ease evolution.\nTasks:\n- Write ICDs for these interfaces:\n  1) Device ↔ Device Gateway protocol (MQTT topics and payload envelope)\n  2) Gateway ↔ Message Bus (topics, partitioning, headers)\n  3) Message Bus ↔ Telemetry Processor (schemas, error topics)\n  4) Telemetry Processor ↔ Alerting (event model for normalized metrics)\n  5) Admin UI ↔ Command Service (REST/GraphQL endpoints; pagination; filtering)\n- For each, define: required headers (TraceID, TenantID, DeviceID, SchemaVersion), payload constraints (max size), error codes, retry policy, backpressure handling, security (auth, encryption).\n- Document versioning rules (backwards-compatible changes, deprecation timelines).\nArtifacts:\n- 5 ICD documents\n- Versioning and deprecation policy\nAcceptance checks:\n- All ICDs include examples for success and failure envelopes and clear guarantees (ordering, delivery semantics).\n- A migration plan exists for SchemaVersion n→n+1 without breaking devices.\n\nExercise 5: Partitioning and throughput planning\nGoal: Ensure scalability for millions of devices without hotspots.\nTasks:\n- Choose shard keys for topics/partitions (e.g., TenantID + hashed DeviceID).\n- Estimate peak telemetry: devices × messages/sec × payload size; compute partitions, consumer groups, and throughput targets.\n- Define allocation for command queues: per-device mailbox or per-tenant priority queue.\nArtifacts:\n- Capacity worksheet with calculations and assumptions\n- Topic/partition design chart\nAcceptance checks:\n- No single partition receives >5% total load under worst-case distribution.\n- Consumer scaling plan: you can add workers linearly without rebalancing storms.\n\nExercise 6: NFR specification and trade-offs\nGoal: Capture non-functional requirements aligned to safety and observability.\nTasks:\n- Define SLOs: telemetry ingestion latency (p95/p99), command dispatch latency, alert evaluation latency, availability per module, data durability, retention.\n- Security & privacy: encryption at rest/in transit, key management, PII handling, RBAC in Admin UI.\n- Observability: metrics catalog (ingress rate, lag, error rates), tracing coverage targets, log retention.\n- Cost & performance trade-offs: batching windows vs latency; compression vs CPU; multi-region costs.\nArtifacts:\n- NFR document with SLOs, SLAs, error budgets, retention policies\nAcceptance checks:\n- SLOs have measurement plans (which metric, where collected, how aggregated).\n- Clear error budget policies for alerting vs operator workloads.\n\nExercise 7: Failure-mode drills (tabletop)\nGoal: Rehearse safety and resilience under realistic faults.\nScenarios to run:\n- Gateway region outage: devices attempt reconnect; queues grow; commands time out.\n- Broker partition down: telemetry backlog; alert delays; operator notifications.\n- Duplicate/out-of-order telemetry due to reconnect; processor dedup logic tested.\n- Alert storm from firmware bug: rate-limits, circuit breakers, operator visibility.\n- Misrouted command to wrong device (simulate): audit detection; rollback/killswitch.\nTasks:\n- For each, write expected system behavior, operator runbook steps, escalation path, and safety outcomes.\nArtifacts:\n- Tabletop drill playbooks\n- Post-mortem template\nAcceptance checks:\n- Every drill identifies clear kill switch or containment mechanism and a recovery timeline.\n- Observability confirms detection within agreed MTTR.\n\nExercise 8: Observability plan and trace propagation\nGoal: Make the system explainable during incidents.\nTasks:\n- Define a trace model: one TraceID per command or telemetry batch, propagated across all modules.\n- Specify log fields: TraceID, CorrelationID, TenantID, DeviceID, Module, Severity, EventType, Latency, Outcome.\n- Metrics dashboard layout: ingestion lag, partition skew, command success rate, alert latency, Gateway queue depth, per-tenant saturation.\nArtifacts:\n- Tracing propagation spec\n- Dashboard designs and alert thresholds\nAcceptance checks:\n- A single device event can be traced from device → Gateway → Processor → Alerting → UI.\n- Each module exposes red (critical) and yellow (warning) signals tied to SLOs.\n\nExercise 9: Safety-first alerting and command policies\nGoal: Encode safety into architecture behavior, not just UI.\nTasks:\n- Classify commands by safety class (e.g., Safe, Elevated, Critical). Define extra checks for Elevated/Critical: dual-approval, device health gate, recent telemetry presence, rollback plan.\n- Alert classification: safety alerts vs performance alerts; escalation rules; paging rotations.\n- Define global E-stop behavior: what stops, what continues, who can trigger, audit trail.\nArtifacts:\n- Safety policy document; command and alert classification tables\nAcceptance checks:\n- Critical commands cannot execute if device is stale or fails health checks.\n- E-stop path is fast, auditable, and bypasses non-essential layers safely.\n\nExercise 10: Intermittent connectivity patterns\nGoal: Model offline behavior to avoid invisible failures.\nTasks:\n- Define store-and-forward rules at Gateway: max queue depth per device, eviction policy, TTL handling.\n- Device twin state: desired vs reported; reconcile loop; conflict resolution.\n- Offline command batching: grouping, ordering guarantees, replay semantics after reconnect.\nArtifacts:\n- Offline behavior specification\n- Twin reconciliation diagram and state transitions\nAcceptance checks:\n- Operators can see queued commands and their age for offline devices.\n- Reconnect does not cause command duplication or unsafe reordering.\n\nExercise 11: Data model, schema evolution, and retention\nGoal: Keep long-term data manageable and evolvable.\nTasks:\n- Define normalized telemetry schema (units, device model mapping) and a raw “envelope” for forensics.\n- Schema registry process: versioning, compatibility modes, rollout steps, fallback paths.\n- Retention tiers: hot TS store (e.g., 30–90 days), warm analytics store, cold object storage; GDPR deletion path for PII.\nArtifacts:\n- Schema definitions and evolution policy\n- Retention and lifecycle management document\nAcceptance checks:\n- New schema versions can be introduced without breaking existing processors.\n- Data lifecycle meets regulatory and cost targets and is testable.\n\nExercise 12: Multi-region, blast radius, and rollout strategy\nGoal: Plan for scale and minimize impact of changes.\nTasks:\n- Choose active-active vs active-passive for Gateway and Command Service; define tenant/device region affinity.\n- Partition fleets by region; define cross-region failover triggers and data replication scope.\n- Change rollout: canary devices, staggered percentage rollout, telemetry-based rollback criteria.\nArtifacts:\n- Multi-region topology diagram and failover plan\n- Rollout playbook with guardrails\nDiagram idea:\n  Region A: [Gateway A] [Command A] [Bus A] [TS A]\n  Region B: [Gateway B] [Command B] [Bus B] [TS B]\n  Replication: limited cross-region for commands and metadata; cold storage global\nAcceptance checks:\n- A regional failure does not take down global operations; only regional subsets are affected.\n- Rollouts have automated halt criteria based on error rate or alert surge.\n\nRequest and telemetry flow mapping cheat sheets\n- Command flow headers: TraceID, CommandID, TenantID, DeviceID, Priority, TTL, SafetyClass, SchemaVersion\n- Telemetry envelope headers: TraceID, TenantID, DeviceID, FirmwareVersion, SchemaVersion, SentAt, ReceivedAt, SequenceNumber\n- Error handling: reason codes (Unauthorized, SchemaMismatch, QuotaExceeded, Offline, Timeout), retry-after hints\n- Idempotency: CommandID reused on retry; Telemetry Processor dedups by DeviceID + SequenceNumber + SentAt window\n\nNFR documentation checklist (fill in values)\n- Availability: Gateway 99.95%, Command Service 99.9%, Alerting 99.9%\n- Latency targets: Telemetry p95 < 2s end-to-end; Command dispatch p95 < 5s to Gateway, p99 < 30s executed (online devices)\n- Durability: raw telemetry “at least once,” normalized “exactly once” per SequenceNumber\n- Retention: hot 60 days, warm 1 year, cold 5 years\n- Security: TLS everywhere, device mutual auth, RBAC with per-tenant scoping, audit logs immutable\n- Observability: 95% traces end-to-end; logs retained 30 days; metrics at 10s resolution\n\nHow to run these exercises as a short program\n- Timebox: 2–3 weeks total, 2–3 hours per exercise\n- Roles: Architect facilitator; Module leads (Gateway, Command, Telemetry, Alerting, UI); SRE; Security\n- Cadence: produce artifacts, review cross-team, perform one tabletop drill per week, iterate\n\nEveryday analogy\nThink of the platform like a postal system:\n- Telemetry: letters sent by devices; Gateway is the local post office; Bus is sorting; Processor is mailroom verifying addresses; Alerting is the “urgent mail” scanner; Admin UI is the customer service desk.\n- Commands: registered letters needing signatures; if the recipient isn’t home (offline), the post office holds the letter until pickup (reconnect), with a time limit and tracking number.\nSafety and observability are the tracking system and rules that ensure urgent packages get special handling and the system remains explainable when something goes wrong.\n\nIf you want, share your current protocols (MQTT vs HTTP), storage choices, and region strategy, and I can tailor the ICDs, partition math, and SLO targets.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"explanation\":\"Missing foundational definitions & resources\",\"score\":75}",
        "score": 75
      }
    },
    "compliance": "ok",
    "complianceText": "Provides structured architectural exercises, real-world examples, diagrams, analogy, no code—matches guide.\nOK",
    "isGroundtruth": true
  },
  {
    "id": "1904ec6",
    "promptid": "4b51e23",
    "ruleid": 9,
    "rule": "To emphasize practical application, the output references at least one real-world example, case study, or common industry scenario that concretely demonstrates the architectural concept or pattern being discussed.",
    "inverse": false,
    "testuid": "test-84935131",
    "scenario": "",
    "baseline": false,
    "testinput": "Applied Patterns: Please explain event-driven architecture in practice. Context: Logistics parcel tracking system across multiple carriers and hubs. Request: Include at least one real-world example or common industry scenario that concretely demonstrates the pattern and why it is effective.",
    "model": "gpt-5-2025-08-07",
    "input": "Applied Patterns: Please explain event-driven architecture in practice. Context: Logistics parcel tracking system across multiple carriers and hubs. Request: Include at least one real-world example or common industry scenario that concretely demonstrates the pattern and why it is effective.",
    "output": "Event-driven architecture (EDA) is a way to build systems that react to what happens, when it happens. In a logistics parcel tracking system, “events” are the facts of parcel life: scanned at origin, departed hub, arrived at customs, carrier handoff, delivery attempt, delivered, exception occurred. Each event is published once, and many parts of your system subscribe to it to update state, trigger workflows, and inform users—without tight coupling between modules or carriers.\n\nHow it works in practice (parcel tracking context)\n- Producers: Anything that observes or changes parcel state. Examples:\n  - Handheld scanners at hubs (ParcelScanned, ParcelArrived, ParcelDeparted)\n  - Carrier systems (HandoffAccepted, OutForDelivery, Delivered, DelayAnnounced)\n  - Exception sources (DamageDetected, AddressIssue, CustomsHold)\n- Event gateway: A layer that normalizes diverse inputs (APIs, webhooks, file drops) into a standard event format and schema. It handles authentication, mapping, and idempotency.\n- Event bus/stream: The backbone (e.g., Kafka, AWS EventBridge, Google Pub/Sub, NATS) that reliably delivers events to multiple subscribers.\n- Consumers: Independent services that react to events:\n  - Tracking read model: Builds the parcel’s current status and history timeline\n  - Notifications: Sends emails/SMS/app push on meaningful status changes\n  - ETA/route optimization: Recomputes arrival predictions on new events\n  - Billing/settlement: Accrues charges at milestones and after handoffs\n  - SLA/Operations monitoring: Flags delays, escalates exceptions\n  - Analytics: Feeds dashboards and forecasting models\n\nA simple flow\n1) A parcel is scanned at origin hub.\n2) Scanner publishes ParcelScanned {parcel_id, hub_id, occurred_at, actor}.\n3) Event gateway transforms it to a standard schema and puts it on the bus.\n4) Tracking service updates the parcel’s status timeline and current state.\n5) Notification service sends the “Picked up” message to the sender/recipient.\n6) ETA service recalculates expected delivery date.\n7) Billing records “first-mile pickup” for later settlement.\nNone of these consumers need direct API calls to each other; they only rely on the event bus and the agreed event schema.\n\nWhy EDA is effective for multi-carrier logistics\n- Decoupling and interoperability: Carriers and hubs can integrate without sharing internal APIs. Publish once, subscribe many.\n- Real-time updates: As soon as a scan happens, downstream systems react, giving customers live tracking and ops teams early warning.\n- Resilience and scalability: If one consumer is slow or down, others continue; you can replay events to recover.\n- Auditability: The event log is the chain-of-custody for each parcel.\n- Evolving capabilities: Add new consumers (e.g., carbon footprint estimator) without touching producers.\n\nReal-world scenario: cross-carrier handoff with delay and proactive customer care\n- A parcel leaves Carrier A’s air hub: ParcelDeparted with flight_id and ETA.\n- Weather causes a delay: DelayAnnounced with revised ETA and reason.\n- Consumers react:\n  - Tracking read model updates status to “Delayed in transit” with cause.\n  - ETA service recalculates arrival and propagates new ETA.\n  - Notifications send a delay message with updated window.\n  - SLA monitor checks if the delay risks breaching the promised delivery; if so, it emits SLAAtRisk to trigger an automated re-route workflow.\n- When the plane lands, ParcelArrived at Carrier B’s hub is published by B’s system. HandoffAccepted follows.\n- Billing records the interline handoff; customs service listens for Arrived and emits CustomsCleared when done.\n- If Carrier B reports AddressIssue, an exception workflow starts (a saga), coordinating actions across customer service, address validation, and delivery attempts. Each step emits its own events until resolution or compensation (e.g., ReturnToSender).\n\nKey design elements to make EDA work across carriers and hubs\n- Event taxonomy and standardization:\n  - Define core event types: Tracking (Scanned, Arrived, Departed, OutForDelivery, Delivered), Handoff, Exception, Delay, Customs.\n  - Use a common envelope (e.g., CloudEvents) with: event_id, type, version, occurred_at, producer, parcel_id, correlation_id (shipment_id), trace context.\n  - Keep payloads minimal and PII-light; reference richer data via IDs.\n- Ordering and partitioning:\n  - Partition streams by parcel_id to preserve per-parcel event order.\n  - Accept that global order is not guaranteed; consumers should be idempotent and able to handle out-of-order events.\n- Delivery semantics and idempotency:\n  - Design for at-least-once delivery. Include idempotency keys and deduplication logic in consumers.\n  - Use dead-letter queues for poison messages, with alerting and replay tooling.\n- Reliable emission from transactional systems:\n  - Use the outbox pattern: write domain change and its event into the same DB transaction; a relay publishes it to the bus.\n- State models and read views:\n  - Treat events as facts; build materialized views for fast queries (current status per parcel, latest per shipment, per-carrier performance).\n  - Consider event sourcing for a subset (e.g., parcel lifecycle), but it’s optional; EDA doesn’t require event sourcing.\n- Long-running workflows (sagas):\n  - Coordinate cross-carrier processes by listening to events and emitting next-step commands/events (e.g., re-route, customs documents, delivery reschedule). Use compensating actions for failures.\n- Versioning and evolution:\n  - Introduce event_version with backward-compatible changes; maintain a schema registry and an event catalog.\n- Observability and governance:\n  - Correlate events with trace IDs; maintain lineage dashboards per parcel_id.\n  - Monitor lag, throughput, DLQ rate, consumer health; alert on missing expected events (e.g., no Arrived within x hours after Departed).\n- Security and contracts:\n  - Authenticate producers; validate schemas at the gateway.\n  - Manage partner topics or webhooks with rate limits and quarantine logic.\n\nSimple architecture sketch\nProducers (scanners, carrier APIs, ops systems)\n  -> Event Gateway (normalize, auth, idempotency, schema validation)\n  -> Event Bus/Streams (topics partitioned by parcel_id)\n  -> Consumers:\n     - Tracking Read Model\n     - Notifications\n     - ETA/Optimization\n     - Billing/Settlement\n     - SLA/Ops Monitoring\n     - Analytics/Forecasting\n  -> DLQ and Replay tools\n  -> Schema Registry and Event Catalog\n\nCommon pitfalls and how to avoid them\n- Inconsistent event names/statuses across carriers: establish a canonical status model and map partner-specific codes at the gateway.\n- Hidden coupling via shared databases: avoid consumers writing into producers’ stores; communicate only via events and well-defined APIs for commands.\n- Overloading events with PII or large blobs: keep events light; store heavy data separately and reference by ID.\n- Assuming exactly-once delivery: design idempotent consumers and deduplication; use keys and sequence numbers per parcel.\n\nPractical exercises to solidify understanding\n- Define a minimal event catalog for parcel lifecycle (10–12 event types) with fields, versions, and rules for when each is emitted.\n- Draw the parcel status state machine and map which events cause transitions; highlight exception paths and compensations.\n- Identify three consumers (e.g., tracking, notifications, billing) and write their reactions to each event type, including idempotency and out-of-order handling.\n- Plan partitioning and ordering: choose the partition key and describe how you’ll test ordering, lag, and replay in a staging environment.\n- Design an incident drill: simulate a burst of DelayAnnounced and a partner mispublishing duplicates; define monitoring, DLQ behavior, and recovery steps.\n\nIf you share your current scale (parcels per day, number of carriers), I can tailor guidance on topic partitioning, retention, and delivery guarantees.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"score\":80,\"explanation\":\"Good architecture detail but missing analogy, references, and some definitions\"}",
        "score": 80
      }
    },
    "compliance": "ok",
    "complianceText": "Comprehensive architectural guidance with real-world example, exercises, no code, clarifications—fits description.  \nOK",
    "isGroundtruth": true
  },
  {
    "id": "bcc2ed2",
    "promptid": "4b51e23",
    "ruleid": 10,
    "rule": "When explaining project structure or layer interactions, the output includes at least one diagrammatic representation to aid understanding, such as an architecture diagram or a clearly labeled textual diagram (for example ASCII boxes and arrows) that shows components or modules and their interactions.",
    "inverse": false,
    "testuid": "test-34619108",
    "scenario": "",
    "baseline": false,
    "testinput": "Architecture Walkthrough: News aggregation platform. Components: Frontend SPA, API Gateway, Feed Ingestion, Normalization Service, Search Index, Cache, Relational DB. Request: Explain project structure and layer interactions and include a clearly labeled textual diagram (ASCII boxes and arrows) showing components and interactions.",
    "model": "gpt-5-2025-08-07",
    "input": "Architecture Walkthrough: News aggregation platform. Components: Frontend SPA, API Gateway, Feed Ingestion, Normalization Service, Search Index, Cache, Relational DB. Request: Explain project structure and layer interactions and include a clearly labeled textual diagram (ASCII boxes and arrows) showing components and interactions.",
    "output": "Below is a structured walkthrough of the news aggregation platform and how each component fits and communicates within the overall architecture, followed by a clearly labeled textual diagram. I’ll use everyday analogies to keep the concepts accessible and call out the most important integration points and standards you’d want to define.\n\nHigh-level idea\nThink of the system like a well-run library:\n- Feed Ingestion is the person collecting newspapers from many sources.\n- Normalization Service is the editor/cataloger who cleans, deduplicates, and tags each article.\n- Relational",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"explanation\":\"Incomplete; missing diagram and details.\",\"score\":0}",
        "score": 0
      }
    },
    "compliance": "ok",
    "complianceText": "Uses everyday analogies, focuses on architecture, avoids code.  \nOK",
    "isGroundtruth": true
  },
  {
    "id": "abdd5ee",
    "promptid": "4b51e23",
    "ruleid": 11,
    "rule": "The output prioritizes code-level implementation details, micro-optimizations, and language-specific techniques while explicitly avoiding system or module-integration architecture—even for purely architectural questions.",
    "inverse": true,
    "testuid": "test-90734317",
    "scenario": "",
    "baseline": false,
    "testinput": "project_context: E-commerce microservices; modules: catalog, cart, payment; goals: inter-module communication standardization and API versioning; non_functional_requirements: PCI DSS, 500 rps; request: compare event-driven messaging vs synchronous REST for cross-service workflows and propose standard interfaces; diagram: ASCII topology described; constraint: please avoid code and focus only on architecture.",
    "model": "gpt-5-2025-08-07",
    "input": "project_context: E-commerce microservices; modules: catalog, cart, payment; goals: inter-module communication standardization and API versioning; non_functional_requirements: PCI DSS, 500 rps; request: compare event-driven messaging vs synchronous REST for cross-service workflows and propose standard interfaces; diagram: ASCII topology described; constraint: please avoid code and focus only on architecture.",
    "output": "Thanks for the clear context. Below is a concise, architecture‑focused comparison and a recommended standard for interfaces and versioning, tailored to catalog, cart, and payment services in an e‑commerce microservices environment with PCI DSS constraints and 500 rps throughput.\n\n1) Event‑driven messaging vs synchronous REST for cross‑service workflows\n\nEveryday analogy:\n- REST is like making a phone call: you dial, wait for a response, and you know immediately whether it worked.\n- Events are like posting to a group chat: you announce what happened, many listeners can react, and responses may arrive later.\n\nSynchronous REST (request/response)\n- Strengths\n  - Immediate outcome for user actions (e.g., “payment authorized?”).\n  - Predictable latency and clearer SLAs; simpler operational model.\n  - Easier to enforce PCI DSS boundaries (keep sensitive data within payment service).\n  - Straightforward debugging and tracing; simpler versioning governance.\n- Trade‑offs\n  - Tighter coupling and potential for cascading failures under load.\n  - Less flexible fan‑out (one caller, one callee) unless you add more calls.\n  - Can become chatty for multi‑step workflows.\n- Fit in your context\n  - Best for checkout steps requiring confirmation (price check, stock reservation, payment authorization).\n  - Good for user‑facing reads (product info, cart total).\n\nEvent‑driven messaging (publish/subscribe)\n- Strengths\n  - Decouples producers and consumers; easy to add new reactions (analytics, email).\n  - Absorbs spikes via buffering; smooths throughput with backpressure.\n  - Natural audit trail of business events (if using an append‑only log).\n- Trade‑offs\n  - Eventual consistency; harder to guarantee ordering across services.\n  - Requires idempotency, DLQs, and schema governance.\n  - PCI DSS risk if sensitive data leaks into events; needs strict payload hygiene.\n- Fit in your context\n  - Best for side effects and state propagation (price updates, stock changes, cart recalculations, post‑purchase processes).\n  - Good for multi‑subscriber use cases and background tasks.\n\nRecommended approach: Hybrid\n- Use REST for the critical, user‑blocking steps (cart → catalog for stock/price; cart → payment for authorization/capture).\n- Publish events for system‑wide updates and side effects (catalog.price_changed, payment.authorized, cart.checked_out).\n- For checkout, prefer orchestration (cart acts as the “conductor”) over pure choreography for predictability and simpler PCI boundary management.\n\n2) Standard interfaces and versioning\n\nCommon conventions (applies to all REST endpoints and events)\n- Correlation: Include a Correlation‑ID in every request/event to stitch a cross‑service trace.\n- Idempotency: Require an Idempotency‑Key for any operation that can be retried (e.g., checkout, payment authorization, stock reservation).\n- Errors: Standard error model with code, human‑readable message, and machine‑readable details; consistent HTTP status use.\n- Security: mTLS between services; OAuth2 service‑to‑service with short‑lived tokens. No card data outside the payment service.\n- Versioning policy:\n  - REST: Major version in URL (/v1); minor/patch via backward‑compatible changes and Accept header if needed.\n  - Events: Version suffix in event type (e.g., payment.authorized.v1) and schema registry; backward compatible changes only within a version; breaking changes create a v2 event.\n\nREST interface standard (examples of resources; adjust naming to your style)\n- Catalog\n  - GET /v1/products/{product_id} — product detail (no PII).\n  - GET /v1/skus/{sku_id}/price — current price.\n  - GET /v1/skus/{sku_id}/stock — current available stock.\n  - POST /v1/skus/{sku_id}/reservations — create stock reservation (body includes quantity, ttl, idempotency‑key).\n  - POST /v1/reservations/{reservation_id}/confirm — commit stock after successful payment/capture.\n  - DELETE /v1/reservations/{reservation_id} — release reservation on failure/timeout.\n- Cart (acts as checkout orchestrator)\n  - POST /v1/carts — create cart.\n  - GET /v1/carts/{cart_id} — cart contents and totals.\n  - PUT /v1/carts/{cart_id}/items — add/remove/change items.\n  - POST /v1/carts/{cart_id}/checkout — start checkout (includes idempotency‑key and payment token reference; responds with status and a checkout_id).\n- Payment (PCI zone; tokens only)\n  - POST /v1/payments/authorize — authorize an amount against a payment token (no PAN in requests).\n  - POST /v1/payments/{payment_id}/capture — capture funds.\n  - POST /v1/payments/{payment_id}/void — void authorization.\n  - POST /v1/payments/refund — refund captured payment.\n\nStandard headers\n- X‑Correlation‑ID: trace across calls.\n- X‑Idempotency‑Key: deduplicate retried commands.\n- X‑API‑Version: optional minor version negotiation (major stays in URL).\n- Authorization: OAuth2/JWT; service account identity; mTLS enforced at transport.\n\nEvent interface standard\n- Topic naming and event types (one domain per topic; include version)\n  - catalog.price_changed.v1\n  - catalog.stock_reserved.v1, catalog.stock_released.v1\n  - cart.item_added.v1, cart.checkout_started.v1, cart.checked_out.v1, cart.checkout_failed.v1\n  - payment.authorized.v1, payment.captured.v1, payment.failed.v1, payment.voided.v1\n- Event envelope (all events)\n  - event_id (UUID), type, version, occurred_at (UTC), source (service name), correlation_id, causation_id, subject_id (e.g., cart_id/payment_id), partition_key (e.g., cart_id), data (domain fields).\n- Ordering and delivery\n  - At‑least‑once delivery; consumers must be idempotent.\n  - Partition by cart_id or payment_id for per‑entity ordering.\n- Payload hygiene\n  - No cardholder data; use payment_token references only.\n  - Minimal PII; avoid email/phone unless strictly required by the consumer.\n- Governance\n  - Schema registry with compatibility rules (additive changes only within v1).\n  - Deprecation policy with sunset dates; dual‑publishing during migrations.\n- Error handling\n  - Consumer DLQ per event type; replay procedures documented.\n  - Producer outbox pattern to avoid lost events on local transactions.\n\n3) Checkout workflow (recommended orchestrated flow)\n\n- Step 1 (REST): Client → Cart /checkout. Cart calculates totals (local), optionally revalidates price/stock via Catalog GET, then creates a stock reservation via Catalog POST /reservations.\n- Step 2 (REST): Cart → Payment authorize (POST /payments/authorize) with payment_token and amount; include idempotency‑key.\n- Step 3 (REST): If authorization succeeds, Cart → Catalog confirm reservation (POST /reservations/{id}/confirm), then Cart → Payment capture (POST /payments/{payment_id}/capture). If stock confirm fails, Cart → Payment void.\n- Step 4 (Events): Cart publishes cart.checked_out.v1 with correlation_id for downstream subscribers (email, analytics, fulfillment). Payment publishes payment.captured.v1; Catalog publishes catalog.stock_reserved.v1 / released.v1 as appropriate.\n\nAlternative choreography note:\n- If you prefer fewer REST calls after initial authorization, Cart can publish cart.checkout_started.v1 and let Catalog and Payment react. For PCI clarity and user experience, orchestration by Cart keeps the sensitive boundary and timely outcomes simpler.\n\n4) Non‑functional considerations (PCI DSS, 500 rps)\n\n- PCI DSS boundary\n  - Payment service in a segmented PCI zone; only tokenized data crosses boundaries.\n  - Encrypt in transit (mTLS) and at rest; strict access controls and audit logs.\n  - No PAN or sensitive auth data in logs, caches, or events.\n  - WAF/API gateway in front of payment endpoints; role‑based service accounts.\n- Throughput and latency\n  - 500 rps is moderate; size instances for p95 latency targets (e.g., 200–300 ms internal SLOs for REST calls).\n  - Use connection pooling, timeouts (e.g., 300–500 ms), retries with exponential backoff for transient errors, circuit breakers to isolate failures.\n  - Event broker sized for peaks (e.g., >2x peak throughput), with partitions keyed by cart_id.\n- Reliability\n  - Outbox pattern for event publishing tied to local DB commits.\n  - Idempotency keys for payment/checkout operations.\n  - DLQs and replay procedures; chaos testing for message duplication and partial failures.\n- Observability\n  - Distributed tracing using correlation_id; per‑service dashboards for latency, error rate, and queue lag.\n  - Business metrics: authorization success rate, stock reservation success rate, checkout completion rate.\n\n5) ASCII topology (hybrid)\n\n                      +------------------+\n                      |      Client      |\n                      +---------+--------+\n                                |\n                             [HTTPS]\n                                |\n                        +-------v-------+\n                        |  API Gateway  |\n                        +-------+-------+\n                                |\n                           mTLS/OAuth2\n                                |\n                        +-------v-------+\n                        |     Cart      |  Orchestrator for checkout\n                        +---+-------+---+\n                            |       |\n                   REST (sync)   REST (sync)\n                            |       |\n                +-----------v-+   +-v-------------------+\n                |   Catalog   |   |   Payment (PCI)     |\n                +------+------|   +----------+----------+\n                       |                     |\n                       |                     |\n               publish/subscribe       publish/subscribe\n                       |                     |\n                 +-----v---------------------v-----+\n                 |           Event Bus (PS)        |\n                 +---------------------------------+\n\nTypical subscriptions:\n- Cart subscribes to catalog.price_changed.v1 (updates cart totals).\n- Downstream services subscribe to cart.checked_out.v1 and payment.captured.v1 (notifications, fulfillment, analytics).\n- Catalog emits stock events; Cart reacts if using choreography or for reconciliation.\n\n6) Practical exercises to apply\n\n- Define your API style guide: resource naming, headers, error model, and versioning rules. Produce a 2‑page document and have all teams sign off.\n- Create an event catalog: list all event types, schemas, and topic names for v1; register them in a schema registry.\n- Implement idempotency policy: choose which operations require Idempotency‑Key; document server behavior for duplicate requests.\n- Draw a checkout sequence diagram (as‑is and to‑be) showing REST calls and events, including timeouts and fallback paths (stock fail, payment fail).\n- Map PCI data flows: identify where tokens vs PAN exist; confirm no sensitive data leaves the payment boundary via events or logs.\n\nClarifying questions\n- Do you plan to add an order/fulfillment service soon? If yes, we can refine event topics and checkout handoffs.\n- Which event broker are you standardizing on (e.g., Kafka, SNS/SQS, NATS)? That choice affects partitioning and delivery semantics we’ll tailor.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"score\":60,\"explanation\":\"Missing resources, layer IDs, acronyms\"}",
        "score": 60
      }
    },
    "compliance": "ok",
    "complianceText": "Detailed architecture guidance with integration insights, analogies, diagrams, exercises, clarification questions matches description.\nOK",
    "isGroundtruth": true
  },
  {
    "id": "2935928",
    "promptid": "4b51e23",
    "ruleid": 12,
    "rule": "The output must include programming language source code, command-line scripts, configuration files, and step-by-step coding instructions that together form a complete coding solution, including edge cases like bootstrapping dev environments.",
    "inverse": true,
    "testuid": "test-60865806",
    "scenario": "",
    "baseline": false,
    "testinput": "project_context: SaaS product analytics platform; layers: frontend dashboard, backend ingestion and query, data storage columnar DB; constraints: small team, need architecture only; request: outline architectural approach and risks; note: do not provide code, but mention dev environment bootstrapping considerations at a high level.",
    "model": "gpt-5-2025-08-07",
    "input": "project_context: SaaS product analytics platform; layers: frontend dashboard, backend ingestion and query, data storage columnar DB; constraints: small team, need architecture only; request: outline architectural approach and risks; note: do not provide code, but mention dev environment bootstrapping considerations at a high level.",
    "output": "Architectural approach for a small-team SaaS product analytics platform\n\n1) Goals and constraints\n- Deliver reliable event ingestion and fast analytical queries with minimal operational complexity.\n- Support multi-tenant SaaS (data isolation, access control, usage limits).\n- Optimize for a small team: fewer moving parts, clear boundaries, standard protocols, and easy local bootstrap.\n\n2) Recommended architectural style\n- Modular monolith for the backend with clear internal module boundaries, or a very small set of services (write path and read path). This reduces operational overhead while preserving separation of concerns.\n- Event-driven ingestion with a message queue to decouple collection from storage.\n- CQRS-inspired separation: ingestion (write path) and query (read path) are distinct modules/services.\n- Columnar warehouse as the system of record for analytics; optional raw event archive in object storage for backfills.\n\n3) High-level architecture (diagram described verbally)\n- Clients and SDKs (web/mobile/server) -> Ingestion API -> Message Queue/Stream -> Loader/Transformer -> Columnar DB (analytics tables)\n- Query API -> Columnar DB (SQL queries, pre-aggregations/materialized views) -> Result Cache (for hot queries) -> Frontend Dashboard\n- Admin/Control Plane: tenant management, schema registry, authentication/authorization, usage metering\n- Optional: Raw event archive in object storage for reprocessing\n\nThink of this like a sorting center: events arrive at the intake desk (Ingestion API), get placed on a conveyor (queue), are sorted and packed (transformer/loader), stored in organized shelves (columnar DB), and queried by staff using standardized pick lists (Query API).\n\n4) Module boundaries and communication\n- Ingestion API (REST/gRPC): accepts batched events from SDKs; responds quickly (validate, enqueue).\n- Queue/Stream (Kafka/Redpanda/Kinesis/PubSub): asynchronous handoff; backpressure control; at-least-once delivery.\n- Loader/Transformer: consumes from queue; applies normalization, enrichment, and idempotent dedup; writes to columnar DB; publishes operational metrics.\n- Query API (REST): translates dashboard requests to parameterized SQL; enforces tenant and role-based filters; returns paginated results; consults cache when applicable.\n- Frontend Dashboard: authenticated SPA; charting, segmentation, funnels; never talks to DB directly.\n\nStandardize event contracts with a schema (JSON/Avro/Protobuf), keep a registry, and version changes. Backwards-compatible, additive schema changes only where possible.\n\n5) Data model and storage strategy\n- Core event table with required columns: event_name, org_id (tenant), user_id/device_id, timestamp (UTC), properties (semi-structured), ingestion_id (for dedup).\n- Partition by time (e.g., daily) and order by org_id, timestamp; leverage columnar compression and TTLs for retention policy.\n- Pre-aggregation/materialized views for common queries (DAU/MAU, top events, funnels), updated incrementally.\n- Optional dimension tables (users, devices) to support joins while keeping hot paths simple.\n\n6) Ingestion pipeline details\n- At-least-once semantics; dedup by ingestion_id (client-generated UUID or server-side fingerprint).\n- Handle late and out-of-order events via watermarks; allow slight backfill windows.\n- Rate limiting and size limits per tenant; compress payloads (gzip).\n- Optional raw archive to object storage for compliance and reprocessing.\n\n7) Query service and analytics features\n- Parameterized query templates for common analyses: time-series, breakdowns, retention cohorts, funnels, segmentation.\n- Result caching for hot queries by key: org_id + query params + time window; short TTL plus cache bust on new data arrival.\n- Pre-aggregations/materialized views to bound response times for large tenants; schedule refresh aligned to ingestion.\n- Pagination and sampling for very large result sets to keep UI responsive.\n\n8) Multi-tenancy and security\n- Strong tenant isolation enforced at every boundary:\n  - AuthN: SSO or token-based per tenant.\n  - AuthZ: roles (admin, analyst, viewer) with permission checks in Query API.\n  - Data filtering: all queries must include org_id; consider row-level security features if the DB supports it.\n- PII handling: minimize, hash/anonymize where possible; encrypt sensitive fields at rest; configurable retention.\n- Audit logging: access and administrative actions; tenant-visible usage metrics.\n\n9) Observability and operations\n- Metrics: ingestion throughput, queue lag, loader latency, DB write throughput, query latency, cache hit rate, per-tenant quotas.\n- Tracing across ingestion and query path; logs with correlation IDs (ingestion_id, request_id).\n- Data quality monitors: duplicate rate, schema violations, late-event percentage.\n- Runbooks for incident response: queue backlog, hot partition, DB saturation.\n\n10) Scalability and performance patterns\n- Scale ingestion horizontally via queue consumers; increase partitions as needed.\n- Protect against hot shards (e.g., one large tenant or burst on a single event) by bucketing or composite sort keys.\n- Use columnar DB features: compression, appropriate index/order, materialized views.\n- Consider separating very large tenants (noisy neighbors) to dedicated tables or clusters as you grow.\n\n11) Dev environment bootstrapping (high level)\n- Single-machine Docker Compose:\n  - Columnar DB (e.g., ClickHouse) with seed schemas and fixtures.\n  - Lightweight queue (Redpanda/Kafka) with a topic for events.\n  - Ingestion and Query modules as containers; an API gateway or reverse proxy.\n  - Frontend dashboard container served locally.\n- Sample data generator to produce realistic event streams for test tenants.\n- Mock identity provider (local SSO) and test secrets via env vars.\n- Makefile or one-liner script to start/stop stack, reset data, run smoke tests.\n- Optional ephemeral environments per branch using the same compose file.\n- Documentation: quickstart, data contracts, common queries, troubleshooting.\n\n12) Key risks and mitigations\n- Data correctness (duplicates, late events, schema drift):\n  - Idempotency keys, dedup constraints, schema registry, contract tests, additive changes only.\n- Queue backlog and ingestion bottlenecks:\n  - Autoscale consumers, set topic partitions, enforce tenant rate limits, backpressure in SDKs.\n- Hot partitions/time skew in DB:\n  - Time bucketing, balanced sort keys, pre-aggregation, workload-aware scheduling.\n- Query performance variability:\n  - Caching, materialized views, param templates, guardrails (limit cardinality, paginate).\n- Multi-tenant noisy neighbor:\n  - Per-tenant quotas, query limits, cache isolation, dedicated resources for largest tenants.\n- Cost and vendor lock-in:\n  - Abstract data access in Query module, choose standard SQL-compatible engines, periodic cost reviews.\n- Security and privacy:\n  - Encryption, minimization of PII, audit trails, consistent org_id enforcement, data retention policies.\n- Operational load for small team:\n  - Keep service count low, use managed queue/DB where feasible, strong automation and IaC, clear runbooks.\n\n13) Phased roadmap\n- Phase 1 (MVP):\n  - Modular monolith backend with Ingestion and Query modules.\n  - Single columnar DB; basic schema; simple cache; core dashboards (events, time-series).\n  - Basic multi-tenancy and auth; local dev stack.\n- Phase 2 (Scale and depth):\n  - Queue decoupling if not in MVP, pre-aggregations, materialized views.\n  - Data quality monitors; admin features; usage metering.\n- Phase 3 (Enterprise readiness):\n  - Advanced tenant isolation options, SSO integrations, compliance features, dedicated resources for large tenants.\n\n14) Practical exercises for the team\n- Draw the end-to-end flow of a single “button_click” event from SDK to chart; list interfaces and contracts at each hop.\n- Define the event schema v1 and plan an additive change to v2; outline compatibility testing.\n- Run a local load test: 10k events/min for one tenant; measure queue lag and query latency; identify bottlenecks.\n- Create a pre-aggregation for DAU and benchmark dashboard latency with and without it.\n- Simulate a late-event scenario and verify correctness of retention charts.\n\nClarifications to tailor the plan\n- Which columnar DB do you prefer or already use (e.g., ClickHouse, BigQuery, Snowflake, Redshift)?\n- Expected ingestion volume and peak rates per tenant and overall?\n- Required analytics features for MVP (events, funnels, retention, segmentation)?\n- Compliance requirements (GDPR/CCPA) and data retention policies?\n- Do you plan to use managed services for queue/DB or self-host?\n\nOnce these are known, we can refine partitions, caching strategies, and tenant isolation choices to fit your targets and budget.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"score\":60,\"explanation\":\"No resource nor acronym definitions\"}",
        "score": 60
      }
    },
    "compliance": "ok",
    "complianceText": "Fits architecture focus: no code, clear, practical, uses analogy, asks clarifications.  \nOK",
    "isGroundtruth": true
  },
  {
    "id": "5d2e83a",
    "promptid": "4b51e23",
    "ruleid": 13,
    "rule": "When introducing a topic, the output skips foundational architectural principles and omits inter-module communication and standardization, assuming readers already know every interface, protocol, and convention—even proprietary or undocumented ones.",
    "inverse": true,
    "testuid": "test-89953371",
    "scenario": "",
    "baseline": false,
    "testinput": "audience: junior engineers; topic: introduce modular project architecture basics; request: explain foundational principles, inter-module communication patterns, and standardization practices like API versioning and schema governance; goal: start from fundamentals with simple language and analogies.",
    "model": "gpt-5-2025-08-07",
    "input": "audience: junior engineers; topic: introduce modular project architecture basics; request: explain foundational principles, inter-module communication patterns, and standardization practices like API versioning and schema governance; goal: start from fundamentals with simple language and analogies.",
    "output": "Here’s a beginner-friendly introduction to modular project architecture, focused on fundamentals, how modules talk to each other, and how to keep things consistent with versioning and schema governance.\n\n1) What is a “module” in a project\n- Think of a module like a shop on a street. Each shop has a clear job (e.g., bakery, pharmacy), a front door where customers interact (its interface/API), and a back room where it does the work (its implementation).\n- Core principles:\n  - Clear boundaries: Define what the module owns (data, responsibilities) and what it doesn’t.\n  - Single responsibility: One main job per module. Avoid “everything shop” modules.\n  - Stable interfaces, flexible internals: Change inside the module freely; keep the public contract consistent.\n  - High cohesion, low coupling: Functions inside a module belong together; modules depend on other modules only when necessary.\n  - Explicit contracts: Be precise about inputs, outputs, errors, and performance expectations.\n  - Dependency direction: Prefer depending on abstractions (interfaces) over concrete implementations.\n\n2) How modules communicate (patterns with everyday analogies)\n- Synchronous request/response (HTTP/REST)\n  - Analogy: A phone call—immediate answer expected.\n  - Use when: You need a quick result (e.g., “Get product details”).\n  - Watch out for: Latency, retries/timeouts, cascading failures. Use idempotent operations and clear error codes.\n\n- RPC (e.g., gRPC)\n  - Analogy: A walkie-talkie with specific commands—fast and binary, but tightly coupled.\n  - Use when: High-performance internal calls and strict contracts.\n  - Watch out for: Strong coupling to message formats; careful versioning required.\n\n- Asynchronous messaging/queues (e.g., SQS, RabbitMQ)\n  - Analogy: A to-do inbox—drop a task; the worker picks it up later.\n  - Use when: Work can be delayed or smoothed (e.g., sending emails, resizing images).\n  - Watch out for: Ordering, retries, dead-letter queues, duplicate messages (design for idempotency).\n\n- Event-driven pub/sub (e.g., Kafka)\n  - Analogy: A radio broadcast—one speaker, many listeners.\n  - Use when: Multiple services need to react to a business event (e.g., “OrderPlaced” triggers inventory update and notifications).\n  - Watch out for: Event schema evolution, consumer lag, exactly-once vs at-least-once delivery.\n\n- Batch/file-based integration\n  - Analogy: A nightly mail truck—bulk data delivered on a schedule.\n  - Use when: Large data transfers or integration with legacy systems.\n  - Watch out for: Data freshness, reconciliation, and failure recovery.\n\n3) Standardization practices (keeping teams aligned)\n- API design conventions\n  - Naming: Consistent resource names, verbs, and paths.\n  - Inputs/outputs: Use shared formats (e.g., JSON) with defined schemas.\n  - Errors: Standard error structure (code, message, details), clear retry guidance.\n  - Pagination, filtering, sorting: Consistent parameters across services.\n  - Security: Shared authentication/authorization approach; consistent token handling.\n\n- API versioning\n  - Goal: Allow change without breaking consumers.\n  - Compatibility rules:\n    - Backward-compatible changes (safe): Add fields, add optional endpoints; don’t change meanings.\n    - Breaking changes: Remove fields, change types/semantics; require a new version.\n  - Versioning strategies:\n    - Path versioning: /v1/orders, /v2/orders (simple and visible).\n    - Header or content negotiation: Clients request versions via headers (flexible, but harder to discover).\n  - Lifecycle:\n    - Document changes and migration steps.\n    - Mark deprecations clearly; provide timelines.\n    - Support multiple versions concurrently for a transition period.\n    - Communicate “sunset” dates and monitor usage per version.\n\n- Schema governance (data contracts for APIs/events)\n  - Use a schema registry (for JSON Schema, Avro, Protobuf) so producers/consumers share the same contract.\n  - Evolution rules:\n    - Prefer additive changes (add optional fields).\n    - Avoid removals or type changes; if needed, create a new version.\n    - Maintain backward compatibility for a defined period.\n  - Process:\n    - Change proposals and reviews (architecture or data council).\n    - Automated validation in CI/CD (reject incompatible schemas).\n    - Consumer impact analysis before releasing changes.\n  - Documentation:\n    - Own the schema per module.\n    - Include examples, constraints, and event semantics (what does “OrderPlaced” mean?).\n\n4) Putting it together: a simple layered project\n- Frontend Application: Web/mobile app that calls backend APIs. It should know only public contracts, not internal details.\n- Backend Services (modules), example:\n  - Auth service: Identity, tokens.\n  - Catalog service: Products, prices.\n  - Order service: Cart, checkout, order creation.\n  - Payment service: Charge cards.\n- Data Storage: Each service owns its data (avoid shared writable databases).\n- Typical flow:\n  - The UI calls Order service (sync) to create an order.\n  - Order service calls Payment (sync) for authorization.\n  - After success, Order service publishes an “OrderPlaced” event (async).\n  - Inventory and Notification services consume the event to update stock and send emails.\n- Benefits:\n  - Clear responsibilities, fewer unintended side effects.\n  - Mix of sync (for immediate results) and async (for scalability and decoupling).\n\n5) Practical exercises to build intuition\n- Boundary mapping: Pick a small project and list modules, their responsibilities, and which data each owns. Identify anything that looks like an “everything shop” and split it.\n- Contract first: Define an API for “CreateOrder”—inputs, outputs, error model, idempotency key, and performance expectations (e.g., p95 latency).\n- Version a change: Add a new optional field (e.g., discountCode) and decide how clients discover it; write a deprecation note for any old behavior you plan to remove.\n- Event design: Draft an “OrderPlaced” event schema with clear meanings for each field. Identify at least two consumers and what they do.\n- Communication choice: For three interactions (get product details, charge payment, send confirmation email), choose sync vs async and explain the trade-offs.\n\n6) Simple checklist for healthy modular architecture\n- Do we have clear module boundaries and data ownership?\n- Are interfaces documented and consistent across services?\n- Are we using a standard error model, pagination, and security approach?\n- Do we have a versioning policy and deprecation timelines?\n- Are schemas validated and stored in a registry with review gates?\n- Do we mix sync and async wisely to balance responsiveness and resilience?\n- Are modules observable (logs, metrics, traces) and tested via contract/integration tests?\n\nKey takeaways\n- Treat module interfaces like promises to other teams: keep them clear, stable, and well-documented.\n- Choose communication patterns based on business needs: “phone call” for immediate answers; “radio broadcast” for many listeners; “to-do inbox” for background work.\n- Standardization (versioning and schema governance) is how you move fast without breaking others.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"score\":60,\"explanation\":\"Missing styles, resource, diagram\"}",
        "score": 60
      }
    },
    "compliance": "ok",
    "complianceText": "Clear, analogy-based modular architecture guide, covers basics, integration, practical exercises, layering, standardization.\nOK",
    "isGroundtruth": true
  },
  {
    "id": "25b471c",
    "promptid": "4b51e23",
    "ruleid": 14,
    "rule": "The output does not describe how modules integrate or communicate and provides no concrete example or case, even when the question asks for a specific integration scenario.",
    "inverse": true,
    "testuid": "test-23790292",
    "scenario": "",
    "baseline": false,
    "testinput": "project_context: Single-page app with React frontend, Node.js backend, Postgres DB; request: show a concrete end-to-end example of how modules integrate and communicate including HTTP paths, headers, an event emission, and a SQL query; ask: include at least one concrete case.",
    "model": "gpt-5-2025-08-07",
    "input": "project_context: Single-page app with React frontend, Node.js backend, Postgres DB; request: show a concrete end-to-end example of how modules integrate and communicate including HTTP paths, headers, an event emission, and a SQL query; ask: include at least one concrete case.",
    "output": "Below is a concrete, end-to-end example that traces a single use case across modules in a React SPA, a Node.js backend, and a Postgres database. It shows the HTTP path, headers, the event emitted, and the SQL query, so you can see how each layer collaborates. Think of it like a relay race: the baton (data + intent) is passed cleanly from the UI to the API, to the database, and then to downstream listeners.\n\nUse case: Create a new task\n\nHigh-level module map\n- Frontend (React): TaskForm component ➝ API Client module ➝ State/Store\n- Backend (Node): HTTP Router/Controller ➝ Auth & Validation middleware ➝ TaskService ➝ TaskRepository ➝ Event Bus (with Outbox)\n- Data (Postgres): tasks table ➝ outbox_events table ➝ audit_logs table\n- Event Consumers: Notification service, Audit logger, Real-time gateway (WebSocket)\n\nFlow, step by step\n1) User action in the SPA\n- Module: TaskForm (React)\n- Action: User clicks “Save” for a new task.\n- The TaskForm calls the API Client module with the form data and passes a correlation ID (helps trace the request through the system).\n\n2) HTTP request from frontend to backend\n- Path: POST /api/tasks\n- Headers:\n  - Authorization: Bearer <jwt>\n  - Content-Type: application/json\n  - Accept: application/json\n  - X-Request-ID: 7f2d8c1b-4a9b-4b28-8d2e-1b8d12345678\n- Body:\n  - { \"title\": \"Prepare quarterly report\", \"description\": \"Collect KPIs and draft summary\", \"dueDate\": \"2025-10-15\" }\n\n3) Backend request handling pipeline\n- Module: HTTP Router/Controller\n  - Matches POST /api/tasks route.\n  - Attaches the X-Request-ID to the logger context for traceability.\n- Module: Auth middleware\n  - Verifies JWT, extracts user_id (e.g., 42).\n- Module: Validation middleware\n  - Validates body fields (title required, dueDate format, max lengths).\n- Module: TaskService\n  - Starts a DB transaction (so writes and the event enqueue happen atomically).\n  - Calls TaskRepository to write the task.\n- Module: TaskRepository (Postgres)\n  - Executes SQL to create the task row, returning the created record:\n  - Example SQL: INSERT INTO tasks (id, user_id, title, description, status, due_date, created_at) VALUES ($1, $2, $3, $4, 'open', $5, NOW());\n  - Also writes a domain event into an outbox table (to guarantee the event is captured even if a downstream broker is temporarily unavailable):\n  - Example outbox SQL: INSERT INTO outbox_events (id, event_type, payload_json, occurred_at) VALUES ($1, 'TaskCreated', '{\"taskId\":\"<uuid>\",\"userId\":42,\"title\":\"Prepare quarterly report\"}', NOW());\n- Module: TaskService (commit)\n  - Commits the transaction so both the task and its outbox event are durable.\n\n4) Event emission (post-commit)\n- Module: Event Dispatcher (Outbox Processor)\n  - Polls outbox_events, publishes a “TaskCreated” event to the internal event bus (could be Node’s EventEmitter, a message broker, or a pub/sub library).\n- Event details:\n  - Type: TaskCreated\n  - Headers/metadata: x-request-id=7f2d8c1b-4a9b-4b28-8d2e-1b8d12345678, trace-id=…\n  - Payload: { \"taskId\": \"<uuid>\", \"userId\": 42, \"title\": \"Prepare quarterly report\", \"dueDate\": \"2025-10-15\", \"status\": \"open\", \"occurredAt\": \"2025-09-26T12:34:56Z\" }\n\n5) Backend HTTP response to the SPA\n- Status: 201 Created\n- Headers:\n  - Location: /api/tasks/<uuid>\n  - Content-Type: application/json\n  - X-Request-ID: 7f2d8c1b-4a9b-4b28-8d2e-1b8d12345678\n  - Cache-Control: no-store\n- Body:\n  - { \"id\": \"<uuid>\", \"title\": \"Prepare quarterly report\", \"description\": \"Collect KPIs and draft summary\", \"status\": \"open\", \"dueDate\": \"2025-10-15\", \"createdAt\": \"2025-09-26T12:34:56Z\" }\n\n6) Event consumers react to TaskCreated\n- Module: Notification service\n  - Listens to TaskCreated, sends a “Task created” WebSocket push to the user’s active sessions.\n  - If using WebSockets/SSE, it may attach x-request-id so the client can correlate messages with the originating action.\n- Module: Audit logger\n  - Writes an audit row so admins can reconstruct the action history.\n  - Example SQL: INSERT INTO audit_logs (id, actor_user_id, action, resource_type, resource_id, occurred_at, request_id) VALUES ($1, 42, 'TASK_CREATED', 'task', '<uuid>', NOW(), '7f2d8c1b-4a9b-4b28-8d2e-1b8d12345678');\n\n7) Frontend state update\n- Module: API Client\n  - Receives the 201 response, normalizes the task into the store.\n- Module: Store/State\n  - Adds the new task, triggers UI update in TaskList and any open dashboards.\n\nA quick diagram (modules and message flow)\nReact TaskForm ➝ API Client ➝ POST /api/tasks (Node Router → Auth → Validation → TaskService → TaskRepository → Postgres)\nAfter commit: Outbox Processor ➝ Event Bus emits TaskCreated ➝ Consumers (Notifications, Audit, Real-time gateway)\nBack to client: 201 Created + Location + body ➝ Store updates ➝ UI refresh\n\nWhat to notice about integration and communication\n- Boundaries:\n  - Frontend communicates only via HTTP (clear API paths).\n  - Backend manages business logic and coordinates DB writes and event publishing (outbox pattern prevents lost events).\n  - Consumers are decoupled via events (they don’t call TaskService directly).\n- Standardization:\n  - Consistent headers (Authorization, Content-Type, X-Request-ID) across modules help tracing and security.\n  - API paths follow resource-oriented naming (/api/tasks).\n- Transactionality:\n  - The task write and the event enqueue occur in one DB transaction, ensuring the event matches the committed data.\n- Observability:\n  - X-Request-ID persists from UI to DB audit, making cross-layer debugging practical.\n\nOptional second concrete case (status change)\n- Path: PATCH /api/tasks/<uuid>\n- Headers: Authorization: Bearer <jwt>, Content-Type: application/json, If-Match: \"<etag>\"\n- Body: { \"status\": \"done\" }\n- Repository SQL (optimistic concurrency with version or etag): UPDATE tasks SET status='done', version=version+1 WHERE id=$1 AND version=$2;\n- Event: TaskCompleted with payload { taskId, userId, occurredAt } and correlation headers.\n- Response: 200 OK, ETag: \"<new-etag>\"\n\nIf you want, I can adapt this pattern to your actual module names and any middleware or libraries you’re already using (e.g., Fastify vs Express, Redux vs React Query), and sketch a tailored diagram for your repo structure.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"score\":50,\"explanation\":\"Includes SQL code, no exercises.\"}",
        "score": 50
      }
    },
    "compliance": "ok",
    "complianceText": "Detailed, clear architectural walkthrough with modules, integration, analogy, no forbidden coding solutions.  \nOK",
    "isGroundtruth": true
  },
  {
    "id": "0c9c4e1",
    "promptid": "4b51e23",
    "ruleid": 15,
    "rule": "Where architectural choices are relevant, the output names at most one style (or declares a single \"best\" style) and refuses to discuss strengths, trade-offs, or fit criteria across contexts.",
    "inverse": true,
    "testuid": "test-18247307",
    "scenario": "",
    "baseline": false,
    "testinput": "project_context: Fintech wallet with KYC, fraud checks, ledger, support; constraints: low latency, consistency needs, compliance; request: compare layered, microservices, event-driven, and serverless architectures with trade-offs, strengths, and fit criteria.",
    "model": "gpt-5-2025-08-07",
    "input": "project_context: Fintech wallet with KYC, fraud checks, ledger, support; constraints: low latency, consistency needs, compliance; request: compare layered, microservices, event-driven, and serverless architectures with trade-offs, strengths, and fit criteria.",
    "output": "Context recap\n- Domain: Fintech wallet with KYC, fraud checks, a ledger, and customer support.\n- Constraints: low latency on the “pay” path, strict consistency for balances and transactions, strong compliance and auditability.\n\nBefore we compare, a few universal must-haves in this domain:\n- Strong consistency for the core ledger (ACID, idempotency, tamper-evident audit logs).\n- Clear data boundaries: PII and payment data separation, encryption at rest/in transit, least-privilege access, key management, audit trails.\n- Synchronous low-latency hot path for balance check and payment authorization; asynchronous for everything else where possible (KYC verification, fraud enrichment, notifications).\n- Standardized inter-module contracts: request/response schemas, event schemas, error codes, idempotency keys, tracing propagation.\n\nComparison by architecture style\n\n1) Layered (N‑tier, modular monolith)\n- What it is: A single deployable application organized in layers (UI/API, domain/services, persistence). Think of one building with well-defined floors.\n- Strengths\n  - Fast intra-process calls → low latency for the hot path.\n  - Easier to keep strict consistency (single transaction boundary).\n  - Simpler governance and compliance: one codebase, one changelog, centralized role-based access.\n  - Straightforward debugging and tracing.\n- Trade-offs\n  - Scaling and change isolation are coarse-grained; one hotspot can force scaling the whole app.\n  - Release cadence tied together; harder to adopt different tech per module.\n  - If not carefully modularized, can drift into tight coupling.\n- Latency & consistency fit\n  - Excellent for the ledger and wallet authorization path (checking balance, creating a transaction).\n  - Fits synchronous needs well; avoids network latency.\n- Compliance implications\n  - Fewer moving parts; easier audit, but requires strict internal modular boundaries to segregate PII and financial data.\n- Example fit for this wallet\n  - Ledger and wallet: inside the same process with transactional DB (e.g., PostgreSQL) for ACID writes.\n  - KYC and fraud: either internal modules calling external providers synchronously for critical checks or via async jobs for enrichment.\n  - Support: separate UI component in the same app consuming data via domain services.\n- When to choose\n  - Early to mid-stage, small to medium team.\n  - High consistency and low latency are paramount.\n  - Limited need for independent scaling per module.\n- Integration patterns to standardize\n  - Domain modules with clear interfaces; outbox pattern for emitting events to other systems.\n  - API Gateway in front; background workers for KYC/fraud enrichment.\n\n2) Microservices\n- What it is: Independent services per bounded context (Ledger, Wallet, KYC, Fraud, Support), each with its own data store. Think of a campus of specialized buildings connected by roads.\n- Strengths\n  - Independent scaling and deployments; teams own services.\n  - Tech diversity and isolation of failure domains.\n  - Clear data boundaries helpful for compliance (e.g., PII isolated in KYC service).\n- Trade-offs\n  - Distributed consistency challenges; sagas/outbox needed for cross-service workflows.\n  - Added network latency and more complex retries/timeouts.\n  - Higher operational overhead (service mesh, observability, security).\n- Latency & consistency fit\n  - Hot path must minimize cross-service hops. Keep wallet authorization + ledger operations within one or two services.\n  - Ledger stays strongly consistent in its own store; expose side-effect-free reads for others.\n- Compliance implications\n  - More surfaces to secure and audit, but stricter boundaries possible (separate IAM, VPCs, data policies).\n- Example fit for this wallet\n  - Ledger Service: ACID DB, strict controls; emits events post-commit.\n  - Wallet Service: orchestrates payments; calls Ledger synchronously for commits.\n  - Fraud Service: consumes transaction events, scores in near real-time; provides a risk decision cache.\n  - KYC Service: workflow engine integrating third-party providers; stores PII separately.\n  - Support Service: reads via APIs or read models; no direct access to PII or ledger tables.\n- When to choose\n  - Larger team, high growth, need independent scaling or global footprint.\n  - Mature DevOps, observability, and security posture.\n  - Product velocity with frequent module-level changes.\n- Integration patterns to standardize\n  - API Gateway, mTLS between services, consistent idempotency semantics.\n  - Outbox + event bus for reliable event publication.\n  - Saga for multi-step business flows where needed, but keep the payment commit local to Ledger.\n\n3) Event-driven (streaming backbone)\n- What it is: Services communicate via events on a durable log (e.g., Kafka). Think of a city’s bus network for messages: producers publish, many consumers listen.\n- Strengths\n  - Decoupling and scalability; easy to add new consumers (e.g., analytics, notifications).\n  - Naturally audit-friendly (immutable event log).\n  - Real-time fraud scoring, enrichment, and back-pressure handling.\n- Trade-offs\n  - Eventual consistency; careful design for ordering and idempotency.\n  - Operational complexity (schema evolution, retention, reprocessing).\n  - Not all workflows fit async; hot path still needs synchronous components.\n- Latency & consistency fit\n  - Great for fraud, KYC, notifications, and read-model projections.\n  - Use events as audit for ledger, but keep the ledger’s transactional commit synchronous in the service/database.\n- Compliance implications\n  - Clear audit trails; must manage PII in streams (encryption, topic ACLs, retention policies, data minimization).\n- Example fit for this wallet\n  - “TransactionCommitted” events from Ledger feed Fraud, Analytics, and Support read models.\n  - KYC emits “KycVerified” events; Wallet listens to enforce business rules.\n  - Notification service consumes events to inform users without blocking the hot path.\n- When to choose\n  - Need high extensibility and real-time insights.\n  - Desire to separate write path (strict) from read/side-effects (flexible) via CQRS.\n- Integration patterns to standardize\n  - Schema registry, versioning, consumer groups, idempotent consumers.\n  - Outbox pattern to ensure events exactly match committed DB changes.\n  - Dead letter queues and replay strategies with guardrails.\n\n4) Serverless\n- What it is: Functions-as-a-Service and managed services for bursty or peripheral workloads. Think of renting pop-up kiosks only when needed.\n- Strengths\n  - Minimal ops, auto-scaling, pay-per-use; good for spiky traffic and background processing.\n  - Fast iteration for edge handlers (document OCR, webhooks, notifications).\n- Trade-offs\n  - Cold starts and network egress can impact latency.\n  - Limited execution time and concurrency constraints for high-throughput transactions.\n  - Harder to enforce strong transactional boundaries across functions.\n  - Vendor lock-in and compliance boundaries depend on cloud capabilities.\n- Latency & consistency fit\n  - Not ideal for the core payment/ledger hot path unless provisioned concurrency and VPC tuning mitigate latency.\n  - Good for KYC document processing, fraud feature extraction, notifications, and support automation.\n- Compliance implications\n  - Rely on cloud controls; need careful VPC isolation, secrets/KMS integration, audit/log shipping, and policy-as-code.\n- Example fit for this wallet\n  - KYC pipeline: image processing, liveness checks via serverless functions triggered by uploads.\n  - Fraud: feature extraction functions fed by events; model execution may run on a managed service.\n  - Support: workflow automation, scheduled jobs, email/SMS sending.\n- When to choose\n  - Small team, non-hot-path workloads, unpredictable spikes.\n  - Desire to offload ops while keeping strict parts (ledger) on managed DB/compute.\n- Integration patterns to standardize\n  - Event triggers, retries with DLQs, idempotency keys.\n  - Provisioned concurrency for low-latency endpoints if needed.\n\nFit criteria checklist\n- Transaction profile: peak TPS on wallet authorization; latency SLA (e.g., p95 < 100 ms).\n- Consistency needs: ledger must be strictly consistent; cross-module workflows can be eventual.\n- Compliance scope: PCI DSS, SOC2, GDPR/CCPA—data residency, PII segregation, audit requirements.\n- Team and ops maturity: observability, on-call, CI/CD, infra as code, security automation.\n- Scale and geography: single region vs multi-region active-active; network latency between services.\n- Product volatility: how often modules change independently.\n- Budget and tooling: cost of platform, service mesh, streaming backbone, managed services.\n\nPutting it together: typical hybrid for fintech wallets\n- Core hot path: Layered or a small number of microservices with synchronous APIs\n  - API Gateway → Wallet Service → Ledger Service → ACID DB\n  - Caching only for reads with careful invalidation; writes go to the ledger.\n- Side effects and insights: Event-driven\n  - Outbox publishes “TransactionCommitted”, “BalanceUpdated”, “KycVerified”.\n  - Fraud, Analytics, Notifications, and Support consume these events.\n- Serverless for bursty/peripheral tasks\n  - KYC document processing, webhook handlers, scheduled compliance exports.\n- Data storage layering\n  - Ledger: relational DB with strict constraints, audit tables, and secure key management.\n  - Read models: separate stores (e.g., Elastic/Redis) built from events for support dashboards and user notifications.\n  - PII: isolated store with tighter IAM; access via KYC service only.\n\nInter-module communication standards\n- Synchronous contracts: clearly versioned APIs (HTTP/gRPC), standard error codes, idempotency for payment requests.\n- Asynchronous contracts: schema-registry-managed event schemas, compatibility rules, consumer lag monitoring.\n- Security: mTLS between services, OAuth2/JWT for client/API, policy enforcement (OPA), secrets rotation.\n- Observability: trace propagation headers, consistent logging, metrics per module, audit logs for all data accesses.\n\nTrade-off summary in plain terms\n- Layered: fastest and most consistent for the core; simpler to govern. Less flexible at scale.\n- Microservices: flexible and scalable with team autonomy; complexity grows, must tame distributed consistency and latency.\n- Event-driven: great for decoupling and real-time features; accept eventual consistency and invest in reliable event engineering.\n- Serverless: efficient for non-core, spiky tasks; mind cold starts and compliance. Don’t put the vault (ledger) in a kiosk.\n\nPractical exercises\n- Map bounded contexts: identify Ledger, Wallet, KYC, Fraud, Support domains and list what data each owns; draw their APIs and events.\n- Hot path call graph: sketch the exact synchronous steps from “Pay” request to ledger commit; label latency budgets per hop.\n- Consistency plan: mark which flows must be strongly consistent vs eventual; define idempotency keys and retry policies.\n- Data classification: categorize data (PII, financial, operational) and decide storage + access boundaries.\n- Event catalog: design the core events (names, schemas, versioning rules) and the outbox integration from Ledger.\n- Failure drills: simulate service/network failure; define fallbacks (e.g., fraud timeouts default policy, KYC pending state).\n- Read model design: choose what support needs (tickets + transaction views) and how events populate those read stores.\n\nOptional clarifications to refine recommendations\n- Expected peak TPS and latency SLA for payments.\n- Compliance regimes (PCI DSS, SOC2, GDPR) and data residency requirements.\n- Team size and platform maturity (service mesh, streaming, serverless experience).\n- Deployment model (single vs multi-region, active-active needs).\n\nIf you share those, I can tailor the hybrid and module boundaries more precisely to your context.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"score\":80,\"explanation\":\"Missing resource recommendation and diagram.\"}",
        "score": 80
      }
    },
    "compliance": "ok",
    "complianceText": "Provides styles, trade-offs, exercises, analogies, clarifications, professional tone; aligns with architecture guide roles.\nOK",
    "isGroundtruth": true
  },
  {
    "id": "7c4b40c",
    "promptid": "4b51e23",
    "ruleid": 16,
    "rule": "When discussing architectural styles, the output deliberately provides no resources or references—no titles, authors, or URLs—even if the user requests citations.",
    "inverse": true,
    "testuid": "test-13180945",
    "scenario": "",
    "baseline": false,
    "testinput": "topic: CQRS vs CRUD for order management; request: discuss suitability across contexts and provide at least three authoritative references with titles and URLs for further learning.",
    "model": "gpt-5-2025-08-07",
    "input": "topic: CQRS vs CRUD for order management; request: discuss suitability across contexts and provide at least three authoritative references with titles and URLs for further learning.",
    "output": "Short answer\n- CRUD is a good fit for straightforward order management in a single application when workflows are simple, consistency must be immediate, and the team wants low operational overhead.\n- CQRS shines when reads and writes have very different requirements, the domain has complex workflows and integrations (payments, inventory, shipping), and you need scalable, flexible read models, auditability, and resilience through asynchronous processing. It adds architectural complexity that pays off at scale or complexity, but can be overkill for small, simple systems.\n\nClear definitions\n- CRUD: A conventional, layered architecture where the same data model and service handles create, read, update, delete operations. Simple to build and operate; immediate consistency; fewer moving parts.\n- CQRS (Command Query Responsibility Segregation): Separate models and paths for writes (commands that change state) and reads (queries that do not). Often paired with messaging and projections; reads may be eventually consistent; better scalability and tailored read models; higher complexity and operational demands.\n\nHow to choose (decision criteria)\n- Domain complexity: If order states and transitions are simple (e.g., New → Paid → Shipped), CRUD is often enough. If you have many states, escalations, split shipments, partial returns, fraud checks, and cross-service orchestration, CQRS becomes attractive.\n- Read/write ratio and query diversity: If reads dominate and you need many different query shapes (dashboards, filtered order lists, real-time status), CQRS enables purpose-built read models and projections. CRUD can struggle or lead to ad-hoc, performance-focused queries sprinkled across the codebase.\n- Consistency needs: If business rules demand immediate, transactional consistency across operations (e.g., reserve inventory and charge card atomically within one database), CRUD is simpler. If eventual consistency is acceptable (inventory updates, notification emails, shipping labels processed asynchronously), CQRS and messaging align well.\n- Scale and performance: At high throughput (e.g., marketplaces, flash sales), separating write pressure from read workloads and scaling read replicas or distinct stores is easier with CQRS. CRUD can scale too, but often hits limits around complex queries or hot tables.\n- Auditability and compliance: CQRS, often with event sourcing, gives a strong audit trail of order changes. CRUD typically relies on change logs and triggers; adequate for many cases, but less rich for forensic analysis.\n- Team and operational maturity: CQRS introduces messaging, projections, monitoring for eventual consistency, and more failure modes. If the team or ops tooling is not ready, CRUD reduces risk.\n- Architecture context: In a monolith or single-service system, CRUD fits naturally. In a microservices, event-driven ecosystem (payments, inventory, shipping as separate services), CQRS aligns well with boundaries and asynchronous integration.\n\nScenarios for order management\n- Small retail site or startup:\n  - Suitability: CRUD is best. One database, one service, immediate consistency, simple order lifecycle.\n  - Rationale: Minimizes complexity and cost; easier to iterate.\n- Growing e-commerce with multiple channels and reporting:\n  - Suitability: Hybrid approach. Keep CRUD for writes, introduce read-side projections and caching for dashboards and order searches.\n  - Rationale: You get many CQRS benefits on the read side without fully splitting write models.\n- High-scale marketplace with complex workflows and integrations:\n  - Suitability: Full CQRS. Commands validated by a write model; events drive projections to materialized views for queries; sagas coordinate payments, inventory, shipping.\n  - Rationale: Read scalability, resilience to failures, flexible query models.\n- Regulated or highly audited environments:\n  - Suitability: CQRS + event sourcing for traceability of every state change, with projections for operational reads.\n  - Rationale: Strong audit, replay, and compliance capabilities.\n\nIntegration overview (how modules interact)\nThink of a restaurant:\n- Commands are orders sent to the kitchen (change state).\n- Queries are diners reading the menu or checking order status (no state change).\nA typical CQRS flow for orders:\n- Frontend Application → API Gateway\n- Command path:\n  - Command/Order Service (validates business rules)\n  - Message broker publishes domain events (e.g., OrderPlaced, PaymentAuthorized)\n  - Write model database persists state changes\n  - Sagas/orchestrators trigger integrations (payments, inventory, shipping)\n- Read path:\n  - Projection workers consume events and update read models (e.g., OrderList, OrderDetails views)\n  - Query Service serves low-latency queries from read-optimized stores (SQL views, NoSQL, cache)\n\nWith CRUD, the same service and schema handle both paths; projections and messaging are optional and usually minimal.\n\nTrade-offs to keep in mind\n- CQRS pros: Scalable reads, tailored schemas per query, strong auditing, decoupled integrations, better fit for event-driven ecosystems.\n- CQRS cons: More moving parts (brokers, projections, multiple data stores), eventual consistency to manage, complex testing and monitoring, higher cognitive load.\n- CRUD pros: Simplicity, transactional consistency, straightforward debugging, easier onboarding.\n- CRUD cons: Can become rigid under complex query needs, harder to scale diverse reads, risk of bloated entities and services.\n\nPractical exercises to apply the concepts\n- Map the order lifecycle and invariants: List every state and business rule (e.g., “Cannot ship before payment capture,” “Hold order if AVS mismatch”).\n- Separate read concerns: Identify top queries (order search, dashboards, customer history). Decide if a read model or materialized view would simplify them.\n- Define commands and events: Draft commands (PlaceOrder, AuthorizePayment, ShipOrder) and events (OrderPlaced, PaymentAuthorized, OrderShipped). Clarify which modules publish/consume.\n- Consistency boundaries: Decide which actions must be transactional (within one service) and which are acceptable as eventually consistent (cross-service).\n- Failure scenarios: Plan how you detect and recover from message retries, projection lag, and partial failures (e.g., payment success but inventory reservation failure).\n\nRecommendations by context\n- If you are early-stage or your order flow is uncomplicated: Start with CRUD. Add indexes, pragmatic caching, and a few materialized views for heavy queries.\n- If your reads are performance-critical and varied: Introduce read-side projections (partial CQRS). Keep writes transactional in one service.\n- If you operate at scale with many integrations and complex workflows: Adopt CQRS, likely with messaging and sagas. Consider event sourcing for auditability.\n- If regulatory audits are strict: CQRS with event sourcing and well-defined projections is usually worth the complexity.\n\nAuthoritative references for further learning\n- Martin Fowler — CQRS\n  - https://martinfowler.com/bliki/CQRS.html\n- Microsoft Azure Architecture Center — Command and Query Responsibility Segregation (CQRS) pattern\n  - https://learn.microsoft.com/en-us/azure/architecture/patterns/cqrs\n- Udi Dahan — Clarified CQRS\n  - https://udidahan.com/2009/12/09/clarified-cqrs/\n- Chris Richardson — Microservices Patterns: CQRS\n  - https://microservices.io/patterns/data/cqrs.html\n- Greg Young — CQRS (InfoQ talk)\n  - https://www.infoq.com/presentations/cqrs\n\nIf you share your expected order volume, number of integrations (payments, inventory, shipping), and any hard consistency or audit requirements, I can tailor a concrete architectural recommendation and a migration path.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"explanation\":\"Missing explicit standardization details\",\"score\":90}",
        "score": 90
      }
    },
    "compliance": "ok",
    "complianceText": "Detailed architectural advice with examples, exercises, analogies, no code – matches guide description.\nOK",
    "isGroundtruth": true
  },
  {
    "id": "5437dac",
    "promptid": "4b51e23",
    "ruleid": 17,
    "rule": "In project analysis, the output avoids identifying Frontend, Backend, or Data Storage layers, offers no responsibilities or interactions, and may collapse everything into an undifferentiated \"monolith\" box.",
    "inverse": true,
    "testuid": "test-57242482",
    "scenario": "",
    "baseline": false,
    "testinput": "repo_structure: frontend/ (React), backend/ (NestJS), infra/ (Terraform), db/ (migrations SQL); README_excerpt: describes API routes and DB schemas; request: analyze the architecture by identifying frontend, backend, and data storage layers and how they interact in request flows.",
    "model": "gpt-5-2025-08-07",
    "input": "repo_structure: frontend/ (React), backend/ (NestJS), infra/ (Terraform), db/ (migrations SQL); README_excerpt: describes API routes and DB schemas; request: analyze the architecture by identifying frontend, backend, and data storage layers and how they interact in request flows.",
    "output": "Here’s a clear picture of your project’s architecture and how the layers talk to each other during a request.\n\nLayers and responsibilities\n- Frontend (frontend/ – React): The user interface. It collects user input, renders data, and calls backend APIs over HTTP. Think of it as the “front desk” where requests are made.\n- Backend (backend/ – NestJS): The application logic and API. It validates requests, enforces rules, orchestrates work, and reads/writes data. This is the “dispatcher,” deciding what happens with each request.\n- Data storage (db/ – SQL migrations): The relational database schema and changes over time. This is the “warehouse” where information is stored and organized.\n- Infrastructure (infra/ – Terraform): Cloud resources and wiring (servers, databases, networking, secrets). This is the “building” and “utilities” ensuring everything is reachable, secure, and scalable.\n\nHow they interact (typical request flows)\nOverview diagram:\nBrowser → React UI → HTTP fetch → NestJS Controller → Service → Data Access → Database\nInfrastructure supports the path: CDN/static hosting, load balancer/API gateway, VPC/network, DB instance, secrets.\n\n1) Read data (GET)\n- User action: A React component mounts and calls fetch('/api/widgets?filter=…').\n- Frontend: Sends an HTTP GET with headers (auth token, content type).\n- Backend Controller: Matches route (e.g., GET /widgets), parses query params, runs validation.\n- Service layer: Applies business rules (e.g., filter allowed fields).\n- Data access: Runs a SELECT against the database. Given you have SQL migrations, the app likely uses an ORM (e.g., TypeORM/Prisma) or a query builder to map tables defined in migrations to entities.\n- Response: Service returns DTOs; Controller sends JSON back.\n- Frontend: Renders the list, handles loading/error states.\n\n2) Write data (POST/PUT/DELETE)\n- User action: Submits a form in React to create or update a record.\n- Frontend: Sends POST /widgets with JSON body.\n- Backend Controller: Validates payload against a DTO; returns 400 on invalid shape.\n- Service: Checks business rules (e.g., uniqueness, permissions).\n- Data access: Executes INSERT/UPDATE/DELETE with a transaction if multiple steps are involved.\n- Response: Returns created/updated resource; frontend updates UI optimistically or after confirmation.\n\n3) Authentication flow (common pattern)\n- Frontend: Submits credentials to POST /auth/login.\n- Backend: Verifies against users table, issues a JWT or session token.\n- Frontend: Stores token securely (memory or httpOnly cookie) and includes it in subsequent calls.\n- Backend: Guards (NestJS) check token on protected routes; reject unauthorized requests early.\n\nHow the repository structure supports the flows\n- frontend/ (React): Components encapsulate views; API client module centralizes HTTP calls and shared headers; types mirror backend DTOs to keep contracts consistent.\n- backend/ (NestJS): \n  - Controllers define the API routes described in the README.\n  - Services encapsulate business logic independent of transport.\n  - Data access layer (repositories) isolates database queries and maps DB schemas to code models.\n  - Pipes/Guards/Interceptors handle validation, auth, and cross-cutting concerns.\n- db/ (migrations SQL): \n  - Migrations create and evolve tables that correspond to the resources exposed by the API (e.g., users, widgets).\n  - Versioning ensures backend code and schema stay in sync across environments.\n- infra/ (Terraform): \n  - Provisions runtime for frontend (e.g., S3+CloudFront or static hosting), backend (e.g., ECS/Kubernetes/VMs with a load balancer), and DB (e.g., RDS/Postgres).\n  - Sets up networking and secrets (API base URL, DB connection strings) that the backend uses to connect to the database.\n  - Optionally defines monitoring/logging (CloudWatch, Prometheus) to trace requests across layers.\n\nRequest contract alignment with README\n- API routes in README act as the contract between frontend and backend (paths, methods, request/response shapes).\n- DB schemas in README map those routes to tables/relationships. For example:\n  - /users → users table\n  - /orders → orders table, joined with users and items\n- Keep DTOs and frontend types aligned with those schemas to avoid mismatches.\n\nIntegration standards to keep communication smooth\n- Consistent response envelope: { data, error, meta } helps frontend handle success and failure uniformly.\n- DTOs and validation: Define request/response types in NestJS; mirror them in the React API client.\n- Error taxonomy: Standardize codes/messages (e.g., VALIDATION_ERROR, NOT_FOUND) and HTTP status usage.\n- Auth propagation: Decide on JWT vs cookies, document headers, and expiration/refresh behavior.\n- Versioning: Version your API (e.g., /v1) to evolve without breaking clients.\n- Observability: Correlation IDs per request, logs at controller-entry and DB-call boundaries for tracing.\n\nPractical request flow example\n“Create a widget”:\n- React form → POST /api/widgets { name, color }\n- NestJS Controller validates and passes to Service.\n- Service checks uniqueness and permissions.\n- Repository inserts into widgets table (defined by migrations), returns the new ID.\n- Service returns DTO; Controller sends 201 Created with JSON.\n- React updates UI or navigates to /widgets/:id.\n\nHow infrastructure influences the flow\n- The frontend is likely served via a CDN. API calls go through a load balancer/API gateway to the NestJS service.\n- Terraform controls network policies so only the backend can reach the DB (private subnet, security groups).\n- Secrets management (e.g., AWS Secrets Manager) provides DB credentials to backend at startup.\n- Health checks and autoscaling keep the backend responsive under load; the DB instance sizing affects query performance.\n\nExercises to deepen understanding\n- Draw a sequence diagram for one route (e.g., GET /orders) from React component to DB query. Identify where validation, auth, and caching happen.\n- Trace a single field (e.g., “status”) across layers: form input → API payload → DTO → table column → response → UI rendering.\n- Map Terraform outputs to backend config: which variables feed the API base URL, DB host, and secrets?\n- Review one migration and locate the corresponding repository method and controller route; confirm naming alignment.\n- Define an error response standard and apply it to two endpoints; adjust the React error handling to match.\n\nTargeted clarifications that will refine the advice\n- Which database engine is provisioned by Terraform (Postgres, MySQL, etc.)?\n- Does the backend use an ORM (TypeORM/Prisma) or raw SQL/query builder?\n- What is the auth strategy (JWT, sessions/cookies, OAuth)?\n- How is the frontend hosted (CDN/static bucket) and what is the API base URL routing (same domain with path, or separate subdomain)?\n- Any caching layers (Redis) or message queues (e.g., for async tasks) involved?\n- CI/CD: how are migrations applied in each environment before backend deployment?\n\nIf you share a snippet of the README routes and one schema, I can map the exact request-to-table flows and highlight any gaps or risks.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"explanation\":\"Missing definitions and foundational framing\",\"score\":85}",
        "score": 85
      }
    },
    "compliance": "ok",
    "complianceText": "Comprehensive architectural overview, analogies, exercises, and clarification aligned with description.  \nOK",
    "isGroundtruth": true
  },
  {
    "id": "9893f33",
    "promptid": "4b51e23",
    "ruleid": 18,
    "rule": "The output includes no practical exercises or actionable tasks and discourages hands-on work, even for quick wins like sketching a module diagram or mapping a request flow.",
    "inverse": true,
    "testuid": "test-47327591",
    "scenario": "",
    "baseline": false,
    "testinput": "training_request: provide practical exercises to practice architecture skills; tasks: sketch a module diagram, map a request flow, define contracts for two services; goal: actionable steps I can do today.",
    "model": "gpt-5-2025-08-07",
    "input": "training_request: provide practical exercises to practice architecture skills; tasks: sketch a module diagram, map a request flow, define contracts for two services; goal: actionable steps I can do today.",
    "output": "Here’s a set of practical, time-boxed exercises you can do today to sharpen architecture skills. They’re designed to be hands-on, clear, and immediately useful. Use your own project if you prefer; if not, the examples assume a simple mini-commerce system with: Frontend Web App, API Backend, Order Service, Inventory Service, Payment Provider, and a Database/Event Bus.\n\nPrep (10 minutes)\n- Materials: pen and paper or a simple diagramming tool (draw.io/diagrams.net, Miro, FigJam).\n- Choose scope: one domain slice (e.g., “Place Order”) so you can finish in one sitting.\n- Set timebox: 90–120 minutes total.\n\nExercise 1: Sketch a Module Diagram (30 minutes)\nGoal: Show the main building blocks, their responsibilities, and how they communicate.\n\nSteps\n1) List 5–8 modules you need for the slice:\n   - Frontend Web App (UI)\n   - API Backend (gateway/controller)\n   - Order Service (domain logic; owns Orders)\n   - Inventory Service (stock reservations; owns Inventory)\n   - Payment Adapter (outbound integration to payment provider)\n   - Authentication/Authorization (cross-cutting)\n   - Database(s) or Data Stores (Orders DB, Inventory DB)\n   - Event Bus/Message Broker (for async events)\n\n2) For each module, write one concise “module card”:\n   - Name and single responsibility (what it owns and decides)\n   - Inputs (who calls it, protocols: HTTP/JSON, gRPC, events)\n   - Outputs (who it calls or what events it emits)\n   - Data ownership (which tables/collections)\n   - Dependencies (limit and make explicit)\n\n3) Draw the diagram:\n   - Boxes for modules; group them into layers: Frontend, Backend Services, Data Storage, External Integrations.\n   - Arrows for communication; label each arrow:\n     - Sync (HTTP/gRPC) vs Async (events)\n     - Direction and protocol (e.g., “API -> Order Service (HTTP)” or “Order Service -> Event Bus: OrderPlaced”)\n   - Mark external systems clearly (Payment Provider).\n   - Add cross-cutting components that apply to many modules (Auth, Logging/Monitoring).\n\n4) Standardize:\n   - Naming conventions (verbs for events, nouns for resources: Order, Inventory).\n   - Interface boundaries: ensure no module reaches into another’s database directly.\n   - Avoid cycles: if A->B and B->A appears, reconsider the responsibilities.\n\n5) Quick quality lens (2 minutes):\n   - Changeability: Can you add a new payment provider without touching Order?\n   - Reliability: What happens if Inventory is down?\n   - Performance: Where’s the slowest hop? Any caching needed?\n\nDeliverable\n- One page diagram plus a bullet list of module cards.\n\nChecklist\n- Each module has one clear responsibility.\n- All data access goes through the owning module/service.\n- All arrows have a labeled protocol and sync/async.\n- No circular dependencies.\n\nEveryday analogy: Think of modules as rooms in a house. Each room has a purpose. Doors are interfaces. Hallways (arrows) show how people move between rooms without knocking down walls.\n\nExercise 2: Map a Request Flow (30 minutes)\nGoal: Trace a single user action end-to-end, including alternative paths and failure handling.\n\nPick a scenario\n- Example: “User clicks Buy” -> Order is placed -> Inventory reserved -> Payment authorized -> Order confirmed.\n\nSteps\n1) Define the entry point:\n   - Frontend sends “Place Order” to API (include user/session, cart, payment method).\n\n2) Sequence the steps (10–15 steps max):\n   - API validates request and auth.\n   - API calls Order Service (sync) to create a pending order.\n   - Order Service requests reservation from Inventory Service (sync) or emits “OrderPending” event (async, consumed by Inventory).\n   - Inventory reserves stock or returns “Insufficient stock”.\n   - Order Service calls Payment Adapter to authorize payment (sync) OR emits “PaymentRequested” event (async).\n   - Payment Provider responds success/failure.\n   - Order Service transitions order state: Pending -> Confirmed (success) or Failed (failure).\n   - Order Service emits “OrderConfirmed” or “OrderFailed” event.\n   - Frontend receives final status via API response or polling/websocket.\n\n3) Annotate each step:\n   - Component name, action, input/output data, sync/async, expected latency.\n   - Error paths (inventory fail, payment decline, timeout/retry).\n   - State changes (order status, stock reservation).\n   - Observability: log/event/trace at key points (request received, reservation result, payment outcome).\n\n4) Identify boundaries and risks:\n   - Network hops (API->Order, Order->Inventory, Order->Payment).\n   - Idempotency needs (retry Place Order, duplicate payment calls).\n   - Compensation paths (release stock if payment fails).\n\nDeliverable\n- A sequence flow (bulleted steps) with one success path and 2–3 alternative paths and failure handling.\n\nChecklist\n- All components involved are named.\n- Data and state transitions are explicit.\n- Sync vs async is clear.\n- Failure scenarios and retries are covered.\n- Observability points noted.\n\nEveryday analogy: A guest checks in at reception (API), gets a room (Inventory reserve), pre-authorizes a deposit (Payment), then receives keys (Order confirmed). If the card is declined, reception cancels the room hold.\n\nExercise 3: Define Contracts for Two Services (45 minutes)\nGoal: Specify clear, stable interfaces that services promise to each other and to clients.\n\nChoose two: Order Service and Inventory Service.\n\nFor each service, create a short contract spec (1–2 pages):\n\nA) Service purpose and ownership\n- Order Service: owns Order lifecycle (Pending, Confirmed, Failed).\n- Inventory Service: owns stock levels and reservations.\n\nB) Synchronous API contracts (if applicable)\n- Endpoint list with method and path:\n  - Inventory: POST /reservations — reserves stock for a list of items.\n  - Inventory: DELETE /reservations/{id} — releases a reservation.\n  - Order: POST /orders — create pending order.\n  - Order: POST /orders/{id}/confirm — finalize after successful payment.\n- Request schema fields (names, types, required vs optional):\n  - Use clear, consistent names (itemId, quantity, reservationId, orderId).\n- Response schema and status codes:\n  - Success payloads with IDs and statuses.\n  - Standardized errors (400 validation, 409 conflict, 422 business rule, 500 server).\n- Idempotency:\n  - Define idempotency-key header for POST /orders and POST /reservations.\n- Versioning:\n  - Include a version in path (/v1) or accept-header. State how you deprecate fields.\n- Security:\n  - Required scopes (e.g., order:write, inventory:write).\n  - Authentication method (JWT/OAuth2).\n- Rate limits and timeouts:\n  - State expected max RPS and recommended client timeout.\n\nC) Asynchronous event contracts (if applicable)\n- Events published:\n  - Order Service: OrderPending, OrderConfirmed, OrderFailed.\n  - Inventory Service: InventoryReserved, InventoryReservationFailed, InventoryReleased.\n- Event schema:\n  - Event name, version, required fields (eventId, timestamp, orderId, reservationId, items[]).\n- Delivery semantics:\n  - At-least-once delivery; consumers must be idempotent.\n  - Ordering guarantees per key (e.g., per orderId).\n- Topics/subjects and routing keys.\n- Error handling:\n  - Dead letter queue policy; retries.\n\nD) SLAs and expectations\n- Latency targets:\n  - Sync: <200ms for Inventory reserve; <500ms for Order create (excluding payment).\n- Availability targets and retry guidance.\n- Data ownership rules:\n  - Only Inventory updates stock.\n  - Only Order updates order status.\n\nE) Evolution policy\n- Backward compatibility rules (additive changes only in minor versions).\n- Field deprecation process (announce, dual-write, remove).\n\nDeliverables\n- Two contract specs (Order, Inventory), each with:\n  - Endpoint list, schemas, errors, security, idempotency, versioning.\n  - Event list with schemas and delivery semantics.\n  - SLAs and evolution policy.\n\nChecklist\n- Clear ownership; no shared database tables.\n- Requests/responses have explicit fields and meanings.\n- Error taxonomy and idempotency defined.\n- Versioning and compatibility plan included.\n- Security and rate limits stated.\n\nStretch Goals (if you have extra 30 minutes)\n- Add a simple saga outline: how Order coordinates Inventory and Payment with compensations.\n- Add a high-level deployment diagram (containers/pods, networks).\n- Run a quick peer review: ask a colleague to find ambiguities.\n\nHow to use this in your own project today\n- Swap in your modules and scenario (e.g., “Create Workspace” in a SaaS app).\n- Keep the same structure: module cards, request flow with alt paths, two service contracts.\n- Aim for clarity over completeness. If a detail is fuzzy, write an assumption beside it.\n\nSuccess criteria for today\n- One module diagram with clear responsibilities and labeled interfaces.\n- One request flow with success and failure paths, including observability and retries.\n- Two concise service contract specs covering sync APIs, events, idempotency, and versioning.\n\nIf you share the domain or project type you’re working on, I can tailor the module set, flow steps, and contract templates to your exact context.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"score\":80,\"explanation\":\"Missing foundational intro, no diagram, jargon undefined\"}",
        "score": 80
      }
    },
    "compliance": "ok",
    "complianceText": "Detailed, focused on architecture exercises with analogies and no code—fits the guide.  \nOK",
    "isGroundtruth": true
  },
  {
    "id": "6bb8aff",
    "promptid": "4b51e23",
    "ruleid": 19,
    "rule": "To minimize practicality, the output avoids real-world examples, case studies, and industry scenarios, relying solely on abstract generalities—even for regulated or high-scale domains.",
    "inverse": true,
    "testuid": "test-73682699",
    "scenario": "",
    "baseline": false,
    "testinput": "domain: healthcare patient portal; constraints: HIPAA compliance, millions of users, high availability; request: provide real-world case studies and industry scenarios demonstrating architectural choices and trade-offs for this context.",
    "model": "gpt-5-2025-08-07",
    "input": "domain: healthcare patient portal; constraints: HIPAA compliance, millions of users, high availability; request: provide real-world case studies and industry scenarios demonstrating architectural choices and trade-offs for this context.",
    "output": "Below are real-world industry scenarios and case-study style examples tailored to a healthcare patient portal serving millions of users with HIPAA constraints and high availability goals. Each scenario highlights architectural choices, why teams made them, and the trade-offs they accepted. I’ll also call out cross-cutting concerns you’ll face regardless of approach and conclude with practical exercises you can use with your team.\n\nScenario 1: Large health system adopting an EHR vendor’s portal (Epic MyChart/Cerner HealtheLife-style)\n- Context\n  - A multi-hospital Integrated Delivery Network (IDN) wants a single patient digital front door for scheduling, test results, secure messaging, and bill pay.\n  - They already use Epic or Cerner as the system of record.\n- Key architectural choices\n  - Portal as a hybrid: vendor portal for core patient features + a cloud-native “experience layer” for custom UX, search, and marketing content.\n  - Integration via FHIR APIs for clinical data and HL7 v2 for legacy feeds (admission/discharge/transfer).\n  - API Gateway + WAF as the ingress, with strict segmentation: PHI APIs behind private networking; public content via CDN.\n  - Regional deployment (multi-AZ, active-passive DR) aligned to data residency.\n  - Identity using the EHR’s patient identity plus an external customer IAM for multi-factor authentication and device trust.\n- Trade-offs\n  - Speed vs flexibility: you ship faster by leaning on the vendor’s proven workflows, but you accept vendor UX constraints and slower change velocity.\n  - Consistency vs interoperability: FHIR coverage varies; you may need adapters to map to canonical models.\n  - HA vs simplicity: active-passive DR is simpler and cost-effective, but recovery time isn’t as low as active-active.\n- Outcomes and pitfalls\n  - Faster rollout with lower integration risk, but custom features can be harder to maintain across vendor upgrades.\n  - Patient identity proofing is tricky; aligning EHR identity with consumer-grade MFA can create edge cases (e.g., proxy access, minors).\n  - Watch for session mix-ups if you embed vendor components in custom shells—strict token scoping and same-site cookie strategies help.\n\nScenario 2: Cloud-native, multi-region, microservices portal with a canonical FHIR model\n- Context\n  - A national telehealth/provider network offers services across states; it integrates with multiple EHRs and wants consistent patient experiences.\n- Key architectural choices\n  - Microservices on Kubernetes with a domain-driven design: scheduling, messaging, documents, billing, notifications, consent, and profile as separate services.\n  - Event-driven integration using Kafka/PubSub for appointment status, lab result availability, and messaging delivery receipts.\n  - Canonical data model based on FHIR R4; adapters for each EHR (Mirth/NextGen Connect or custom) translate from HL7 v2/FHIR variants to the canonical model.\n  - Active-active multi-region deployment for high availability and surge handling, with geo-aware routing.\n  - Strong access controls: zero trust networking, per-service IAM, mutual TLS, and strict audit logging with immutable storage.\n- Trade-offs\n  - Agility vs complexity: microservices and event streams scale well to millions of users, but require mature DevSecOps, SRE, and observability.\n  - Availability vs consistency: active-active introduces conflict resolution and eventual consistency in some domains (e.g., notifications), while clinical updates may require strong consistency.\n  - Cost vs performance: multi-region active-active is more expensive; cost optimization needs intelligent autoscaling and message batching.\n- Outcomes and pitfalls\n  - Rapid feature iteration and resilience to regional outages, but higher operational overhead.\n  - Common failure mode: cross-region write conflicts. Patterns like “source-of-truth per patient per region” or command routing can help.\n  - Strict separation between PHI-processing subsystems and customer engagement subsystems reduces blast radius.\n\nScenario 3: Multi-tenant SaaS portal for a hospital group (tenant isolation focus)\n- Context\n  - A health system consortium wants a shared platform with isolated data and branding per hospital.\n- Key architectural choices\n  - Logical tenant isolation at the application layer (tenant-aware services) plus physical isolation of PHI storage per tenant or per region.\n  - Key management: per-tenant encryption keys (KMS/HSM) and periodic key rotation with re-encryption jobs.\n  - Shared stateless layers (CDN, API gateway, frontends), dedicated stateful layers (databases, queues) where PHI is stored.\n  - Centralized audit pipeline with tenant tags, immutable storage, and retention policies per tenant’s legal requirements.\n- Trade-offs\n  - Efficiency vs compliance margin: shared components reduce cost but increase the blast radius; regulators often prefer stronger isolation for PHI.\n  - Operational simplicity vs data residency: per-tenant data stores improve residency compliance, but complicate queries and analytics.\n  - Customizability vs maintainability: per-tenant feature toggles and branding can multiply testing matrices.\n- Outcomes and pitfalls\n  - Economies of scale with controlled risk; tenants can scale independently.\n  - Be cautious with caching: never cache PHI at the CDN; cache only static assets and anonymized content.\n\nScenario 4: Telehealth surge resiliency using managed communications and media services\n- Context\n  - Rapid spikes in video visits during outbreaks or seasonal peaks.\n- Key architectural choices\n  - Use HIPAA-eligible managed video services with BAAs (e.g., vendor platforms supporting SRTP/TLS and end-to-end encrypted media) integrated via your portal.\n  - Separate “session orchestration” service from media transport service; store session metadata with minimal PHI.\n  - On-demand scaling via auto-scaling groups or serverless functions for session setup and signaling; pre-provision region capacity for known peak windows.\n- Trade-offs\n  - Build vs buy: in-house media stacks offer more control but are expensive and risky for HIPAA; managed services speed compliance and scale but may limit custom features.\n  - Quality vs cost: multi-region media relays and TURN servers improve call quality but increase cost.\n- Outcomes and pitfalls\n  - Reliable surge handling with predictable ops; be mindful of HIPAA boundaries (no recording without explicit consent and compliant storage).\n  - Network conditions vary—adaptive bitrate helps; track patient experience SLOs separately from system SLOs.\n\nScenario 5: Analytics with strict PHI minimization and de-identification\n- Context\n  - Product and clinical teams want insights on usage, outcomes, and performance without violating HIPAA.\n- Key architectural choices\n  - Dual pipelines: operational PHI systems (care delivery) and de-identified analytics systems (insights).\n  - Privacy-preserving ETL: tokenization/pseudonymization at the edge; controlled linking keys kept in a separate secure vault.\n  - Access governed by purpose-based access controls and data use agreements; analysts work in secure enclaves with audited queries.\n- Trade-offs\n  - Fidelity vs privacy: fully de-identified data limits longitudinal analysis; limited-use datasets with robust controls may be needed for certain studies.\n  - Latency vs quality: real-time analytics are tempting but raise re-identification risks; use aggregated, delayed feeds for product dashboards.\n- Outcomes and pitfalls\n  - Reduced compliance risk; analytics still useful for capacity planning and UX optimization.\n  - Leakage risks often arise in log pipelines; scrub PHI from application logs and enforce structured logging.\n\nScenario 6: HA/DR strategy decisions (active-passive vs active-active)\n- Context\n  - Millions of users require high availability; downtime erodes trust and affects care.\n- Key architectural choices\n  - Active-passive: primary region with hot standby, cross-region replication, automated failover drills, and RTO/RPO targets defined per subsystem (e.g., seconds for auth, minutes for reports).\n  - Active-active: multi-region read/write with global load balancing, conflict resolution strategies, and regional “home” for write-heavy domains.\n- Trade-offs\n  - Complexity vs RTO: active-active improves availability but introduces consistency challenges and higher cost; active-passive is simpler but failover drills must be routine.\n  - Data residency vs global UX: active-active for global users must respect regional PHI boundaries; consider partial active-active (UI globally, PHI regionally).\n- Outcomes and pitfalls\n  - Clear SLOs per domain are essential; not all services need the same HA tier.\n  - Run game days; many DR failures result from untested runbooks and unreplicated secrets.\n\nMulti-layer architecture overview for a patient portal\n- Frontend application\n  - Web and mobile clients. Static assets via CDN; dynamic content through API gateway.\n  - Strict content security (CSP), secure cookies, and minimal client-side PHI storage.\n- Backend services\n  - Domain services: scheduling, messaging, lab results, documents, billing, notifications, consent, identity, proxy access.\n  - Integration services: FHIR API layer, HL7 v2 listener, EHR adapters; iPaaS or integration engines for mapping/transformation.\n  - Support services: audit logging, feature flagging, configuration, secrets, and compliance reporting.\n- Data storage\n  - PHI stores: relational DBs for transactional integrity (e.g., appointments, messages), object storage for documents with envelope encryption, and queue storage for asynchronous workflows.\n  - Non-PHI stores: content, marketing, and telemetry in separate accounts/projects.\n  - Key management: per-region KMS/HSM with strict rotation; least-privilege access.\n- Security and compliance\n  - Zero trust: mutual TLS, per-service identities, robust IAM, and network segmentation.\n  - Encryption in transit and at rest; BAA with all cloud and SaaS vendors handling PHI.\n  - Audit and monitoring: centralized logs, tamper-evident storage, anomaly detection, and alerting tuned to clinical workflow impacts.\n- Observability and operations\n  - SLOs per domain (e.g., 99.95% for scheduling, 99.99% for authentication).\n  - Tracing and metrics that intentionally exclude PHI; redaction at the edge.\n  - On-call rotations with runbooks; frequent DR drills tied to compliance reporting.\n\nKey architectural style choices and how they fit this context\n- Modular monolith\n  - Pros: simpler to secure, fewer moving parts, good for teams early in cloud maturity.\n  - Cons: scaling to millions can stress a single deployable; changes can ripple across modules.\n  - Use when: scope is focused, one EHR, regional user base, and strict governance.\n- Microservices + event-driven\n  - Pros: scales horizontally, fits multi-EHR integrations, resilient to spikes, supports independent teams.\n  - Cons: higher operational complexity, needs strong platform engineering, testing across services is harder.\n  - Use when: national footprint, many domains, and frequent feature change.\n- Hybrid EHR vendor + custom experience layer\n  - Pros: fastest path to value, leverages certified workflows.\n  - Cons: vendor lock-in, limited deep customization, dependency on vendor release cycles.\n  - Use when: large health systems standardizing on a major EHR and prioritizing time-to-market.\n\nCommon trade-offs you’ll evaluate\n- Consistency vs availability: clinical data often demands strong consistency; engagement features can tolerate eventual consistency.\n- Build vs buy: HIPAA-eligible managed services accelerate delivery but constrain customization; in-house increases control and cost.\n- Isolation vs operational efficiency: per-tenant/per-region isolation improves compliance posture but complicates shared services and analytics.\n- Performance vs privacy: caching helps scale, but never cache PHI at the edge; prefer server-side caching inside private networks with strict token checks.\n- Identity assurance vs friction: NIST-aligned identity proofing and MFA reduce fraud but can frustrate users; offer multiple MFA methods (push, WebAuthn) with risk-based prompts.\n\nIndustry references and patterns to study\n- HL7 FHIR R4 and SMART on FHIR for interoperability and app launch flows.\n- US ONC and CMS interoperability rules for patient access APIs and consent.\n- Cloud provider healthcare blueprints and HIPAA guidance (AWS, Azure, Google Cloud).\n- HITRUST and SOC 2 for security controls; NIST 800-63 for digital identity assurance.\n\nPractical exercises for your team\n- Data flow mapping: Diagram the journey of “view lab results” from click to data fetch to display. Mark where PHI appears, how it is encrypted, and where it is logged. Identify any place PHI could leak (e.g., error logs).\n- SLO tiering: Assign availability and latency SLOs to each service (auth, scheduling, messaging). Decide which are active-active vs active-passive and justify the choice.\n- Threat modeling: Run a session on secure messaging. Consider spoofing, tampering, replay, and unauthorized proxy access. Define mitigations (MFA, signed tokens, short-lived links).\n- Integration drill: Pick two EHRs and design adapters to transform their HL7 v2 feeds into your canonical FHIR model. Note differences you’ll normalize and what you’ll preserve as extensions.\n- DR game day: Simulate a regional outage. Practice failing over auth and scheduling while keeping lab results read-only. Measure RTO/RPO and document gaps.\n- Privacy posture check: Inventory all observability pipelines. Ensure PHI redaction at source and confirm immutable audit log retention with access reviews.\n- Capacity planning: Model peak appointment traffic (e.g., flu season Friday mornings). Size autoscaling thresholds for API, queue consumers, and DB read replicas.\n\nIf you can share your geographic footprint, EHRs involved, preferred cloud, and whether you’re leaning vendor portal or custom build, I can tailor the scenarios with more specific architectural diagrams and decisions for your context.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"explanation\":\"Missing diagrams, definitions, analogies\",\"score\":70}",
        "score": 70
      }
    },
    "compliance": "ok",
    "complianceText": "Well-structured architectural guidance with real-world examples, exercises, styles, and clarification request. \nOK",
    "isGroundtruth": true
  },
  {
    "id": "36ef45c",
    "promptid": "4b51e23",
    "ruleid": 20,
    "rule": "When explaining structure or interactions, the output provides no diagram—graphical or textual—and declines to add one even if ambiguity remains.",
    "inverse": true,
    "testuid": "test-60287517",
    "scenario": "",
    "baseline": false,
    "testinput": "ambiguity: our current architecture description is unclear; request: add a Mermaid sequence diagram and an ASCII or PlantUML component diagram to explain structure and interactions between frontend, backend, and data storage.",
    "model": "gpt-5-2025-08-07",
    "input": "ambiguity: our current architecture description is unclear; request: add a Mermaid sequence diagram and an ASCII or PlantUML component diagram to explain structure and interactions between frontend, backend, and data storage.",
    "output": "Below are two concise diagrams to clarify how the frontend, backend, and data storage interact. Use them as a starting point, then tailor names and endpoints to your project.\n\nMermaid sequence diagram (typical read and write flows)\nCopy into a Mermaid renderer (e.g., Mermaid Live Editor).\n\n```mermaid\nsequenceDiagram\n    autonumber\n    participant U as User\n    participant FE as Frontend (Browser/SPA)\n    participant BE as Backend API\n    participant DS as Data Storage (DB/Blob)\n\n    U->>FE: Click \"View Item\"\n    FE->>BE: GET /items/{id}\n    BE->>DS: Query item by id\n    DS-->>BE: Item data\n    BE-->>FE: 200 OK + JSON\n    FE-->>U: Render item details\n\n    U->>FE: Submit \"Add Review\"\n    FE->>BE: POST /items/{id}/reviews (JSON)\n    BE->>BE: Validate & authorize\n    BE->>DS: Write review\n    DS-->>BE: Write OK\n    BE-->>FE: 201 Created + reviewId\n    FE-->>U: Show confirmation\n\n    alt Error path\n        BE-->>FE: 4xx/5xx + error payload\n        FE-->>U: Display error message\n    end\n```\n\nPlantUML component diagram (structure and interfaces)\nCopy into a PlantUML viewer.\n\n```plantuml\n@startuml\nskinparam componentStyle rectangle\nskinparam shadowing false\ntitle High-level Components: Frontend ↔ Backend ↔ Data Storage\n\nactor User as U\n\nnode \"Client\" {\n  component \"Frontend App\\n(SPA/SSR)\" as FE\n}\n\nnode \"Server\" {\n  component \"Backend Service\\n(API)\" as BE\n}\n\nnode \"Data Storage\" {\n  database \"Primary Database\\n(SQL/NoSQL)\" as DB\n  component \"Object/Blob Store\\n(files/media)\" as OS\n}\n\nU --> FE : UI interactions (click, submit)\nFE --> BE : HTTPS API calls (GET/POST/PUT)\nBE --> DB : Queries / Transactions\nBE --> OS : Upload / Download files\nBE <-- DB : Results / ACK\nBE <-- OS : Files / URLs\nFE <-- BE : JSON / HTML responses\n@enduml\n```\n\nASCII component diagram (fallback if PlantUML isn’t available)\n\n```\n+-------+            HTTPS            +----------------+         DB protocol         +-------------------+\n| User  | <----->  Frontend App  <--> | Backend Service| <-------------------------> | Primary Database  |\n|Browser|          (SPA or SSR)       |   (API layer)  |                             |  (SQL/NoSQL)      |\n+-------+                              +----------------+                             +-------------------+\n                                               |\n                                               | File transfer (SDK/HTTP)\n                                               v\n                                     +---------------------+\n                                     | Object/Blob Storage |\n                                     |   (files/media)     |\n                                     +---------------------+\n```\n\nHow to read this\n- Frontend is the “storefront”: it handles user interactions and calls the backend via HTTPS.\n- Backend is the “warehouse coordinator”: it validates requests, enforces rules, and reads/writes data.\n- Data storage is the “inventory system”: the database stores structured data and the blob store holds files/media.\n\nIf you share a few specifics (SPA vs SSR, REST vs GraphQL, DB type, whether you store files), I can refine these diagrams to match your exact architecture and endpoint shapes.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"score\":0,\"explanation\":\"Missing principles, styles, exercises\"}",
        "score": 0
      }
    },
    "compliance": "ok",
    "complianceText": "Used clear diagrams, everyday analogies, focus on architecture, asked clarifications, avoided code solutions—complies fully.  \nOK",
    "isGroundtruth": true
  },
  {
    "id": "f3ec1d1",
    "promptid": "4b51e23",
    "ruleid": null,
    "testuid": "baseline-test-851ekzusc",
    "scenario": "",
    "baseline": true,
    "testinput": "user_level: experienced module developer\ngoal: learn basics of project architecture\nproject_type: web app\nmodules: auth service, product catalog, order processing, notification module\nask: explain foundational principles of inter-module communication and standardization practices; use clear language and avoid jargon; include everyday analogies; no code\nrequest_diagram: yes (textual description of architecture diagram)\nconstraints: team of 4, tight deadline",
    "model": "gpt-5-2025-08-07",
    "input": "user_level: experienced module developer\ngoal: learn basics of project architecture\nproject_type: web app\nmodules: auth service, product catalog, order processing, notification module\nask: explain foundational principles of inter-module communication and standardization practices; use clear language and avoid jargon; include everyday analogies; no code\nrequest_diagram: yes (textual description of architecture diagram)\nconstraints: team of 4, tight deadline",
    "output": "Here’s a clear, practical guide to help you move from strong module work to thinking across the whole web app. I’ll keep it simple, use everyday analogies, and include a textual description of an architecture diagram. I’ll also tailor recommendations to a 4-person team on a tight deadline.\n\n1) Foundational principles of inter‑module communication (simple and consistent)\n- Two ways to “talk” between modules:\n  - Synchronous (like a phone call): one module calls another and waits for an answer. Use this for quick, read/write actions where you need an immediate result (e.g., “Is this user logged in?” “What is the price of product X?”).\n  - Asynchronous (like sending a postcard): one module publishes a message/event and another module picks it up later. Use this for follow‑ups that don’t need an immediate reply (e.g., “Order placed” triggers an email).\n- For this project, pick one default approach:\n  - Use HTTP/JSON for synchronous calls (simple “phone calls” with a uniform format).\n  - Use a lightweight message queue for asynchronous events (the “mailroom”).\n- Keep a simple, repeatable “conversation” pattern:\n  - Uniform request/response shape: think of every request as a standardized form; every response as a standardized receipt.\n  - Clear error format: always include a human‑readable message, a machine code, and a reference ID (like a tracking number).\n  - Timeouts and retries: if a “phone call” takes too long, hang up (timeout). If it fails, retry a few times sensibly. If a service is clearly down, stop and return a friendly error rather than hammering it (like stopping after reaching a closed store).\n  - Idempotency (safe repeats): if the same order request is sent twice, only one order should be created. Treat repeat calls with the same request ID like “I already did this—here’s the result.”\n  - Pagination for lists: send results in pages, like flipping through a catalog one section at a time.\n  - Correlation ID: include a unique “tracking number” on every call and event so you can trace a user’s journey across all modules in logs.\n- Security across modules:\n  - Use a simple token passed with each call (like a work badge). Each service checks the badge for access rights.\n  - Avoid passing user passwords between services; only tokens and user IDs.\n- Versioning and change management:\n  - Don’t break old clients: add fields instead of removing; keep old endpoints working while introducing new ones.\n  - Publish a “contract” doc per module (what it expects and what it returns). Update it on every change.\n- Data ownership and consistency:\n  - Each module owns its data (its “desk”). Other modules ask instead of writing directly (no one shuffles papers on someone else’s desk).\n  - For a small team and tight timeline, you can use one shared database with separate schemas/tables per module, but enforce ownership: only the owning module writes to its tables.\n\n2) Standardization practices (keep the team aligned)\n- One API style for all services (e.g., REST over HTTP with JSON). Same naming patterns for endpoints and fields.\n- One error format across all modules:\n  - Fields: code, message, correlation_id, details (optional).\n- One approach for authentication:\n  - All services check tokens in the same way; document roles/permissions once and reuse.\n- One logging pattern:\n  - Every call logs start/end, correlation ID, caller, outcome (success or error), and duration.\n- One checklist per endpoint/event (the “Definition of Done”):\n  - Contract documented\n  - Authentication checked\n  - Error format followed\n  - Timeouts/retries set\n  - Observability (logs with correlation ID)\n  - Test for repeated call (idempotency where applicable)\n- Naming and structure:\n  - Consistent names for resources (e.g., “products,” “orders,” “users”).\n  - Consistent status fields (e.g., “status”: pending, confirmed, cancelled).\n- Simple release/version plan:\n  - Tag releases, keep a change log, and maintain a compatibility note (“what changed and who could be affected”).\n\n3) Integration map for your modules (who talks to whom)\n- Auth Service:\n  - Frontend logs in and gets a token.\n  - Other services verify tokens on every request.\n  - Think of Auth as the “security desk” issuing and checking badges.\n- Product Catalog:\n  - Frontend reads products, prices, availability.\n  - Order Processing calls Catalog to validate product details and stock.\n  - Catalog owns product data; others never write directly to it.\n- Order Processing:\n  - Receives order requests from the Frontend.\n  - Validates user (via Auth) and product/stock (via Catalog).\n  - Changes order status (e.g., pending → confirmed).\n  - Publishes events like “OrderPlaced,” “OrderFailed” to the message queue.\n  - Owns order data and inventory reservations (if in scope).\n- Notification Module:\n  - Listens to events (e.g., “OrderPlaced”) from the queue.\n  - Sends emails/SMS and records notification results.\n  - Decoupled from order placement timing (no need to block the user while sending emails).\n- Frontend App:\n  - Calls Auth (login), Catalog (browse), Order (place orders).\n  - Displays order status and messages from Notification or Order service.\n\n4) Architectural style recommendation (small team, tight deadline)\n- Choose a “modular monolith” or “few simple services”:\n  - Modular monolith: one deployable backend with clear module boundaries inside (Auth, Catalog, Order, Notification). Easiest to run and test for a team of 4 under time pressure.\n  - If you need separate services (due to team split or deployment needs), limit to two or three: combine Catalog + Order into one backend, keep Auth separate, and have Notification as a simple worker. This reduces the operational burden while keeping boundaries clear.\n- Use a message queue only where it truly helps (notifications and order events). Keep everything else as direct calls.\n\n5) Practical exercises (no code, quick wins)\n- Contract one‑pager per module:\n  - Purpose\n  - Endpoints/events\n  - Request/response fields\n  - Error codes/messages\n  - Auth rules (who can call)\n  - Timeouts/retries\n  - Owner (person on the team)\n- Sequence drill: “Place Order”\n  - Step through: Frontend → Auth (token) → Order (create) → Catalog (validate stock) → Order (confirm) → Queue (OrderPlaced) → Notification (send).\n  - Note which calls are synchronous vs asynchronous, and where you’d retry vs fail fast.\n- Event list and schema sketch:\n  - OrderPlaced, OrderFailed, OrderCancelled, InventoryReserved/Released.\n  - For each, list minimal fields: order_id, user_id, items[], total, timestamp, correlation_id.\n- Error walkthrough:\n  - Simulate Catalog being slow or down. Ensure Order times out, returns a friendly error with a correlation ID, and doesn’t create duplicate orders on retries.\n- Integration rehearsal:\n  - Run an end‑to‑end test with fake data: user signs in, browses, places order, receives notification. Capture logs and verify the same correlation ID threads through all services.\n\n6) Multi‑layered view (Frontend, Backend, Data)\n- Frontend Application:\n  - Responsible for user flows, keeps tokens in a secure place, attaches the token to every request.\n- Backend Services:\n  - Auth: token issuing and verification; owns users/roles.\n  - Catalog: products, prices, stock; read‑heavy operations.\n  - Order: order lifecycle, inventory reservations, events.\n  - Notification: listens to events, sends messages.\n- Data Storage:\n  - Ideally separate schemas/tables per module to enforce ownership.\n  - Auth: users, sessions/tokens (short‑lived).\n  - Catalog: products, prices, stock levels.\n  - Order: orders, order items, status history.\n  - Notification: templates, delivery logs.\n- Message Queue:\n  - Carries events from Order to Notification (and any future listeners).\n\n7) Textual description of the architecture diagram\n- Boxes:\n  - Frontend Web App\n  - Auth Service (Users/Tokens DB)\n  - Catalog Service (Products/Stock DB)\n  - Order Processing Service (Orders/Inventory DB)\n  - Notification Module (Notifications DB)\n  - Message Queue\n  - External Email/SMS Provider\n- Arrows (data flows):\n  - Frontend → Auth: “Login” to get token.\n  - Frontend → Catalog: “List/Detail Products” (token attached).\n  - Frontend → Order: “Create Order” (token attached).\n  - Order → Auth: “Verify Token” (or local verification) for secure actions.\n  - Order → Catalog: “Check product/stock” before confirming.\n  - Order → Message Queue: publishes “OrderPlaced” (with correlation ID).\n  - Notification → Message Queue: subscribes to “OrderPlaced.”\n  - Notification → Email/SMS Provider: sends messages.\n  - All Services ↔ Their own Databases: read/write their owned data.\n- Notes on arrows:\n  - Solid arrows: synchronous HTTP calls (Frontend ↔ Auth/Catalog/Order; Order ↔ Catalog).\n  - Dashed arrows: asynchronous event flows (Order → Queue → Notification).\n  - Each arrow carries the correlation ID for tracing.\n\n8) Working practices for a 4‑person team on a tight deadline\n- Assign clear ownership:\n  - One owner per module; one integration lead ensuring contracts are consistent.\n- Weekly integration checkpoint:\n  - Run the end‑to‑end flow; fix contract mismatches immediately.\n- Lightweight documentation:\n  - Keep the module contracts in a shared doc; update on change. No heavy specs—just enough to align.\n- Keep tech choices minimal:\n  - One API style (HTTP/JSON), one queue, one database with owned schemas, one logging approach.\n- Non‑functional guardrails:\n  - Timeouts: set sensible limits for cross‑module calls.\n  - Retries: a few short retries, then fail gracefully.\n  - Backpressure: if you get overwhelmed, respond with “try again later” rather than breaking.\n- Rollout discipline:\n  - Tag releases, smoke test each module, run the end‑to‑end path before declaring “done.”\n\nEveryday analogies to remember\n- Phone calls vs postcards: synchronous vs asynchronous.\n- Security badges: tokens carried between services.\n- Tracking numbers: correlation IDs to trace a user’s journey.\n- Desk ownership: only the owning module writes to its data.\n- Standard receipts: uniform response and error formats.\n\nIf you prefer separate services, confirm whether you already have a message queue in place. If not, start with synchronous calls and add the queue only for notifications—this keeps the architecture simple while giving you room to grow.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"score\":80,\"explanation\":\"Missing learning resource and explicit style names\"}",
        "score": 80
      }
    },
    "compliance": "ok",
    "complianceText": "Detailed, clear architectural guidance, analogies, exercises, no code; aligns with all description requirements.\nOK",
    "isGroundtruth": true
  },
  {
    "id": "c1914b2",
    "promptid": "4b51e23",
    "ruleid": null,
    "testuid": "baseline-test-ywlbpnc8s",
    "scenario": "",
    "baseline": true,
    "testinput": "goal: integration insights\nproject_type: SaaS invoicing platform\nmodules: UI frontend, billing engine, payment gateway adapter, reporting service\nuse_case: user creates invoice, sends to client, processes payment, generates report\nask: walk through how these modules integrate and communicate in the larger system; include a case study-style explanation; no code; concise but detailed\nrequest_diagram: yes (ASCII)",
    "model": "gpt-5-2025-08-07",
    "input": "goal: integration insights\nproject_type: SaaS invoicing platform\nmodules: UI frontend, billing engine, payment gateway adapter, reporting service\nuse_case: user creates invoice, sends to client, processes payment, generates report\nask: walk through how these modules integrate and communicate in the larger system; include a case study-style explanation; no code; concise but detailed\nrequest_diagram: yes (ASCII)",
    "output": "Below is a concise, case‑study walkthrough of how the UI frontend, billing engine, payment gateway adapter, and reporting service integrate and communicate in a SaaS invoicing platform. Think of the system as a small office: the UI is the receptionist, the billing engine is the clerk managing records and processes, the payment adapter is the card terminal on the desk, and the reporting service is the accountant who compiles the monthly summaries.\n\nHigh‑level integration diagram (ASCII)\n  \n  +-------------------+      HTTPS/API       +-------------------+\n  |    UI Frontend    |  ------------------->|  Billing Engine   |\n  +-------------------+                      +-------------------+\n                 |                                   |   \\\n                 |                                   |    \\ domain events\n                 |                                   v     \\\n                 |                           [Data Store]   \\\n                 |                                   |       \\\n                 |                                   v        v\n                 |                             [Event Bus]  +-------------------+\n                 |                                   |      | Reporting Service |\n                 |                                   +----->+-------------------+\n                 |\n                 |                                HTTP (internal)         HTTPS/webhooks\n                 |                                   +---------------------+\n                 |                                   | Payment Adapter     |\n                 |                                   +---------------------+\n                 |                                              |\n                 |                                              v\n                 |                                   [External Payment Gateway]\n                 |                                              ^\n                 |                                              | webhooks\n                 +----------------------------------------------+\n\nModule roles and interfaces\n\n- UI Frontend\n  - Role: Collects user input, displays statuses, and drives the workflow.\n  - Interfaces: Calls Billing Engine APIs (create invoice, send invoice, start payment, view status), subscribes to updates via polling or push (WebSocket/SSE).\n\n- Billing Engine\n  - Role: Core domain logic—creates invoices, manages their lifecycle, coordinates payments, and updates invoice/payment statuses.\n  - Interfaces: Exposes REST/GraphQL APIs to the UI; calls Payment Adapter to initiate/confirm charges; persists invoices/payments; emits domain events (e.g., InvoiceCreated, InvoiceSent, PaymentSucceeded/Failed) to the Event Bus for downstream consumers like Reporting.\n\n- Payment Gateway Adapter\n  - Role: A dedicated translator between your system and the external payment gateway. Keeps provider specifics out of your domain.\n  - Interfaces: Receives charge/checkout requests from the Billing Engine; calls external gateway APIs; exposes secure webhook endpoints for gateway callbacks; forwards normalized results back to the Billing Engine.\n\n- Reporting Service\n  - Role: Builds analytical views and reports without affecting transactional performance.\n  - Interfaces: Consumes domain events from the Event Bus; maintains its own optimized reporting store; exposes reporting APIs or scheduled extracts for the UI or back‑office consumption.\n\nCase study: User creates invoice, sends to client, processes payment, generates report\n\n1) Create Invoice (UI → Billing Engine)\n- The user fills an invoice form (line items, taxes, customer details) in the UI.\n- The UI submits “Create Invoice” to the Billing Engine.\n- The Billing Engine validates and writes the invoice to the Data Store, sets status to “Draft,” then emits InvoiceCreated to the Event Bus.\n- The UI receives the invoice ID and displays confirmation.\n\n2) Send to Client (Billing Engine coordinates delivery)\n- The user clicks “Send Invoice” in the UI; the UI calls the Billing Engine.\n- The Billing Engine changes status to “Sent,” generates a secure pay link (to be used later via the Payment Adapter), and dispatches the invoice via email (or shares the link) while emitting InvoiceSent.\n- The customer receives the invoice with a pay link.\n\n3) Process Payment (Billing Engine ↔ Payment Adapter ↔ External Gateway)\n- The customer opens the pay link and initiates payment. The UI or a hosted payment page drives this, but the Billing Engine orchestrates the flow.\n- The Billing Engine requests the Payment Adapter to create a payment session/charge for the invoice.\n- The Adapter calls the external gateway to create the session and returns a session ID and redirect URL to the Billing Engine/UI.\n- The customer completes payment on the gateway. The gateway sends a webhook to the Payment Adapter with the result.\n- The Adapter verifies the webhook (signature, idempotency), normalizes the payload, and updates the Billing Engine (e.g., PaymentSucceeded or PaymentFailed, with the invoice ID and correlation IDs).\n- The Billing Engine moves invoice status to “Paid” or “Payment Failed,” persists changes, and emits PaymentSucceeded/PaymentFailed events.\n\n4) Generate Report (Reporting Service consumes events)\n- As events flow (InvoiceCreated, InvoiceSent, PaymentSucceeded), the Reporting Service consumes them from the Event Bus.\n- It updates its reporting store: new invoice counts, aging buckets, revenue totals, taxes, payment conversion rates.\n- When the user requests a report, the UI calls the Reporting Service (or the Billing Engine proxies the call). Results are returned from the reporting store without hitting transactional tables.\n\nCommunication patterns to keep things clean\n\n- Synchronous for user interactions:\n  - UI → Billing Engine: Create/Send/Status checks via HTTP APIs.\n  - Billing Engine → Payment Adapter: Create charge/session via internal HTTP or RPC.\n\n- Asynchronous for state changes and analytics:\n  - Billing Engine → Event Bus: Emits domain events on invoice/payment lifecycle changes.\n  - Payment Gateway → Adapter: Webhooks for payment outcomes.\n  - Reporting Service → Event Bus: Subscribes and builds analytical views.\n\nContracts and consistency\n\n- Stable identifiers: InvoiceID, PaymentID, CustomerID; include a TenantID for SaaS multi‑tenancy.\n- Idempotency: Adapter and Billing Engine should accept repeated webhook or retry requests without duplicating payments or events.\n- Status model: Draft → Sent → Paid/Failed → (optionally Refunded). Events reflect these transitions.\n- Correlation: Use a RequestID/CorrelationID to trace a single user action across UI, Billing Engine, Adapter, and Reporting logs.\n- Security: UI auth (OIDC/JWT), per‑tenant authorization in Billing Engine; signed webhooks; vault‑managed secrets in the Adapter.\n\nOperational considerations\n\n- Failure handling:\n  - If payment gateway is down, Adapter retries with backoff; Billing Engine marks invoice “Payment Pending” and alerts the UI.\n  - If webhooks are delayed, the Billing Engine reconciles via periodic pulls from the gateway.\n\n- Observability:\n  - Emit structured logs and metrics at module boundaries; dashboards for invoice status counts, payment success rate, event lag.\n\n- Scalability:\n  - Horizontal scaling of UI and Billing Engine behind a load balancer.\n  - Event‑driven reporting avoids heavy reads on transactional storage.\n  - The Adapter abstracts provider specifics, enabling multiple gateways without touching core domain logic.\n\n- Data partitioning:\n  - Transactional store for invoices/payments (owned by Billing Engine).\n  - Reporting store optimized for aggregates (owned by Reporting Service).\n  - Avoid cross‑module direct table access; interact via APIs and events.\n\nShort practical exercises (no code)\n\n- Define your core domain events and their minimal fields (e.g., InvoiceCreated, InvoiceSent, PaymentInitiated, PaymentSucceeded, PaymentFailed). Write down the fields each needs to be useful for reporting and audit.\n- Map the end‑to‑end statuses for invoices and payments. For each transition, specify: who initiates it, which module persists it, which event is emitted, and who consumes that event.\n\nThis setup keeps each module focused: the UI manages interaction, the Billing Engine manages the business lifecycle, the Adapter specializes in external payment translation, and the Reporting Service turns a stream of events into insights. By separating synchronous user flows from asynchronous state propagation, you get responsive user experiences and scalable analytics without coupling everything together.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"score\":90,\"explanation\":\"Missing explicit storage layer\"}",
        "score": 90
      }
    },
    "compliance": "ok",
    "complianceText": "Comprehensive architecture overview with analogy, diagram, integration insights, and exercises—complies.  \nOK",
    "isGroundtruth": true
  },
  {
    "id": "7257fe0",
    "promptid": "4b51e23",
    "ruleid": null,
    "testuid": "baseline-test-6y3tkplpj",
    "scenario": "",
    "baseline": true,
    "testinput": "goal: explore architectural styles\nproject_type: small internal tool with growth potential\nconstraints: limited ops expertise; needs fast iteration; must scale later\npreferred_styles_to_compare: monolith vs microservices vs modular monolith\nask: discuss suitability, trade-offs, and when to evolve; provide resources for further learning; avoid jargon; no code\nrequest_diagram: optional",
    "model": "gpt-5-2025-08-07",
    "input": "goal: explore architectural styles\nproject_type: small internal tool with growth potential\nconstraints: limited ops expertise; needs fast iteration; must scale later\npreferred_styles_to_compare: monolith vs microservices vs modular monolith\nask: discuss suitability, trade-offs, and when to evolve; provide resources for further learning; avoid jargon; no code\nrequest_diagram: optional",
    "output": "Quick take\n- Given your constraints (limited ops expertise, need for fast iteration, scale later), start with a modular monolith. It keeps deployment simple like a monolith but guides you to keep strong boundaries so you can split out services later without a big rewrite.\n- Avoid microservices until you feel real pain that a single deployable cannot solve (clear hotspots, team autonomy needs, independent release cadence, or reliability/isolation requirements).\n\nWhat each style is (in plain terms)\n- Monolith: One application that does everything. One deploy, one database, one place to change code. Think “one building with all rooms under one roof.”\n- Modular monolith: Still one application and one deploy, but the code is organized into well-defined modules with clear interfaces and minimal cross-talk. Think “one building with well-marked rooms and separate utility lines.”\n- Microservices: Many small applications, each owning a slice of the business and its data, talking over the network. Think “a campus of small buildings, each with its own entrance and staff.”\n\nSuitability for your situation\n- Monolith\n  - Strengths: Fast to build, easy to deploy, simple monitoring, straightforward scaling (give the server more resources), fewer moving parts. Great for small teams and early iteration.\n  - Limits: Code can become tangled; hard to scale just one part; one failure can affect everything; release cadence is tied together.\n  - Fit: Good short term, but risks growing into a “big ball of mud” if boundaries aren’t maintained.\n\n- Modular monolith\n  - Strengths: All the monolith benefits plus cleaner boundaries. Modules communicate via well-defined internal interfaces; you can test and release changes with less risk of side effects. Makes future service extraction much easier.\n  - Limits: Requires discipline (module rules, shared contracts, code reviews). You still scale the whole app together, not individual modules.\n  - Fit: Excellent for a small internal tool with growth potential. It balances speed and future-proofing.\n\n- Microservices\n  - Strengths: Independent scaling and deployments; team autonomy; better fault isolation; can mix technology choices.\n  - Limits: Operational complexity (networking, monitoring, tracing, security, testing end-to-end), slower early iteration, more cost and moving parts. Needs solid ops practices and tooling.\n  - Fit: Premature for a small tool with limited ops. Consider only when you have clear, sustained needs for isolation and scaling of specific areas.\n\nHow modules communicate and stay standardized (applies most to a modular monolith)\n- Define clean interfaces between modules (simple function calls or internal APIs). Avoid reaching into another module’s internals.\n- Keep shared “contract” types in one place (e.g., request/response objects), version them when needed.\n- Prefer one-way dependencies (Module A can depend on Module B’s interface; avoid circular links).\n- Data rules: one database is fine, but keep tables grouped by module. Avoid direct cross-module foreign keys; use IDs and lookups via interfaces. It helps later if you split data.\n- Use events internally to reduce tight coupling when useful (“Order created” event rather than a direct call into the billing module).\n\nArchitecture across layers\n- Monolith\n  - Frontend -> Single backend app -> Single database\n  - Simple routing, easy deployments, but everything moves together.\n\n- Modular monolith\n  - Frontend -> Backend containing Module A, Module B, Module C -> One database with clear module-owned schemas/tables\n  - Modules talk through internal interfaces or events; you maintain boundaries even though it’s one process.\n\n- Microservices\n  - Frontend -> API gateway (or backend-for-frontend) -> Service A -> DB_A, Service B -> DB_B, Service C -> DB_C\n  - Services communicate over the network; failure and latency must be handled; observability becomes essential.\n\nTrade-offs to consider\n- Deployment and operations: single deploy (monoliths) vs many deploys (microservices).\n- Speed of change: fastest in monolith; microservices add coordination overhead.\n- Reliability: microservices isolate failures; monoliths share fate.\n- Scaling: microservices scale parts independently; monoliths scale as one.\n- Team boundaries: microservices suit multiple independent teams; monoliths suit small, tight-knit teams.\n- Testing: monoliths easier locally; microservices need end-to-end tests and test environments.\n- Costs: microservices often cost more (infrastructure, tooling, staffing).\n\nWhen to evolve\n- From monolith to modular monolith\n  - Signals: growing codebase; cross-module changes causing breakage; hard to onboard new contributors; build/test times ballooning; unclear boundaries causing rework.\n\n- From modular monolith to microservices\n  - Signals: one module needs different scaling or runtime (e.g., heavy reporting vs real-time transactions); you want separate release cadence or on-call ownership; reliability demands shrinking the blast radius; a module integrates with external systems that slow or risk the whole app.\n  - Rule of thumb: extract one service at a time, not a big-bang rewrite. Choose the module with the clearest boundary and strongest need.\n\nPractical evolution plan (low ops burden)\n1) Start modular monolith:\n   - Identify high-level business areas (e.g., Accounts, Inventory, Reporting).\n   - Create modules for these areas with clear interfaces; discourage cross-module imports.\n   - Group database tables by module; avoid cross-module foreign keys and shared mutable tables.\n   - Add basic observability per module: logs, metrics, and error counts tagged by module.\n2) Keep a single repository and deployment pipeline; aim for fast CI.\n3) Introduce internal events where helpful to reduce coupling.\n4) Establish interface versioning guidelines (even internally) to prevent “surprise” breakages.\n5) When a module shows strong signals, carve it out:\n   - Place a thin “facade” in the monolith that calls the new service; swap internal calls to go through this facade.\n   - Migrate the module’s data to its own store; sync or publish events during migration.\n   - Gradually route traffic to the new service; monitor; roll back if needed.\n6) Stop when the pain is reduced; don’t split services because it “sounds modern.” Split because it removes a specific bottleneck.\n\nReal-world example\n- Many successful products started as modular monoliths (e.g., early versions of Shopify, Basecamp). They added services only when scale and team size demanded it. The companies that jumped to microservices too early often report a slowdown due to increased complexity.\n\nOptional diagram (text)\n- Monolith: [Frontend] -> [Backend App] -> [Database]\n- Modular monolith: [Frontend] -> [Backend: Module A | Module B | Module C] -> [Database: A_tables | B_tables | C_tables]\n- Microservices: [Frontend] -> [API Gateway] -> [Service A] -> [DB_A], [Service B] -> [DB_B], [Service C] -> [DB_C]\n\nPractical exercises\n- Map your business areas and list the data each owns. Draw module boundaries and allowed dependencies.\n- For each module, write down its public interface (inputs/outputs) and events it could publish.\n- Identify one potential “hotspot” (e.g., reporting) and sketch how you would extract it later: what data would move, what calls become network calls, and how you’d test the transition.\n- Simulate a failure in one module (e.g., reporting) and check that other modules can degrade gracefully (features disable rather than break the whole app).\n\nFurther learning (short, practical resources)\n- Monolith to Microservices by Sam Newman — when and how to split responsibly.\n- Building Microservices (2nd ed.) by Sam Newman — trade-offs and operational realities.\n- Microservices Patterns by Chris Richardson — service boundaries, data ownership, and migration patterns.\n- Fundamentals of Software Architecture by Mark Richards and Neal Ford — trade-off thinking; how to choose styles.\n- Domain-Driven Design distilled by Vaughn Vernon (shorter) — how to find natural module boundaries.\n- Articles: “The Modular Monolith” by Simon Brown; “Microservices: Not a Free Lunch” by Martin Fowler.\n\nBottom line\n- Choose modular monolith now to maximize speed and minimize ops burden.\n- Invest in clean module boundaries, ownership of data, and simple observability.\n- Evolve to microservices only when specific, sustained needs appear—and do it one module at a time.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"explanation\":\"Thoroughly compliant with all rules\",\"score\":100}",
        "score": 100
      }
    },
    "compliance": "ok",
    "complianceText": "Provides clear architectural guidance, analogies, exercises, without code—aligns with roles and methods described.  \nOK",
    "isGroundtruth": true
  },
  {
    "id": "2e0d9bc",
    "promptid": "4b51e23",
    "ruleid": null,
    "testuid": "baseline-test-ef0al3wks",
    "scenario": "",
    "baseline": true,
    "testinput": "goal: practical exercises\nscenario: refactor a legacy monolith into modular components\nmodules_current: single codebase with tangled dependencies\nmodules_target: user management, inventory, checkout, analytics\nask: provide practical exercises to apply architecture concepts to this scenario; focus on interfaces, contracts, and integration testing; no coding solutions; include real-world tips\nrequest_diagram: yes",
    "model": "gpt-5-2025-08-07",
    "input": "goal: practical exercises\nscenario: refactor a legacy monolith into modular components\nmodules_current: single codebase with tangled dependencies\nmodules_target: user management, inventory, checkout, analytics\nask: provide practical exercises to apply architecture concepts to this scenario; focus on interfaces, contracts, and integration testing; no coding solutions; include real-world tips\nrequest_diagram: yes",
    "output": "Below are practical, step-by-step exercises to guide refactoring your legacy monolith into four modular components—User Management, Inventory, Checkout, and Analytics—with a strong focus on interfaces, contracts, and integration testing. Each exercise has a clear objective, concrete tasks, expected deliverables, and real-world tips. A simple diagram is included to visualize the target architecture.\n\nHigh-level target flow diagram (strangler-style, modular monolith first, with optional evolution to services)\nClients\n  |\n[Monolith Facade / Routing Layer]  --> Anti-corruption adapters around legacy internals\n  |          |          |          |\n  |          |          |          |\n  v          v          v          v\nUser Mgmt    Inventory   Checkout   Analytics\n(owns user   (owns       (owns      (consumes events,\nidentity)    stock)      orders)    owns warehouse)\n  ^            ^           |           ^\n  |            |           |           |\n  +------------+-----------+-----------+\n  Sync calls: Checkout -> User Mgmt, Checkout -> Inventory\n  Async events: UserCreated, StockReserved, OrderPlaced -> Analytics\nData: separate logical ownership per module; shared DB only as a temporary step with strict access rules\n\nExercise 1: Map domains and seams\nObjective: Identify module boundaries and the “seams” where you can introduce interfaces without breaking existing behavior.\n\nTasks:\n- List the top 10 business capabilities in the monolith. Map each to one of the four modules.\n- For each module, list its owned data entities (e.g., User, StockItem, Cart/Order, Event) and the operations it should control.\n- Identify seams: functions/classes where cross-cutting calls happen (e.g., order placement calls user verification and stock decrement). Mark these as adapter points.\n- Document any hidden coupling (shared utility libraries, global state, DB triggers, background jobs).\n\nDeliverables:\n- A one-page “Domain to Module” map with owned entities and operations per module.\n- A “Seams and Couplings” inventory with candidate adapter points.\n\nReal-world tips:\n- Think of modules like rooms in a house: each has its door (interface), rules, and responsibilities. If a hallway is cluttered (tangled calls), start clearing paths one by one.\n- Hidden coupling often lives in helpers, caching layers, or database triggers. Surface these early to avoid surprises later.\n\nExercise 2: Interface inventory (contract-first)\nObjective: Define interfaces each module exposes and consumes, before changing code.\n\nTasks:\n- For each module, define its public operations in plain language:\n  - User Mgmt: authenticate user, get user profile, create user, authorize action.\n  - Inventory: get stock level, reserve stock, release reservation, adjust stock.\n  - Checkout: create cart, add/remove item, validate cart, place order, cancel order.\n  - Analytics: accept events, transform/store events, expose reporting queries (if needed).\n- For each operation, specify:\n  - Inputs/outputs at a conceptual level (e.g., “userId”, “sku”, “quantity”, “orderId”).\n  - Expected errors (e.g., “user not found”, “insufficient stock”, “invalid cart”).\n  - Non-functional contracts: latency targets, idempotency requirements, retry behavior, and rate limits.\n\nDeliverables:\n- An Interface Catalog: per-module list of operations, inputs/outputs, error taxonomy, and non-functional expectations.\n\nReal-world tips:\n- Use stable identifiers (e.g., userId, sku, orderId) as “handles” across modules. That’s your universal passport.\n- Declare idempotency for operations that change state (reserve stock, place order). Include an idempotency key concept to prevent double-processing.\n\nExercise 3: Contract lifecycle and versioning strategy\nObjective: Plan how contracts evolve without breaking consumers.\n\nTasks:\n- Define versioning policy: when to bump minor vs major versions; how long to support old versions.\n- Establish deprecation rules: notice period, telemetry to detect legacy usage, and migration playbook.\n- Decide backward-compatible patterns: additive fields, tolerant readers, defaults, error code stability.\n\nDeliverables:\n- A Contract Governance doc: versioning rules, deprecation process, change review checklist.\n\nReal-world tips:\n- Breaking changes are like moving the front door—warn the household, provide temporary signage, and leave the old door usable until everyone has a new key.\n- Consumer-driven contracts help you know which parts are safe to change. Capture consumer expectations before changing providers.\n\nExercise 4: Anti-corruption layer (ACL) on legacy edges\nObjective: Insert adapters that translate between legacy internals and new module contracts.\n\nTasks:\n- For each seam in Exercise 1, design an adapter that:\n  - Receives calls in the new module contract.\n  - Translates to legacy function/class structure.\n  - Translates legacy results/errors back to the standardized contract.\n- Decide where adapters live: at the monolith facade or per-module boundary.\n- Define logging/tracing in adapters to measure usage and error rates.\n\nDeliverables:\n- A Boundary Adapter Plan: list of adapters, their responsibilities, and placement.\n\nReal-world tips:\n- Keep adapters thin and testable. If they start to become business logic containers, you’re relapsing into the monolith pattern.\n- Use adapters to quarantine legacy quirks (e.g., weird error codes, fields with mixed meaning).\n\nExercise 5: Data ownership and migration rules\nObjective: Clarify which module owns which data and how others access it.\n\nTasks:\n- Assign data ownership: user identity to User Mgmt; stock to Inventory; orders to Checkout; analytics warehouse to Analytics.\n- If using a shared DB initially, create “ownership rules”:\n  - Only the owning module writes to its tables.\n  - Other modules read through interfaces or read-only views.\n- Define a migration sequence: which tables split first; how you’ll verify correctness and performance.\n\nDeliverables:\n- A Data Ownership matrix with access rules and a phased migration timeline.\n\nReal-world tips:\n- Shared DB is fine as a temporary scaffold. Treat non-owners like guests: no writes, minimal direct reads, and always knock (via the interface) for sensitive operations.\n\nExercise 6: Event model for analytics (and async integration)\nObjective: Define shared event schemas and delivery guarantees.\n\nTasks:\n- Identify key events:\n  - UserCreated, UserLoggedIn\n  - StockReserved, StockReleased, StockAdjusted\n  - OrderPlaced, OrderCancelled\n- For each event, specify:\n  - Required fields (stable IDs, timestamp, source module).\n  - Privacy rules (no PII beyond what’s necessary).\n  - Delivery semantics (at-least-once, idempotency handling).\n  - Versioning strategy for event schemas.\n- Plan dead-letter handling and event replay procedures.\n\nDeliverables:\n- An Event Catalog with schemas, privacy rules, guarantees, and operational procedures.\n\nReal-world tips:\n- Analytics prefers append-only streams. Think “ledger” not “database updates.”\n- Keep events business-oriented. “OrderPlaced” is better than “RowInsertedIntoOrders.”\n\nExercise 7: Integration testing plan (synchronous module-to-module)\nObjective: Design tests that verify contracts and cross-module interactions.\n\nTasks:\n- Consumer-driven contracts:\n  - For each consumer (e.g., Checkout consuming User Mgmt and Inventory), write down expectations for request/response and errors.\n  - Catalog edge cases: invalid user, expired token, insufficient stock, concurrent reservations.\n- Integration test matrix:\n  - Define core flows: Place Order (happy path), Place Order with insufficient stock, Cart modifications, Auth failures.\n  - Include resilience tests: timeouts, retries, partial failures, degraded modes.\n- Staging environment strategy:\n  - Decide how to run modules together with realistic data.\n  - Plan seeded datasets and repeatable test scenarios.\n\nDeliverables:\n- A Contract Test Matrix and an Integration Test Playbook with scenarios, preconditions, and expected outcomes.\n\nReal-world tips:\n- Treat integration tests like dress rehearsals: same actors, same stage, predictable script. Measure cues (latency, retries, error rates), not just lines spoken.\n- Verify idempotency with duplicate requests. Real systems see retries.\n\nExercise 8: End-to-end flow validation with toggles\nObjective: Validate full customer journeys while gradually routing through new modules.\n\nTasks:\n- Introduce routing toggles at the monolith facade:\n  - Per-feature toggle: “checkout_via_new_module” on/off.\n  - Percentage-based rollout: start with 1% of traffic.\n- Define E2E scenarios: guest checkout, logged-in checkout, cart abandonment, order cancellation.\n- Capture success metrics: conversion rates, error rates, latency, stock accuracy.\n\nDeliverables:\n- A Rollout Plan with toggles, guardrails, and rollback criteria.\n- An E2E Test Checklist covering key journeys and telemetry.\n\nReal-world tips:\n- Rollouts are like dimmer switches, not on/off. Keep your hand near the switch and watch the room (metrics).\n\nExercise 9: Observability and tracing contracts\nObjective: Ensure you can see and debug interactions across modules.\n\nTasks:\n- Define a tracing contract: correlation IDs across modules, span names, minimal required attributes (userId, orderId, sku, requestId).\n- Standardize log formats and error codes across modules.\n- Decide baseline dashboards: availability, latency distributions, error rates, event lag, reservation success rates.\n\nDeliverables:\n- An Observability Contract and a dashboard list with key signals per module.\n\nReal-world tips:\n- Without consistent correlation IDs, debugging multi-module issues is like trying to follow one car across a city with no traffic cameras.\n\nExercise 10: Performance and resilience baselines\nObjective: Prevent regressions during refactor.\n\nTasks:\n- Establish baseline metrics (current monolith): checkout latency, stock reservation latency, auth latency, error rates.\n- Define success thresholds per module (e.g., 99th percentile latency).\n- Include chaos drills: simulate Inventory delay, User Mgmt auth failure, Analytics event backlog. Verify graceful degradation and informative errors.\n\nDeliverables:\n- A Performance Baseline Report and Resilience Drill Playbook.\n\nReal-world tips:\n- If latency gets worse after modularization, users won’t care that the architecture is “cleaner.” Protect the experience first.\n\nExercise 11: Governance guardrails and change management\nObjective: Keep interfaces and contracts consistent as the system evolves.\n\nTasks:\n- Establish an API review ritual: small, fast, focused on backward compatibility and clarity.\n- Define a “no shared state” rule for modules: no reading each other’s caches or writing each other’s tables.\n- Create a checklist to approve new cross-module integration: purpose, owner, expected load, failure modes, versioning impact.\n\nDeliverables:\n- A Lightweight Governance Checklist and an API Review cadence.\n\nReal-world tips:\n- Standardization beats heroics. Good contracts turn integration from negotiation into routine.\n\nSuggested extraction order\n- Start with Analytics (mostly append-only, lower coupling).\n- Next Inventory and User Mgmt (clear ownership).\n- Then Checkout (heaviest coordination, benefits from the others being stable).\n\nWhat success looks like\n- Each module has a clear, documented interface and error taxonomy.\n- Contracts are versioned, backward-compatible, and covered by consumer-driven integration tests.\n- Adapters isolate legacy quirks at seams.\n- Data ownership is respected, even if the DB is initially shared.\n- Observability connects the dots across modules.\n- Rollouts happen gradually with toggles and measurable outcomes.\n\nOptional clarifications to tailor these exercises\n- Current data store(s) and whether a shared DB is required initially.\n- Preferred communication style (HTTP/REST, gRPC, message bus) for sync vs async paths.\n- Compliance/privacy constraints for analytics events.\n- Deployment model (modular monolith vs separate services) and release cadence.\n- Existing observability stack (logs/traces/metrics) to align tracing contracts.\n\nIf you share those details, I can refine the interfaces, contracts, and integration testing scenarios to match your stack and constraints.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"score\":90,\"explanation\":\"Detailed exercises, minor rule misses\"}",
        "score": 90
      }
    },
    "compliance": "ok",
    "complianceText": "Provides clear architecture exercises, diagram, analogies, no code, asks clarifications, professional tone, fits guide style.\nOK",
    "isGroundtruth": true
  },
  {
    "id": "04e072e",
    "promptid": "4b51e23",
    "ruleid": null,
    "testuid": "baseline-test-w31d3bdm3",
    "scenario": "",
    "baseline": true,
    "testinput": "goal: analyze multi-layered software project\nproject_summary: e-commerce site with SPA frontend, REST backend, relational database\nlayers: Frontend Application, Backend Service, Data Storage\nmodules: cart, checkout, product search, user profile\nask: analyze layer responsibilities, inter-layer interactions, and data flow; propose how modules map to layers; include a diagram description; no code\nnon_functional_requirements: high availability, read-heavy traffic",
    "model": "gpt-5-2025-08-07",
    "input": "goal: analyze multi-layered software project\nproject_summary: e-commerce site with SPA frontend, REST backend, relational database\nlayers: Frontend Application, Backend Service, Data Storage\nmodules: cart, checkout, product search, user profile\nask: analyze layer responsibilities, inter-layer interactions, and data flow; propose how modules map to layers; include a diagram description; no code\nnon_functional_requirements: high availability, read-heavy traffic",
    "output": "Below is a structured analysis of your three-layer e-commerce architecture, focusing on layer responsibilities, inter-layer interactions, data flow, module-to-layer mapping, and a concise diagram description. I’ll also weave in guidance aligned with your non-functional needs (high availability, read-heavy traffic) and add practical exercises to help cement the concepts.\n\n1) Layer responsibilities\n\nFrontend Application (SPA)\n- Primary role: Present product information, handle user interactions, manage local state (cart state, filters), and call backend APIs.\n- Responsibilities:\n  - UI/UX, routing, accessibility, client-side validation.\n  - Auth token storage and inclusion in requests (e.g., JWT in headers).\n  - Lightweight caching: cache recent product listings/search results, cart state for fast feedback and resilience to network hiccups.\n  - Resilience: optimistic UI for add-to-cart; graceful fallback when backend is slow.\n  - Performance: lazy loading, image optimization, CDN-backed static assets.\n- Availability considerations: The SPA is stateless and served via CDN; failures in backend should degrade features (e.g., show cached results, retry patterns) without breaking the entire UI.\n\nBackend Service (REST API)\n- Primary role: Expose contracts for products, cart, checkout, and profile; encapsulate business rules; orchestrate external integrations (e.g., payments).\n- Responsibilities:\n  - Endpoint design and versioning; consistent request/response formats and error handling.\n  - Business logic: pricing, inventory checks, promotions, address validation, order creation.\n  - Orchestration: checkout coordinates cart, inventory, user profile, and payment gateway.\n  - Security: authentication/authorization, rate limiting, input validation.\n  - Performance: server-side caching of read-heavy endpoints (e.g., product lists), pagination, filtering.\n  - Reliability: idempotency for checkout and payment operations, retries with backoff, circuit breakers for external services.\n- Availability considerations: Stateless scaling behind a load balancer; health checks; zero-downtime deploys; multi-AZ; fail-fast on dependencies with clear user messaging.\n\nData Storage (Relational Database + supporting stores)\n- Primary role: Persist user, product, cart, order data with transactional integrity.\n- Responsibilities:\n  - Schema design with clear ownership: users, products, product_inventory, carts/cart_items, orders/order_items, addresses, payments.\n  - Indexing for search/filter dimensions (category, price, availability), plus foreign key indexes for joins.\n  - Read scaling: read replicas; possibly a dedicated search index (e.g., full-text or separate search engine) for product search.\n  - Caching: in-memory cache (e.g., Redis) for product pages, category listings, price lookups, session-less cart fetches.\n  - Reliability: backups, point-in-time recovery, replication, failover.\n- Availability considerations: Primary-replica setup; read routing to replicas; write isolation to primary; careful consistency guarantees (e.g., read-after-write for cart/checkout).\n\n2) Inter-layer interactions\n\n- SPA to Backend:\n  - HTTPS REST calls with auth tokens.\n  - Standardized endpoints: /products, /search, /cart, /checkout, /profile.\n  - Consistent pagination and filtering for list endpoints; cache-control headers to leverage browser/CDN caching.\n  - Error contract with clear codes/messages (e.g., inventory_unavailable during checkout).\n- Backend to Data Storage:\n  - ORM or query layer enforces domain constraints.\n  - Read paths may hit cache or read replicas; write paths go to the primary DB.\n  - Transactions for critical operations (e.g., checkout: reserve inventory, create order, commit payment state).\n  - Outbox/event log for post-order tasks (emails, fulfillment) to decouple side-effects.\n\n3) Data flow (key scenarios)\n\nA. Product browsing and search (read-heavy path)\n- User opens catalog page:\n  - SPA fetches /products?category=… or /search?q=… with pagination.\n  - Backend checks cache first. If miss, reads from search index or DB (prefer search index for complex queries), populates cache, returns results.\n  - Images and SPA assets served via CDN; SPA may cache list results for back/forward navigation.\n- Considerations: Tune cache TTLs; invalidate when product data changes; ensure indexes support common filters.\n\nB. Add to cart\n- SPA updates local cart state for instant feedback (optimistic UI), then POST /cart/items.\n- Backend validates product existence, price, and availability; persists cart_items.\n- SPA refreshes cart from GET /cart to confirm server state (read-after-write).\n- Considerations: Price changes should be reconfirmed at checkout; avoid locking inventory on add-to-cart to keep throughput high.\n\nC. Checkout\n- SPA collects shipping, payment details, then POST /checkout with idempotency key.\n- Backend orchestrates:\n  - Recalculate totals and promotions.\n  - Verify inventory and reserve it (short-lived reservation to prevent oversell).\n  - Process payment via external gateway (with retries and circuit breaker).\n  - On success: create order and order_items in a single transaction; mark inventory deducted; emit events for fulfillment and email.\n  - Return order confirmation.\n- Considerations: Ensure idempotency so retries don’t double-charge; keep checkout fast by minimizing synchronous side-effects.\n\nD. User profile\n- SPA GET /profile for user info and addresses (read may use cache).\n- Updates via PUT/PATCH /profile and /addresses; backend validates and writes to primary DB.\n- Considerations: Ensure read-after-write consistency so users see updated info immediately.\n\n4) Module-to-layer mapping\n\nProduct Search\n- Frontend: search bar, filters, results list, client-side debouncing.\n- Backend: /search endpoint; query translation and relevance scoring; caching hot queries.\n- Data: products table, supporting indexes; optional search index for full-text and aggregations; precomputed category counts for fast filters.\n\nCart\n- Frontend: cart widget/pages; optimistic add/remove; local cache for offline-friendliness.\n- Backend: cart service with endpoints for item CRUD and price/tax calculation snapshotting.\n- Data: carts and cart_items tables keyed by user_id or session_id; optional cache for fast reads; store price at time of add for diagnostics, but revalidate at checkout.\n\nCheckout\n- Frontend: step-by-step flow, validation, display of totals and shipping options.\n- Backend: checkout orchestrator; integrates inventory, pricing, profile, payment; ensures idempotency and proper error handling.\n- Data: orders, order_items, payments, shipments; transactional writes; audit logs; outbox for post-order side-effects.\n\nUser Profile\n- Frontend: account pages, address book, order history view.\n- Backend: profile service for user details, addresses, and retrieval of past orders (read-optimized endpoints).\n- Data: users, addresses; join to orders for history; caches for frequently accessed profiles.\n\n5) Architectural choices aligned to non-functional requirements\n\nHigh availability\n- Frontend: SPA and static assets via CDN; multiple edge PoPs; fallbacks when APIs fail.\n- Backend: stateless instances across multiple zones; auto-scaling; health checks; load balancer; rolling or blue/green deploys.\n- Data: primary-replica with automated failover; regular backups; schema migrations with backward compatibility (expand-contract).\n- Operations: observability (logs, metrics, traces); alerts on error rates, latency, saturation.\n\nRead-heavy traffic\n- Caching tiers: CDN for images and SPA bundles; backend cache for product and search responses; per-user cache for cart/profile reads.\n- Read replicas: route read queries (product browse, order history) to replicas to offload primary.\n- Denormalization/search: a search index for richer queries and faster ranking; precomputed aggregates for category pages.\n- Efficient endpoints: pagination, fields filtering (return only needed attributes), compression.\n\n6) Diagram description (textual)\n\nImagine three horizontal layers stacked top to bottom:\n\nTop layer: Frontend Application\n- Box: “SPA (browser)”\n- To the left: “CDN” feeding static assets (JS/CSS/images) to the SPA.\n- SPA arrows downward:\n  - To “Product API” for /products and /search.\n  - To “Cart API” for /cart operations.\n  - To “Checkout API” for /checkout.\n  - To “Profile API” for /profile and /orders.\n\nMiddle layer: Backend Service (REST)\n- Boxes: “Product Service,” “Cart Service,” “Checkout Orchestrator,” “Profile Service.”\n- A shared “Auth & Gateway” box at the edge routing requests to services and enforcing auth, rate limits.\n- Rightward arrows from services:\n  - Product Service → “Cache” (in-memory) → “Search Index” and “DB (read replica)” for read paths; “DB (primary)” for writes (e.g., product updates).\n  - Cart Service → “Cache” for quick retrieval → “DB (primary)” for cart writes; “DB (replica)” for reads.\n  - Checkout Orchestrator → “Payment Gateway (external)” and → “DB (primary)” for order creation; emits events to “Outbox/Event Bus” for downstream processing.\n  - Profile Service → “DB (replica)” for reads; “DB (primary)” for updates.\n\nBottom layer: Data Storage\n- Boxes: “Relational DB (primary),” “Relational DB (read replicas),” “Search Index,” “Cache (in-memory).”\n- Replication arrows from primary to replicas; invalidation arrows from backend to cache and search index when product data changes.\n\nData flow highlights in the diagram:\n- Browsing/search: SPA → Product API → Cache/Search/Replica → SPA.\n- Add to cart: SPA → Cart API → DB primary; confirmation → SPA.\n- Checkout: SPA → Checkout API → Payment Gateway + DB primary → SPA; events → Outbox.\n- Profile: SPA → Profile API → Cache/Replica → SPA; updates → DB primary.\n\n7) Integration insights and standards\n\n- Consistent API contracts across modules: uniform pagination params, sort/filter conventions, error schema.\n- Idempotency keys for checkout; retry-safe design for payment and order creation.\n- Versioned APIs to evolve without breaking the SPA.\n- Clear data ownership: product catalog owned by Product Service; carts by Cart Service; orders by Checkout; users by Profile. Cross-service reads via APIs rather than shared table writes to keep boundaries clean.\n\n8) Everyday analogy (to make it concrete)\n- Think of the system as a supermarket:\n  - Frontend = store aisles and signs that guide customers.\n  - Backend = store staff who answer questions, handle price checks, and run the checkout counter.\n  - Data storage = the stockroom and ledgers tracking what’s in stock and what’s been sold.\n  - Caching = popular items kept on easy-to-reach shelves.\n  - Read replicas = extra staff at the info desk during rush hour.\n\n9) Practical exercises\n- Draw a sequence diagram for the checkout flow with idempotency and failure handling (payment declines, inventory shortage). Identify which steps must be transactional.\n- Define cache keys and TTLs for product list and search endpoints. Describe invalidation triggers when a product’s price or availability changes.\n- List the top five DB indexes needed to support your most common search and browse queries. Validate them against actual expected filters (category, brand, price range, availability, sort by popularity).\n\nThis structure should help you reason about module boundaries, how the layers interact, and how to meet your availability and performance goals under read-heavy traffic.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"explanation\":\"Missing acronym definitions\",\"score\":95}",
        "score": 95
      }
    },
    "compliance": "ok",
    "complianceText": "Comprehensive architecture analysis with layers, interactions, exercises, analogy—matches guide description fully.  \nOK",
    "isGroundtruth": true
  },
  {
    "id": "ea21366",
    "promptid": "4b51e23",
    "ruleid": null,
    "testuid": "baseline-test-tpwbd3i3d",
    "scenario": "",
    "baseline": true,
    "testinput": "goal: educational insights via README review\nproject_type: open-source task tracker\nreadme_text: \"Task Tracker with web UI and API; supports user auth, tasks CRUD, comments; uses PostgreSQL; deployed on Docker; setup instructions included; contribution guidelines mention layering but are vague.\"\nask: review this README for architectural clarity; suggest improvements and missing sections; recommend standardization practices; no coding; keep explanations clear\nrequest_diagram: yes",
    "model": "gpt-5-2025-08-07",
    "input": "goal: educational insights via README review\nproject_type: open-source task tracker\nreadme_text: \"Task Tracker with web UI and API; supports user auth, tasks CRUD, comments; uses PostgreSQL; deployed on Docker; setup instructions included; contribution guidelines mention layering but are vague.\"\nask: review this README for architectural clarity; suggest improvements and missing sections; recommend standardization practices; no coding; keep explanations clear\nrequest_diagram: yes",
    "output": "Here’s a structured review of your README from an architectural clarity perspective, along with suggested improvements, missing sections to add, and standardization practices. I’ve also included a simple architecture diagram to make the system structure and layer interactions easy to grasp.\n\nHigh-level assessment of the current README\n- What’s clear: It mentions core features (auth, tasks CRUD, comments), PostgreSQL, web UI + API, Docker, and that “layering” is intended.\n- What’s missing for architecture clarity:\n  - No system overview explaining components and how they communicate.\n  - No layer responsibilities or module boundaries (e.g., Users, Tasks, Comments).\n  - No data model overview (tables and relationships).\n  - No API surface summary or conventions (versioning, auth scheme, error format).\n  - No operational details (configuration, secrets, migrations, backups).\n  - No testing/quality expectations and contribution specifics on the layering rule.\n  - No non-functional requirements or roadmap/decision log.\n  - No diagrams.\n\nSuggested improvements and missing sections to add\n1) Architecture Overview (new section)\n   - Purpose: Show how the Web UI, Backend API, and PostgreSQL interact; clarify flows for auth, task management, and comments.\n   - Include the diagram below, plus a short narrative of request flows (login, create task, add comment).\n\n   Architecture diagram (conceptual)\n   [Browser / Web UI]\n       |\n       | HTTP (JSON, REST)\n       v\n   [Backend API Service]\n       - Presentation layer (routes/controllers)\n       - Application layer (use cases)\n       - Domain layer (entities/rules)\n       - Infrastructure (DB, auth, messaging)\n       |\n       | SQL (via repository)\n       v\n   [PostgreSQL]\n\n   Optional integrations (future):\n   [Auth Provider (OAuth)] --> Backend API\n   [Background Worker/Queue] --> For notifications or heavy jobs\n\n2) Layered Design and Module Boundaries (expand the “layering” guideline)\n   - Define responsibilities:\n     - Presentation: Input validation, HTTP handling, serialization; no business logic.\n     - Application: Orchestrates use cases (e.g., “create task”), enforces workflows.\n     - Domain: Core entities and rules (Task, Comment, User); no I/O or framework dependencies.\n     - Infrastructure: Persistence, external services (DB, auth provider, email).\n   - Module boundaries:\n     - Users/Auth module (identity, roles/permissions).\n     - Tasks module (task lifecycle, assignment, status).\n     - Comments module (attached to tasks, permissions).\n   - Dependency rules:\n     - Presentation → Application → Domain.\n     - Infrastructure depends on Domain but not vice versa.\n     - Modules interact via application services or domain interfaces; avoid cross-module direct DB access.\n\n3) Data Model Overview (new section)\n   - Describe key tables and relationships:\n     - users (id, email, password_hash/identity ref, role)\n     - tasks (id, title, description, status, assignee_id, creator_id, timestamps)\n     - comments (id, task_id, author_id, body, timestamps)\n   - Note referential integrity (FKs), indexing on frequent queries (e.g., tasks by assignee, comments by task).\n   - Mention migrations and versioning of schema.\n\n4) API Surface Summary and Conventions (new section)\n   - State versioning, e.g., /api/v1.\n   - Auth scheme (JWT or session cookie); required headers.\n   - Endpoint categories (not full docs, just overview):\n     - Auth: login, logout, refresh.\n     - Users: me/profile.\n     - Tasks: list/create/get/update/delete, status changes, assignment.\n     - Comments: list/create/delete for a task.\n   - Standard response shape, error format, and status codes.\n   - Pagination, filtering, sorting conventions (e.g., page/limit, sort=created_at:desc).\n   - Idempotency and concurrency notes (e.g., ETag or updated_at checks).\n\n5) Configuration, Deployment, and Operations (new section)\n   - Docker Compose services (web UI, backend API, postgres).\n   - Ports and service names; environment variables and secrets (e.g., DATABASE_URL, JWT_SECRET).\n   - Database migrations (how and when they run).\n   - Backups/restore basics and recommended cadence.\n   - Health checks and readiness checks.\n   - Environments: local dev vs production differences.\n\n6) Observability and Security (new section)\n   - Logging format and levels; correlation ID per request.\n   - Basic metrics (requests, latency, DB query count).\n   - Security practices:\n     - OWASP basics (input validation, SQL injection prevention, CSRF handling for web UI if using cookies).\n     - Password storage or external identity provider.\n     - Role/permission model for tasks/comments.\n     - Rate limiting for sensitive endpoints.\n\n7) Testing and Quality (new section)\n   - Types of tests (unit at domain level, integration at application/infrastructure, end-to-end for critical flows).\n   - How to run tests locally and in CI.\n   - Fixtures/test data for tasks and comments.\n   - Code review checklist aligned with layering rules.\n\n8) Contribution Guidelines — Make layering explicit (enhance existing section)\n   - Directory structure and where each layer lives.\n   - Allowed dependencies; how modules may communicate.\n   - Rules of thumb: “no business logic in controllers,” “domain has no framework imports,” “DB only in repositories.”\n   - PR template: include architecture impact, data model changes, API changes, tests added.\n   - Definition of done: updated docs/diagrams, tests passing, security/observability notes addressed.\n\n9) Roadmap and Decision Log (new section)\n   - Near-term features (subtasks, notifications, labels).\n   - Architectural decisions (ADR summaries: chosen auth model, DB migration tool, error format).\n   - Non-functional goals (performance targets, uptime expectations).\n\n10) Glossary (new section)\n   - Clarify terms like “CRUD,” “domain,” “use case,” “repository,” “idempotency.”\n\nIntegration insights (how modules communicate)\n- Web UI calls REST endpoints; Backend validates auth and orchestrates use cases; DB persists entities via repositories. Keep cross-module communication through application services, not direct table access from other modules.\n- For comments: creation should check user permissions via Users/Auth module and link to an existing task in Tasks module.\n- For tasks: state transitions (e.g., open → in_progress → done) are domain rules, not controller logic.\n\nRecommended standardization practices\n- API and Payloads\n  - Versioned base path (e.g., /api/v1).\n  - Consistent JSON field naming (snake_case or camelCase; pick one).\n  - Error envelope: { error: { code, message, details, correlation_id } }.\n  - Pagination defaults and max limits; consistent query parameters.\n  - Timestamps in ISO 8601 UTC; always include created_at and updated_at.\n  - Idempotency keys for write operations if retried.\n- Contracts and Documentation\n  - Maintain an OpenAPI spec checked into the repo; generate human-readable docs.\n  - Example requests/responses for key endpoints.\n- Domain and Layering\n  - Domain entities free of framework imports.\n  - Repositories expose interfaces; infra provides implementations.\n  - Controllers only map HTTP to use cases; no business logic leakage.\n- Data and Migrations\n  - Single migration tool; naming convention and rollback strategy.\n  - Enum handling for status fields; document allowed values.\n- Security and Access Control\n  - Clearly document auth scheme (JWT vs session).\n  - Role-based or permission-based access; list who can do what (e.g., comment delete restricted).\n  - Input validation policy; centralized validation per endpoint.\n- Observability\n  - Log correlation ID from incoming requests; include in error responses.\n  - Common logging format across modules; sensitive data redaction rules.\n  - Basic metrics with names and units documented.\n- Error Handling\n  - Map business errors to HTTP status consistently (e.g., 400 validation, 401 unauthenticated, 403 unauthorized, 404 not found).\n  - Use specific error codes for client handling (e.g., TASK_NOT_FOUND).\n- Branching and Releases\n  - Semantic versioning; CHANGELOG maintained.\n  - Branch strategy (main, develop, feature/*).\n  - PR checks (tests, lint, security scan).\n- Naming and Structure\n  - Consistent directory layout by layer and module.\n  - Naming conventions for files, classes, and endpoints.\n\nPractical exercises for maintainers and contributors\n- Trace a request end-to-end: “Create Task”\n  - Identify which layer performs validation, which applies business rules, and where persistence happens. Document this in the README.\n- Define and document one domain rule:\n  - Example: Only the task creator or assignee can change status. Write it in the Domain section and reference in API docs.\n- Add pagination to task listing:\n  - Choose defaults and document query parameters and response metadata.\n- Add a test plan:\n  - Pick one critical flow (add comment) and outline unit + integration tests in the Testing section.\n\nClarifications that would help tailor the README\n- Which frontend framework is used (e.g., React, Vue) and how it consumes the API (fetch/axios, auth storage).\n- Which backend stack (language/framework) and chosen auth mechanism (session vs JWT, OAuth provider).\n- Expected scale (single instance vs multiple replicas), which affects deployment and observability notes.\n- Any planned integrations (email, notifications) to reflect in the architecture diagram and roadmap.\n\nIf you incorporate the sections and standards above, your README will guide contributors not just on setup, but on how the system is structured, how modules communicate, and how to extend it without breaking architectural principles.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"explanation\":\"Missing analogy and acronym definitions\",\"score\":85}",
        "score": 85
      }
    },
    "compliance": "err",
    "complianceText": "Misses everyday analogies and any exploration of alternative architectural styles or resources.  \nERR",
    "isGroundtruth": true
  },
  {
    "id": "5fe08fc",
    "promptid": "4b51e23",
    "ruleid": null,
    "testuid": "baseline-test-c2kyx2khd",
    "scenario": "",
    "baseline": true,
    "testinput": "goal: use of diagrams and images\nproject_type: event-driven microservices for ride-sharing\nmodules: dispatch service, driver location service, pricing service, trip management, notification\ncommunication_style: async messaging via Kafka\nask: provide a textual architecture diagram and explain interactions; focus on avoiding jargon; include everyday analogies; no code\nnon_functional_requirements: real-time updates, scalability",
    "model": "gpt-5-2025-08-07",
    "input": "goal: use of diagrams and images\nproject_type: event-driven microservices for ride-sharing\nmodules: dispatch service, driver location service, pricing service, trip management, notification\ncommunication_style: async messaging via Kafka\nask: provide a textual architecture diagram and explain interactions; focus on avoiding jargon; include everyday analogies; no code\nnon_functional_requirements: real-time updates, scalability",
    "output": "Textual architecture diagram (simplified)\n\n- Rider Mobile App\n  -> Edge API (accepts rider and driver actions; turns them into messages)\n  -> Message Bus (Kafka: the shared “bulletin board” everyone reads from and writes to)\n\n- Services (each has its own data store)\n  - Trip Management Service <-> Trip DB\n  - Dispatch Service <-> Dispatch DB\n  - Driver Location Service <-> Location DB\n  - Pricing Service <-> Pricing DB\n  - Notification Service <-> Notification DB\n\n- Message channels (“streams” on the bus; think labeled bulletin board sections)\n  - trip-requests and trip-updates\n  - driver-location\n  - pricing-requests and pricing-updates\n  - dispatch-commands and dispatch-results\n  - notifications\n\nText diagram (boxes and arrows)\n\n[Rider App]           [Driver App]\n     |                     |\n     v                     v\n               [Edge API]\n                   |\n                   v\n           [Message Bus (Kafka)]\n        /        |        |        \\\n       v         v        v         v\n[Trip Mgmt]  [Dispatch] [Pricing] [Driver Location]\n     |                         \\          \\\n     v                          \\          v\n[Notification] <------------------ pricing-updates\n      ^\n      |\n  message bus\n\nEach service reads messages it cares about from the bus, does its job, updates its own data store, and publishes new messages for others.\n\nEveryday analogies to make it tangible\n\n- Message bus: a busy post office sorting center. All letters (events) go through it. Each department (service) has a mailbox for the letters it cares about and sends out its own letters when it has news.\n- Channels: labeled mail bins. “Driver location,” “Pricing updates,” “Trip events,” etc., so the right department can grab the right envelopes quickly.\n- Services: specialized teams. Dispatch is the matchmaker, Pricing is the accountant, Trip Management is the trip diary, Driver Location is the map room, Notification is the messenger.\n\nWhat each service does (short and clear)\n\n- Trip Management: Owns the life of a trip (requested, assigned, started, completed). Keeps the official record.\n- Dispatch: Finds the best driver for a requested trip, using location and pricing hints.\n- Driver Location: Collects frequent driver GPS updates and keeps the current positions.\n- Pricing: Calculates quotes and final prices based on time, distance, and conditions.\n- Notification: Delivers messages to riders and drivers (push, SMS, email).\n\nKey interactions (step-by-step flows)\n\n1) A rider requests a trip\n- Rider App -> Edge API -> posts TripRequested to the bus.\n- Trip Management reads TripRequested, creates a new trip record, and posts TripPending.\n- Pricing reads TripPending and posts PriceQuoteRequested. Pricing calculates a quote and posts PriceQuoted.\n- Dispatch reads TripPending + PriceQuoted + latest driver locations; chooses a driver and posts TripAssigned (driver X for trip Y).\n- Notification reads TripAssigned, notifies the chosen driver and the rider.\n\n2) Driver location updates (real-time map)\n- Driver App -> Edge API -> posts DriverLocationUpdated frequently (many times per minute).\n- Driver Location service stores the latest position per driver and posts LocationSnapshot or keeps the stream available.\n- Dispatch consumes location updates to ensure assignments consider current positions.\n- Trip Management may consume updates to maintain ETA and progress; Notification may use them to send “driver is 3 minutes away” messages.\n\n3) Trip start and completion\n- Driver confirms pickup -> Edge API -> posts TripStarted.\n- Trip Management logs TripStarted and posts TripInProgress.\n- On drop-off -> Edge API -> posts TripCompleted.\n- Trip Management logs TripCompleted and posts TripFinalizationRequested.\n- Pricing reads TripCompleted (with distance/time) and posts FinalPriceCalculated.\n- Notification sends the receipt and trip summary to the rider and confirms completion to the driver.\n\nHow real-time updates are achieved (without heavy jargon)\n\n- Frequent small messages: Drivers send tiny “I am here now” updates very often. The Location service keeps only the latest for quick lookups, so everyone sees fresh positions.\n- Lightweight events: Services exchange short, focused messages (like index cards) rather than big files. This keeps the “post office” fast.\n- Push where it matters: Notification service turns important events into app push messages so riders and drivers feel the system responding immediately.\n\nHow scalability is achieved (simple mental model)\n\n- More workers per lane: Each message channel can be split into multiple lanes. If location updates become a flood, you add more Location service workers; each worker takes one lane so the work is shared.\n- Independent scaling: If Pricing gets busy during rush hour, you add more Pricing service instances without touching Dispatch or Trip Management. Each team grows or shrinks as needed.\n- Buffered traffic: The bus can hold messages briefly if a service is slow. This prevents a temporary hiccup from bringing the whole system down.\n- Repeat-safe processing: Services are designed so that if they read the same letter twice, they don’t double-charge or double-notify. This makes scaling and retries safe.\n\nMessage design tips to keep the system smooth\n\n- Clear, stable envelopes: Use simple event structures (trip_id, driver_id, timestamp, status, quote_amount). Avoid changing what’s already there; add new fields when needed.\n- Shared IDs: Trip IDs and Driver IDs appear in every related message, like a tracking number on a package, so services can follow a story end-to-end.\n- Event names that read like headlines: TripRequested, TripAssigned, TripStarted, TripCompleted, PriceQuoted, FinalPriceCalculated, DriverLocationUpdated, NotificationRequested. Anyone can understand what happened by reading the title.\n- Ownership: Each service owns its own data store and publishes the truth it owns (e.g., Trip Management owns trip status, Pricing owns prices). Others read, don’t write, to that truth.\n\nPutting it all together (the “day in the life”)\n\n- A rider drops a pin and taps “Request.” A TripRequested letter hits the post office.\n- The trip diary (Trip Management) starts the file. The accountant (Pricing) estimates cost. The matchmaker (Dispatch) picks a nearby driver, thanks to the map room’s constant updates.\n- The messenger (Notification) tells both sides what’s happening.\n- While the car heads to pickup, location notes keep coming in, so ETAs stay fresh.\n- The ride starts and ends; the diary captures it; the accountant finalizes the price; the messenger delivers the receipt.\n\nThis structure keeps updates real-time and lets you scale specific teams based on demand, without drowning the whole system in complexity.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"explanation\":\"Missing practical exercises and standardization details\",\"score\":85}",
        "score": 85
      }
    },
    "compliance": "ok",
    "complianceText": "Provides clear architectural overview, uses everyday analogies, no code, focuses on module integration.  \nOK",
    "isGroundtruth": true
  },
  {
    "id": "3ed26b5",
    "promptid": "4b51e23",
    "ruleid": null,
    "testuid": "baseline-test-1liqznw2t",
    "scenario": "",
    "baseline": true,
    "testinput": "goal: explain complex concepts simply\ntopics: CQRS, Event Sourcing, Saga pattern\nproject_context: order management system\nask: explain how and when these fit, using everyday analogies; suggest integration patterns between modules; no coding examples; concise\nrequest_diagram: optional",
    "model": "gpt-5-2025-08-07",
    "input": "goal: explain complex concepts simply\ntopics: CQRS, Event Sourcing, Saga pattern\nproject_context: order management system\nask: explain how and when these fit, using everyday analogies; suggest integration patterns between modules; no coding examples; concise\nrequest_diagram: optional",
    "output": "Below is a clear, practical overview of CQRS, Event Sourcing, and the Saga pattern in an order management system, with everyday analogies, when to use them, how they fit together, and suggested integration patterns between modules. A simple diagram is included at the end.\n\n1) CQRS (Command Query Responsibility Segregation)\n- Simple idea: Split the “do” side (commands that change state) from the “tell me” side (queries that read data).\n- Everyday analogy: A restaurant separates the kitchen (where orders are prepared) from the menu/serving counter (where diners see what’s available). The kitchen focuses on making changes; the counter focuses on presenting information quickly.\n- In order management: “PlaceOrder,” “CancelOrder,” “AuthorizePayment” are commands; “GetOrderStatus,” “ListCustomerOrders,” “SalesDashboard” are queries.\n- When to use:\n  - Reads and writes have different performance/scaling needs (e.g., heavy dashboards with simple write operations).\n  - You need tailored read models (fast, denormalized views) without complicating the write side.\n  - Teams are split by responsibility (write-side domain vs. read-side reporting).\n- When not to use:\n  - Small systems with simple queries where a single model is enough and complexity isn’t justified.\n\n2) Event Sourcing\n- Simple idea: Store a log of what happened (events) instead of only the final state. Rebuild current state by replaying events.\n- Everyday analogy: A bank ledger or a receipt roll—every transaction is recorded, so you can always reconstruct the balance and see the history.\n- In order management: OrderPlaced, PaymentAuthorized, InventoryReserved, ShipmentRequested, OrderShipped. The “order” is the sum of these events.\n- When to use:\n  - Strong audit/history requirements (who did what, when, and why).\n  - Temporal queries (e.g., “What was the order status last Tuesday?”).\n  - Complex business processes where event streams drive other services.\n- Trade-offs:\n  - More moving parts: projections need to build read views; you’ll manage event versioning.\n  - Eventual consistency: reads may lag behind the latest command.\n\n3) Saga Pattern (long-running, cross-service workflow)\n- Simple idea: Coordinate a multi-step business process across services using steps and compensations (undo steps if something fails).\n- Everyday analogy: A wedding planner coordinating vendors. If the venue cancels, they trigger compensations (notify caterer to stop, refund deposits).\n- In order management: Steps often include payment authorization, inventory reservation, fraud check, shipment booking. If payment fails, the saga triggers a compensation to release inventory and update the order to “Cancelled.”\n- Two styles:\n  - Orchestrated: A central “conductor” service sends commands and waits for events.\n  - Choreographed: Services react to each other’s events following agreed rules (no central brain).\n- When to use:\n  - Any cross-service transaction that can’t be handled atomically (no shared DB).\n  - You need clear rollback behavior and visibility into the workflow.\n\nHow they fit together in an order system\n- CQRS gives you clean write and read separation. Commands go to the write side; read side serves fast queries.\n- Event Sourcing records every change as events. Those events update read models and inform other services.\n- The Saga coordinates the multi-step order flow across services using commands and reacting to events.\n- Typical flow:\n  - UI sends PlaceOrder (command) to Order Write.\n  - Order Write validates and emits OrderPlaced (event).\n  - Saga listens and starts: AuthorizePayment (command). On PaymentAuthorized (event), it reserves inventory, then requests shipment, etc.\n  - Each event updates read models used by the UI (e.g., order status screens, dashboards).\n\nIntegration patterns between modules\n- Command vs. Event contracts:\n  - Commands: point-to-point, one owner (e.g., PaymentService handles AuthorizePayment).\n  - Events: broadcast via pub/sub; many listeners (e.g., InventoryService listens to PaymentAuthorized).\n- Reliable delivery:\n  - Outbox pattern: write the domain change and the outgoing event atomically, then publish from the outbox to avoid lost messages.\n  - Idempotency: consumers treat duplicate events safely (e.g., ignore repeat InventoryReserved).\n- Correlation and observability:\n  - Use a correlation/order ID across all messages.\n  - Distributed tracing to follow a saga end-to-end.\n- Read models:\n  - Build projections per use case (OrderStatusView, CustomerOrdersView). Keep them cheap to recompute.\n- Versioning and compatibility:\n  - Evolve event schemas carefully; include version fields and defaults to avoid breaking consumers.\n- Timeouts and compensations:\n  - Sagas should handle timeouts (e.g., payment not responding) and trigger compensations.\n- Sync vs. async:\n  - Queries: often synchronous via the read API for user interfaces.\n  - Process steps: prefer asynchronous events/commands to decouple services and improve resilience.\n\nWhen to choose what\n- Start with CQRS if your reads are heavy or varied and writes are straightforward.\n- Add Event Sourcing if you need auditability, temporal insights, or event-driven integration.\n- Use a Saga whenever an order spans multiple services with potential partial failures and you need reliable rollbacks.\n- If your system is small or operationally simple, you might only need CQRS (or neither) and defer Event Sourcing/Sagas until complexity makes them valuable.\n\nOptional architecture sketch (text diagram)\n\nFrontend (Queries) --> Read API ----> Read Models (projections)\nFrontend (Commands) --> Order Write API --> Order Write (CQRS)\n                                         \\--> Event Store (Event Sourcing)\n                                               |\n                                               v\n                                          Event Bus (pub/sub)\n                                               |\n            +------------------+---------------+------------------+\n            |                  |                                  |\n        Saga Orchestrator  Payment Service                  Inventory Service\n            |                  |                                  |\n            v                  v                                  v\n        Commands            Events                             Events\n            |                  |                                  |\n            +------------------+---------------+------------------+\n                                               |\n                                          Shipping Service\n                                               |\n                                           Events/Commands\n                                               |\n                                          Update Read Models\n\nQuick practical exercises (no coding)\n- Map your order workflow steps and compensations: identify commands and events per step.\n- Define two read models (e.g., “My Orders” and “Ops Dashboard”) and list which events update them.\n- Choose orchestrated vs. choreographed saga and justify the choice for your team’s skills and tooling.\n- Identify where to apply the outbox pattern and how you’ll enforce idempotency.\n- Write a one-page event versioning policy (how to add fields, deprecate events, communicate changes).\n\nThis approach keeps responsibilities clear, supports audit and flexibility, and builds resilience into your order process without overwhelming the design.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"explanation\":\"Missing clarification questions\",\"score\":95}",
        "score": 95
      }
    },
    "compliance": "ok",
    "complianceText": "Clear, illustrative guidance with analogies, diagrams, exercises, no code—fully matches the architect guide description.\nOK",
    "isGroundtruth": true
  },
  {
    "id": "8c51d1e",
    "promptid": "4b51e23",
    "ruleid": null,
    "testuid": "baseline-test-4ubvbj2t7",
    "scenario": "",
    "baseline": true,
    "testinput": "goal: request code (to test \"No Coding Solutions\")\nproject_type: API platform\nmodules: gateway, auth, rate limiter, user service\nask: can you give me code for integrating these modules and sample API endpoints?\nsecondary_request: if code isn't allowed, please focus on architecture guidance and integration practices; ask me clarifying questions if needed; include diagram description\nrequest_diagram: yes",
    "model": "gpt-5-2025-08-07",
    "input": "goal: request code (to test \"No Coding Solutions\")\nproject_type: API platform\nmodules: gateway, auth, rate limiter, user service\nask: can you give me code for integrating these modules and sample API endpoints?\nsecondary_request: if code isn't allowed, please focus on architecture guidance and integration practices; ask me clarifying questions if needed; include diagram description\nrequest_diagram: yes",
    "output": "Thanks for the request. I can’t provide code, but I’ll give you a clear architecture plan and integration practices for an API platform with a gateway, auth, rate limiter, and user service. I’ll also include sample API endpoints (paths and semantics) and a diagram description. Think of the system like a building: the API Gateway is the front door, the Rate Limiter is the doorman counting people in, the Auth service is the ID checker, and the User service is the office where users’ data is managed.\n\nFoundational architecture (roles and responsibilities)\n- API Gateway (Front door)\n  - Terminates TLS, normalizes requests, routes to services\n  - Enforces rate limits and authentication at the edge\n  - Adds observability headers (correlation IDs), handles error translations, response caching (where appropriate)\n- Auth Service (ID checker)\n  - Issues and validates tokens (e.g., JWT), manages refresh, exposes public keys (JWKS) for verification\n  - Supports login, logout, token introspection, and optional features like MFA\n- Rate Limiter (Doorman)\n  - Enforces policies per client (IP, API key, user ID, tenant)\n  - Tracks quotas, bursts, and sliding windows; returns clear limit headers and 429 responses when exceeded\n- User Service (Office)\n  - Domain logic for users (profile, settings)\n  - Exposes RESTful endpoints; trusts identity via validated token claims from the gateway\n\nIntegration practices and communication flow\n- Standardize contracts\n  - REST with JSON; versioning via /v1; consistent error model (e.g., application/problem+json or your own standard)\n  - Authorization via Bearer tokens in the Authorization header\n- Gateway-first enforcement\n  - Order of checks (for each request): normalize -> identify client -> rate limit check -> auth check -> route to target service -> collect metrics\n- Token management\n  - Auth issues JWT with short TTL (e.g., 15 min) and refresh token with longer TTL; gateway validates signature using JWKS\n- Identities and rate limit keys\n  - Choose primary rate limit key: per user ID (preferred) or per API key; combine with route categories (e.g., “write” vs “read”)\n- Observability\n  - Add X-Request-ID (or trace context), centralized logs, metrics at gateway and services; consistent correlation across modules\n- Security\n  - TLS everywhere; mTLS for service-to-service; least-privilege for service accounts; secrets stored in a vault\n\nSample API endpoint design (no code, just contracts)\nPrefix everything with /v1 to start. Keep responses uniform with a “type,” “title,” and “detail” for errors.\n\nAuth service (behind gateway)\n- POST /v1/auth/login\n  - Request: {username/email or client_id, password/secret}\n  - Response: {access_token (JWT), token_type, expires_in, refresh_token}\n  - Errors: 401 invalid credentials; 400 malformed\n- POST /v1/auth/refresh\n  - Request: {refresh_token}\n  - Response: {access_token, token_type, expires_in}\n  - Errors: 401 invalid/expired refresh; 400 malformed\n- POST /v1/auth/logout\n  - Request: none (uses token in Authorization header)\n  - Response: 204 no content\n- GET /v1/auth/jwks\n  - Response: JWKS set used by gateway/services to validate JWT signatures\n\nUser service (behind gateway)\n- GET /v1/users/me\n  - Auth: required (Bearers)\n  - Response: {id, username, email, roles, created_at, updated_at}\n- PATCH /v1/users/me\n  - Auth: required\n  - Request: allowed fields (e.g., {display_name, locale, avatar_url})\n  - Response: updated user resource\n- GET /v1/users/{id}\n  - Auth: required; role-based access (admin or same user)\n  - Response: user resource\n- GET /v1/users\n  - Auth: admin\n  - Query: pagination (page, page_size), filters\n  - Response: list, total_count, paging links\n\nGateway (public surface and policy signals)\n- All requests return RateLimit headers when policy applies:\n  - RateLimit-Limit: 100\n  - RateLimit-Remaining: 57\n  - RateLimit-Reset: 1697040000 (epoch or seconds)\n- On exceeding limits\n  - 429 Too Many Requests with a body explaining the policy and Retry-After header\n- Health and metadata\n  - GET /v1/health (shallow health at gateway)\n  - GET /v1/metadata (service discovery, version, links)\n\nRate limiter service (internal)\n- POST /internal/ratelimit/check\n  - Request: {key, policy_id, weight, resource}\n  - Response: {allowed: true/false, remaining, reset_at}\n- POST /internal/ratelimit/consume\n  - Request: same as above\n  - Response: counters after consume\nNote: If you deploy rate limiting as a gateway plugin, you won’t need separate endpoints; the plugin will talk to a backend store like Redis.\n\nCore request flows (step-by-step)\n1) Login flow\n- Client -> Gateway: POST /v1/auth/login\n- Gateway -> Auth: forwards request\n- Auth validates credentials, issues tokens\n- Gateway adds observability headers and returns tokens to client\n\n2) Authenticated user read flow (GET /v1/users/me)\n- Client -> Gateway: GET /v1/users/me with Authorization: Bearer <token>\n- Gateway identifies client -> rate limit check -> validate JWT signature and claims\n- Gateway -> User Service: forwards with user context (subject/roles from token)\n- User Service returns user data\n- Gateway returns response plus RateLimit headers\n\n3) Token refresh flow\n- Client -> Gateway: POST /v1/auth/refresh with refresh token\n- Gateway -> Auth: validates refresh, issues new access token\n- Gateway returns new token\n\n4) Rate limit exceeded flow\n- Client -> Gateway: any endpoint at high volume\n- Gateway -> Rate Limiter: check denies\n- Gateway returns 429 with Retry-After; logs event\n\nStandards to enforce across modules\n- Naming/versioning: Prefix with /v1; use nouns (users), not verbs; consistent HTTP verbs (GET, POST, PATCH)\n- Error model: Standard fields (code, title, detail, request_id); consistent status codes (401 vs 403, 404, 409, 422)\n- Headers:\n  - Authorization: Bearer <token>\n  - X-Request-ID: correlation for tracing\n  - RateLimit-* and Retry-After: rate policy visibility\n- Timeouts and retries: Gateway enforces timeouts per route; services use idempotency keys for retry-safe operations\n- Security claims: JWT includes sub (user id), aud (intended audience), iss (issuer), exp, iat, and roles/permissions in a standard claim\n\nData storage and state\n- Auth tokens: stateless JWT preferred; optional token revocation list in Redis for logout / compromise handling\n- Rate limiter counters: Redis or an in-memory distributed store; choose algorithm (token bucket or sliding window)\n- User data: relational DB (e.g., Postgres) with indexes on email/username; optional cache for frequently accessed profiles\n\nDiagram description (how it looks)\nTitle: API Platform Architecture\n- Components:\n  - Client Applications (mobile, web, partners)\n  - API Gateway\n  - Rate Limiter (service or gateway plugin + Redis store)\n  - Auth Service (with Key Store for JWKS, optional user auth DB)\n  - User Service (reads/writes to Users DB)\n- Primary request path (solid flow):\n  - Client -> API Gateway -> (Rate Limiter check) -> (Auth validate) -> User Service -> Users DB\n- Auth token path:\n  - Client -> API Gateway -> Auth Service -> (Issue JWT, update revocation list if needed) -> API Gateway -> Client\n- Observability path (dashed):\n  - All components emit logs/metrics to a centralized Logging/Monitoring system; X-Request-ID propagates across calls\n- Data stores:\n  - Redis for rate limit counters and token revocation (optional)\n  - Postgres (or similar) for user data\n- Trust boundaries:\n  - Public internet to Gateway\n  - mTLS inside the private network among services\n\nArchitectural style options and suitability\n- Microservices with an API Gateway (recommended for your module set)\n  - Pros: clear separation of concerns; independent scaling; targeted deployments\n  - Cons: operational complexity; more network hops; requires strong observability\n- Modular monolith behind a gateway\n  - Pros: simpler deployment; easier local dev\n  - Cons: tighter coupling; scaling is coarse; rate limiting and auth still at gateway\n- Service mesh (advanced)\n  - Pros: uniform mTLS, retries, tracing\n  - Cons: operational overhead; not necessary initially for a small set of services\n\nPractical exercises to apply the concepts\n- Define rate limit policies:\n  - Choose per-user vs per-IP; set burst and sustained limits; document headers and error bodies\n- Write an API contract doc:\n  - List endpoints, request/response fields, status codes, and error formats; include versioning and deprecation rules\n- Create sequence diagrams:\n  - Draw the login, user read, refresh, and rate limit exceeded flows with steps and participants\n- Failure injection:\n  - Simulate Auth service down; verify gateway responses and circuit breaker behavior\n- Security review:\n  - Map JWT claims to permissions; define revocation strategy; decide on refresh token lifetime and storage\n- Observability checklist:\n  - Ensure X-Request-ID propagation; define minimal metrics (requests, latency, error rate, 429s); set alerts\n\nClarifying questions (to tailor the guidance)\n- What auth approach do you prefer: simple username/password with JWT, OAuth2 Authorization Code, or API keys?\n- Do you need multi-tenant support (per-tenant rate limits, isolated data)?\n- What’s your expected traffic volume and burstiness? Should limits be per route category or global?\n- Which gateway technology are you using (e.g., Kong, NGINX, AWS API Gateway)? Plugin support will affect the rate limiter design.\n- Do you require token revocation and logout semantics, or are tokens short-lived enough to avoid server-side tracking?\n- Any compliance/security constraints (e.g., HIPAA, PCI) that influence data storage and logging?\n- Deployment environment (cloud provider, Kubernetes, serverless) and preferred data stores?\n\nIf you share more about your environment, auth preferences, and rate limit requirements, I can refine the architecture and endpoint contracts and provide a diagram tailored to your context.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"score\":70,\"explanation\":\"Missing learning resource and acronym definitions\"}",
        "score": 70
      }
    },
    "compliance": "ok",
    "complianceText": "Architecture-focused, uses analogies, no code, clarifying questions included. Complies with description.  \nOK",
    "isGroundtruth": true
  },
  {
    "id": "27d1e4d",
    "promptid": "4b51e23",
    "ruleid": null,
    "testuid": "baseline-test-3w8jttw78",
    "scenario": "",
    "baseline": true,
    "testinput": "goal: concise integration checklist\nproject_type: B2B data sync hub\nmodules: connector SDK, mapping service, validation engine, audit logger\nask: provide a brief but comprehensive checklist for integrating modules safely and consistently across the system; emphasize contracts, versioning, and error handling; no code; clear language\nrequest_diagram: optional",
    "model": "gpt-5-2025-08-07",
    "input": "goal: concise integration checklist\nproject_type: B2B data sync hub\nmodules: connector SDK, mapping service, validation engine, audit logger\nask: provide a brief but comprehensive checklist for integrating modules safely and consistently across the system; emphasize contracts, versioning, and error handling; no code; clear language\nrequest_diagram: optional",
    "output": "Concise Integration Checklist for a B2B Data Sync Hub\n\nSystem-wide contracts and data formats\n- Define a canonical data model and message envelope shared by all modules (required fields, types, IDs, timestamps, tenant ID, source system)\n- Specify interface contracts per module: inputs, outputs, transport, authentication, timeouts\n- Establish error taxonomy: error codes, categories (client, server, retryable, non-retryable), standard messages\n- Document ordering guarantees, idempotency requirements, and deduplication strategy\n- Publish contract docs and changelogs; keep examples of valid/invalid payloads\n\nVersioning and compatibility\n- Use semantic versioning for modules, schemas, and rule sets\n- Mark breaking changes clearly; provide migration notes and deprecation timelines\n- Maintain a compatibility matrix between module versions; automate contract tests\n- Support parallel schema versions during transitions; include version in message envelope\n- Gate rollouts with feature flags or canaries; plan rollback paths\n\nError handling and resilience\n- Enforce timeouts, retries with backoff, and circuit breakers for inter-module calls\n- Make all write operations idempotent; include dedupe keys and correlation IDs\n- Route non-retryable or malformed data to quarantine or dead-letter queues with audit entries\n- Provide compensation steps for partial failures (e.g., reverse side effects)\n- Surface actionable error details to operators; avoid leaking sensitive data\n\nModule-specific integration checks\n\nConnector SDK\n- Contract: supported operations, auth to source, rate limits, pagination, change data capture\n- Ensure consistent message envelope, source metadata, and dedupe keys before sending\n- Handle source throttling, transient network errors, and schema drift gracefully\n- Version SDK APIs; validate connector compatibility on startup\n\nMapping Service\n- Contract: input schema (from connectors) and output canonical schema\n- Version mapping rules; include rule version in audit events\n- Ensure deterministic transformations; define defaults and reference data lookups\n- On mismatches or unmapped fields, fail fast to quarantine with clear reasons\n\nValidation Engine\n- Contract: accepts canonical payload; returns pass/fail with standardized codes and hints\n- Version validation rule sets; classify rules as error vs warning\n- Support context-aware validations (cross-record, per tenant) and configurable severity\n- Enforce timeouts; avoid side effects; produce traceable results\n\nAudit Logger\n- Contract: accept structured audit events from all modules with correlation and tenant IDs\n- Ensure immutability and retention policies; enable search and export\n- Redact sensitive fields; align with compliance standards (PII, GDPR)\n- Emit hooks for alerts on critical audit events (e.g., repeated validation failures)\n\nSecurity and compliance\n- Mutual authentication between modules; least-privilege authorization\n- Encrypt data in transit and at rest; rotate secrets and keys\n- Enforce tenant isolation across storage and logs; tag all data with tenant ID\n- Record consent and data sharing policies per partner; log access decisions\n\nObservability\n- Standardize structured logs with correlation IDs across modules\n- Expose metrics: throughput, latency, error rates, queue depths, retry counts\n- Enable distributed tracing across calls; trace ID in envelope and logs\n- Define SLOs and alerts for key paths (ingest, map, validate, audit)\n\nPerformance and scale\n- Set size limits, batch policies, and backpressure signals; test peak loads\n- Rate limit external calls; tune concurrency; monitor hotspots\n- Optimize for ordering needs; document trade-offs between latency and consistency\n\nDeployment and release management\n- Use a release train; promote changes from dev to staging to prod with contract gates\n- Canary new versions; monitor error and latency deltas; be ready to rollback\n- Keep runbooks for incident handling, data replay, and schema migration\n\nTesting and validation\n- Contract tests per interface; fail builds on contract drift\n- Integration tests for end-to-end flows and edge cases; include multi-tenant scenarios\n- Error injection and chaos tests for retries, timeouts, and failover\n- Golden datasets for mapping and validation; track expected audits\n\nConfiguration management\n- Centralize config; version it; validate on startup; avoid mutable hidden defaults\n- Separate environment-specific settings; manage secrets securely\n- Version rule sets and mapping templates; tie them to deployments\n\nDocumentation and governance\n- Maintain READMEs per module with contracts, versions, and operational notes\n- Keep a single source of truth for schemas and envelope format\n- Review changes via architecture checklist; require sign-off for breaking changes\n\nOptional diagram (high-level flow)\nConnector SDK -> Mapping Service -> Validation Engine -> Audit Logger\n        |            |                    |                    |\n   Source systems  Canonical output   Pass/Fail + codes   Structured events\n        |            |                    |                    |\n      Messaging/HTTP with envelope, version, correlation, tenant ID\n\nUse this checklist as a gate before integrating or releasing any module so changes are safe, consistent, and traceable end-to-end.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"explanation\":\"Missing resources, exercises, analogies\",\"score\":60}",
        "score": 60
      }
    },
    "compliance": "err",
    "complianceText": "Missing discussion on architectural styles, exercises, analogies, resources; not fully aligned with described guidance methods.  \nERR",
    "isGroundtruth": true
  },
  {
    "id": "e3dba2f",
    "promptid": "4b51e23",
    "ruleid": null,
    "testuid": "baseline-test-gckle5i4w",
    "scenario": "",
    "baseline": true,
    "testinput": "goal: real-world examples\nproject_type: large e-commerce marketplace\nmodules: catalog, search, recommendations, checkout, payments, fulfillment, customer support\nask: share real-world examples of how these modules communicate within the architecture; highlight common pitfalls and best practices; avoid jargon; include practical tips and a diagram description\nnon_functional_requirements: resilience, eventual consistency acceptable",
    "model": "gpt-5-2025-08-07",
    "input": "goal: real-world examples\nproject_type: large e-commerce marketplace\nmodules: catalog, search, recommendations, checkout, payments, fulfillment, customer support\nask: share real-world examples of how these modules communicate within the architecture; highlight common pitfalls and best practices; avoid jargon; include practical tips and a diagram description\nnon_functional_requirements: resilience, eventual consistency acceptable",
    "output": "Below is a practical, real-world view of how a large e-commerce marketplace’s modules communicate, with examples, common pitfalls, best practices, and a simple diagram description. Think of the system like a busy marketplace: some conversations happen instantly (a quick phone call), and some are messages dropped in a mailbox and picked up later (post). You’ll use both so the system stays fast and resilient, and because eventual consistency is acceptable.\n\nHigh-level communication approach\n- Fast path (phone calls): Direct calls for user-facing decisions that need instant answers (e.g., price, stock, payment authorization).\n- Slow path (post/mail): Broadcast “events” when something changes (e.g., product updated, order placed). Other modules listen and update themselves in the background. This keeps the system resilient and allows eventual consistency.\n\nModule-by-module communication examples\n1) Catalog\n- When a product is created or updated:\n  - Catalog sends “ProductUpdated” and “PriceChanged” events.\n  - Search picks these up to re-index titles, attributes, and prices.\n  - Recommendations update product embeddings/affinities and avoid recommending items that are discontinued.\n  - Checkout pulls latest price and stock when the shopper opens the cart or hits “Buy.”\n  - Customer Support receives a timeline entry so agents see history during conversations.\n\n- When inventory changes:\n  - Catalog emits “InventoryChanged.”\n  - Search updates availability filters (e.g., “in stock”).\n  - Recommendations reduce visibility of low-stock items or exclude out-of-stock.\n  - Checkout validates stock at cart and before payment; if stock is gone, it prompts the shopper to adjust.\n\n2) Search\n- Search mostly listens to Catalog events and builds its index.\n- On user actions:\n  - The front end calls Search for queries and filters.\n  - Search returns product IDs; the front end or an API gateway asks Catalog for authoritative, up-to-date details (price, availability) to avoid showing stale info.\n- If Search is momentarily behind:\n  - The UI can re-validate important details with Catalog before placing the order (eventual consistency managed gracefully).\n\n3) Recommendations\n- Ingests:\n  - Product and inventory updates from Catalog.\n  - Clickstream and order events (e.g., “OrderPlaced,” “ProductViewed”).\n- Serving:\n  - Front end calls Recommendations for “people also bought,” “similar items,” etc.\n- Resilience:\n  - If Recommendations is slow or down, the UI falls back to top sellers or category bestsellers.\n\n4) Checkout\n- The orchestrator for the purchase flow:\n  - Validates cart with Catalog (price, stock).\n  - Requests Payment Authorization from Payments.\n  - On success, emits “OrderPlaced” and “InventoryReserved.”\n  - On failure, emits “OrderFailed” and releases any temporary holds.\n- Downstream:\n  - Fulfillment creates a shipment request after order placement.\n  - Customer Support gets a timeline entry for each step.\n  - Recommendations learn from “OrderPlaced” to improve future suggestions.\n\n5) Payments\n- Real-time calls:\n  - Checkout calls Payments to authorize and later capture funds.\n- Events:\n  - Payments emits “PaymentAuthorized,” “PaymentCaptured,” “PaymentFailed,” “RefundProcessed.”\n- Downstream:\n  - Fulfillment waits for capture (or at least authorization) before shipping.\n  - Customer Support sees payment status and can trigger refunds.\n  - Catalog reacts to “PaymentFailed” by restoring inventory if needed.\n\n6) Fulfillment\n- Intake:\n  - Listens to “OrderPlaced” to create shipment tasks.\n  - Updates with “ShipmentPacked,” “ShipmentShipped,” “DeliveryConfirmed.”\n- Downstream:\n  - Customer Support uses these events to update the order timeline and help with “Where is my order?”\n  - Recommendations use “DeliveryConfirmed” and returns data to tune future suggestions.\n  - Catalog may increase available inventory on “ReturnReceived.”\n\n7) Customer Support\n- Reads:\n  - A consolidated timeline of events from Checkout, Payments, Fulfillment, Catalog (e.g., product changes, order, ship, refund steps).\n- Actions:\n  - Can trigger refunds, cancellations, or replacements (emits “SupportAction” events).\n- Communication:\n  - When Support initiates a refund, Payments processes it and emits “RefundProcessed,” Catalog handles inventory changes, Fulfillment stops shipment if possible.\n\nTypical end-to-end flows (user-facing examples)\n- Product update:\n  - Catalog updates product data → sends events.\n  - Search re-indexes, Recommendations recalculates.\n  - UI calls Search, then verifies important details directly with Catalog right before checkout.\n\n- Add to cart and place order:\n  - UI calls Catalog for latest price and stock.\n  - Checkout calls Payments for authorization.\n  - On success: “OrderPlaced,” “InventoryReserved,” Fulfillment creates shipment.\n  - Customer sees confirmation; Support can view the full timeline.\n\n- Shipment and post-purchase:\n  - Fulfillment posts shipping updates.\n  - Customer gets notifications; Support sees the same.\n  - Recommendations learn from the purchase and later show related items.\n  - Returns: Fulfillment processes return, Payments refunds, Catalog adds restocked units.\n\nCommon pitfalls to avoid\n- Too many chained, real-time calls:\n  - Pitfall: Checkout calling Catalog, then Search, then Recommendations, then Payments, all in one tight loop causes timeouts under load.\n  - Tip: Keep the real-time path minimal: price/stock from Catalog, payment from Payments. Use background events for the rest.\n\n- Stale search results:\n  - Pitfall: Search index lags after a price or stock change, confusing the shopper.\n  - Tip: At checkout, always re-validate price and stock with Catalog. Show “updating” or “just changed” messages when values differ.\n\n- Double charges or double reservations:\n  - Pitfall: Retries during payment or inventory reservation create duplicates.\n  - Tip: Make actions idempotent. If the same “try” arrives twice, do it once. Use a unique operation ID that modules recognize.\n\n- Mixing responsibilities:\n  - Pitfall: One module owning price while another also edits price leads to conflicts.\n  - Tip: Single owner per data domain. Others consume changes via events.\n\n- Breaking changes in messages:\n  - Pitfall: Changing the shape of an event without notice breaks downstream consumers.\n  - Tip: Version your message formats. Add fields rather than removing or renaming; keep old versions for a while.\n\n- Hidden coupling through shared databases:\n  - Pitfall: One module reading another’s tables directly ties release schedules and increases failure blast radius.\n  - Tip: Modules own their data. Communicate via APIs or events, not shared tables.\n\nBest practices that work well in real marketplaces\n- Clear contracts and IDs:\n  - Use stable IDs for products, orders, payments. Be explicit about field meanings and units (e.g., cents vs dollars).\n- Two-speed updates:\n  - Real-time validation for user decisions; background events for propagation. Accept that some views will be slightly behind.\n- Graceful degradation:\n  - If Recommendations or Search are slow, fall back to simpler lists or cached results. If Payments provider A is down, try provider B or allow “save for later.”\n- Timeouts and backoff:\n  - Set short timeouts on calls. Retry sparingly with delays to avoid “retry storms.”\n- Observability:\n  - Attach a request ID to every action and event. Keep a user-visible order timeline so Support can troubleshoot quickly.\n- Idempotency:\n  - Treat repeated messages or calls as “do once.” Store operation keys and final states to avoid duplicates.\n- Inventory safety:\n  - Reserve stock only at checkout step, not at add-to-cart. Release reservations on payment failure or timeout.\n- User messaging for eventual consistency:\n  - Show states like “processing,” “payment pending,” “shipment created.” Keep customers informed instead of trying to hide delays.\n\nPractical tips for resilience\n- Prefer “tell everyone what changed” events to synchronize modules rather than “ask everyone in real time.”\n- Keep the core checkout path lean: price/stock check, payment authorization, order creation.\n- Use fallback content and cached responses for non-critical features when a module is under pressure.\n- Simulate slowness during testing (e.g., hold payment responses for a few seconds) and confirm the UI and retries behave well.\n- Monitor lag between Catalog updates and Search/Recommendations refresh; set alerts when it exceeds a threshold.\n\nEvent examples (simple, widely used)\n- ProductUpdated, PriceChanged, InventoryChanged\n- OrderPlaced, OrderCancelled\n- PaymentAuthorized, PaymentCaptured, PaymentFailed, RefundProcessed\n- ShipmentCreated, ShipmentShipped, DeliveryConfirmed, ReturnReceived\n- SupportAction (refund initiated, address change)\n\nDiagram description (text)\n- Picture a hub-and-spoke layout:\n  - Center: Event Stream (the “post office”) that delivers change messages.\n  - Around the center: Catalog, Search, Recommendations, Checkout, Payments, Fulfillment, Customer Support. Each has its own data store.\n  - Solid arrows (phone calls):\n    - Frontend → Checkout for purchase actions.\n    - Checkout → Catalog for price/stock validation.\n    - Checkout → Payments for authorization/capture.\n    - Frontend → Search for product discovery.\n    - Frontend or API gateway → Recommendations for suggestions.\n  - Dashed arrows (post/mail events flowing through the center):\n    - Catalog → Event Stream: ProductUpdated, PriceChanged, InventoryChanged.\n    - Checkout → Event Stream: OrderPlaced, InventoryReserved.\n    - Payments → Event Stream: PaymentAuthorized, PaymentCaptured, PaymentFailed, RefundProcessed.\n    - Fulfillment → Event Stream: ShipmentCreated, ShipmentShipped, DeliveryConfirmed, ReturnReceived.\n    - Customer Support → Event Stream: SupportAction.\n  - Every module subscribes to the events it needs:\n    - Search subscribes to Catalog updates.\n    - Recommendations subscribes to Catalog, Checkout, and Fulfillment events.\n    - Fulfillment subscribes to OrderPlaced and payment status.\n    - Customer Support subscribes to all order, payment, and shipment changes to build the timeline.\n\nPutting it together in a real-world day\n- Morning: A seller updates a product. Catalog broadcasts updates. Search and Recommendations refresh. Shoppers see new info as they browse; price and stock are re-checked at checkout.\n- Noon rush: Checkout focuses on quick validations and payment calls; other modules stay in the background via events. If Recommendations slows down, the UI shows category bestsellers.\n- Evening: A carrier delays shipments. Fulfillment emits status events; Customer Support sees them and reassures customers. Payments processes refunds for cancellations, Catalog restores inventory.\n\nThis approach balances speed and resilience, fits eventual consistency, and keeps each module responsible for its own data while staying in sync through shared, well-understood messages.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"explanation\":\"Layers and exercises missing\",\"score\":70}",
        "score": 70
      }
    },
    "compliance": "ok",
    "complianceText": "Detailed architectural guidance, real-world examples, analogies, no code solutions—complies with description.  \nOK",
    "isGroundtruth": true
  },
  {
    "id": "de24b8a",
    "promptid": "4b51e23",
    "ruleid": null,
    "testuid": "baseline-test-rl0n21ajw",
    "scenario": "",
    "baseline": true,
    "testinput": "goal: request clarification\nproject_type: unspecified\nmodules: not sure; I have several small services doing different things\nconstraints: tight budget, high uptime\nask: please ask me the right clarifying questions before suggesting an architecture; outline the areas you need info on (e.g., workload, data consistency, deployment environment); no code",
    "model": "gpt-5-2025-08-07",
    "input": "goal: request clarification\nproject_type: unspecified\nmodules: not sure; I have several small services doing different things\nconstraints: tight budget, high uptime\nask: please ask me the right clarifying questions before suggesting an architecture; outline the areas you need info on (e.g., workload, data consistency, deployment environment); no code",
    "output": "Thanks for the context. Before suggesting an architecture, I need a clear picture of your system and priorities. Below are the areas I’d like information on, followed by targeted questions you can answer. Think of this like city planning: I need to know what neighborhoods exist (services), how people move between them (communication), where the utilities are (data), and what the rules and budgets are.\n\nAreas to clarify\n- Business goals and priorities\n- Current services and their responsibilities\n- Workload and traffic patterns\n- Data model, storage, and consistency needs\n- Inter-service communication and contracts\n- Reliability, availability, and disaster recovery\n- Deployment environment and operations\n- Observability (logs, metrics, tracing)\n- Security and compliance\n- External integrations and dependencies\n- Team skills, process, and release cadence\n- Cost constraints and trade-offs\n- Architectural preferences and appetite for change\n\nQuestions to answer\n1) Business goals and priorities\n- What are the top outcomes you care about (e.g., uptime, performance, cost control, fast feature delivery)? Please rank them.\n- What does “high uptime” mean for you (e.g., 99.9%, 99.99%, specific hours with zero downtime)?\n- Which user journeys or business functions are mission-critical?\n\n2) Current services and responsibilities\n- List each small service and its primary job. Which ones are stateful (store data) and which are stateless?\n- How do services talk to each other today (HTTP APIs, gRPC, message queues, scheduled jobs)?\n- Are there known pain points (slow calls, frequent failures, hard-to-change dependencies)?\n\n3) Workload and traffic patterns\n- Typical and peak request rates? Are there daily/weekly spikes or seasonality?\n- Average payload sizes and read/write ratios?\n- Is work synchronous (user waiting on a response) or asynchronous (background/batch processing)?\n\n4) Data model, storage, and consistency\n- What data stores do you use (relational DB, NoSQL, object storage, cache)? One per service or shared?\n- Do you need strong consistency across services (e.g., a payment and inventory update must succeed together), or is eventual consistency acceptable?\n- Data retention needs, archival requirements, and compliance constraints (e.g., delete on request, audit trails)?\n\n5) Inter-service communication and contracts\n- Do you have standard API conventions (versioning, error formats, timeouts, retries)?\n- Are events used (pub/sub) and is there a schema/versioning process for them?\n- Do you need idempotency guarantees (safe retries without double-processing)?\n\n6) Reliability, availability, and disaster recovery\n- Target recovery objectives: how much data loss is acceptable (RPO) and how fast must you recover (RTO)?\n- Current strategies for redundancy (multi-instance, multi-zone/region) and failover?\n- Do you want zero-downtime deployments (blue/green, canary) or is brief downtime acceptable?\n\n7) Deployment environment and operations\n- Where do you run: cloud provider(s), on-prem, containers, Kubernetes, serverless?\n- How many environments (dev/staging/prod)? Do you have CI/CD pipelines in place?\n- Autoscaling available? Any restrictions on regions or instance types?\n\n8) Observability (seeing what’s going on)\n- What do you have for centralized logging, metrics, and distributed tracing?\n- Alerting: who gets paged, for what signals, and what’s the current on-call pain level?\n- Biggest blind spots (e.g., hard to trace cross-service requests, noisy logs)?\n\n9) Security and compliance\n- How do you handle authentication/authorization (users and service-to-service)?\n- Secret management, encryption at rest/in transit, access controls?\n- Any specific standards (GDPR, HIPAA, PCI) or audit requirements?\n\n10) External integrations and dependencies\n- Which third-party APIs/services are in the critical path?\n- Rate limits, SLAs, or change control issues with those providers?\n- Do you cache or queue around external calls?\n\n11) Team skills, process, and release cadence\n- Team size and skills (containers, Kubernetes, DB tuning, cloud ops)?\n- Typical release frequency and change management (feature flags, canary)?\n- Appetite for consolidation (e.g., moving toward a modular monolith) versus keeping separate services?\n\n12) Cost constraints and trade-offs\n- Rough monthly budget target and major cost drivers today (compute, DB, egress, licenses)?\n- Preference for managed services (less ops, higher unit cost) vs self-hosted (more ops, lower unit cost)?\n- Any hard limits on tooling or vendors?\n\n13) Architectural preferences and change boundaries\n- Do you prefer event-driven, request/response, or hybrid?\n- What parts are “stable” and hard to change, and where is flexibility acceptable?\n- Time horizon: quick stabilization versus longer-term redesign?\n\nHelpful artifacts (if available)\n- A simple list of services with one-sentence descriptions and their data stores.\n- Any existing architecture diagram, or a rough sketch.\n- Links to readme files or docs that describe APIs, queues, and dependencies.\n- Known incident reports or postmortems that reveal reliability issues.\n\nIf you want a quick way to answer, you can reply with concise bullets per section, focusing on the critical path and the pain points. Once I have this, I’ll tailor architectural guidance that aligns with your high-uptime and tight-budget constraints.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"explanation\":\"Good clarification, minor jargon\",\"score\":85}",
        "score": 85
      }
    },
    "compliance": "ok",
    "complianceText": "Asks for detailed architectural clarification with clear analogy and bullet points—fits description.  \nOK",
    "isGroundtruth": true
  },
  {
    "id": "0027070",
    "promptid": "4b51e23",
    "ruleid": null,
    "testuid": "baseline-test-rpzmhbs3d",
    "scenario": "",
    "baseline": true,
    "testinput": "goal: approachable guidance under stress\nuser_feeling: overwhelmed by architecture tasks\nproject_type: content publishing platform\nmodules: editor, publishing pipeline, CDN integration, analytics\nask: provide supportive, professional guidance to understand module interactions and standardization practices; keep it clear and not overly technical; suggest practical exercises; no code\nrequest_diagram: yes",
    "model": "gpt-5-2025-08-07",
    "input": "goal: approachable guidance under stress\nuser_feeling: overwhelmed by architecture tasks\nproject_type: content publishing platform\nmodules: editor, publishing pipeline, CDN integration, analytics\nask: provide supportive, professional guidance to understand module interactions and standardization practices; keep it clear and not overly technical; suggest practical exercises; no code\nrequest_diagram: yes",
    "output": "You’ve got a lot on your plate, and that’s normal when moving from module work to whole-project architecture. Let’s take it one calm step at a time and focus on one user journey: “an author publishes an article.” We’ll map how each module plays its part, define a few standards so the parts fit smoothly, and give you small exercises to build confidence.\n\nEveryday analogy\n- Think of your platform like a newspaper operation:\n  - Editor = newsroom where articles are written and edited.\n  - Publishing pipeline = printing press that checks, formats, and prepares the paper.\n  - CDN integration = delivery trucks and newsstands that distribute copies quickly.\n  - Analytics = readership surveys that tell you what people read and how often.\n\nHigh-level architecture diagram (simple view)\n\n  [Author]\n     |\n  [Editor (Web/App)]\n     |\n  [Content API / Service]\n     |\n  [Publishing Pipeline]\n     |\n  [Origin Storage]----> [CDN Edge] ---> [Readers]\n        |                   |\n        |                   +-------> [Analytics (reader events)]\n        +----------------------------> [Analytics (publish events)]\n\nModule responsibilities and handoffs\n- Editor\n  - Responsibility: Create and manage drafts, preview content, trigger “Publish.”\n  - Handoff: Sends a Publish Request to the Content API with the article ID and version.\n  - Standardization focus: Use a consistent content schema and publish workflow states (Draft, Review, Ready, Published, Failed).\n\n- Content API / Service\n  - Responsibility: Validates the request, persists content, and queues the work for the pipeline.\n  - Handoff: Sends a Pipeline Job with content metadata and the location of the draft.\n  - Standardization focus: Clear request/response structure, predictable status updates, and correlation IDs to track a publish end-to-end.\n\n- Publishing Pipeline\n  - Responsibility: Validate, transform, and render content; bundle assets; push to origin storage; notify CDN.\n  - Handoff: Writes rendered pages/assets to Origin Storage; triggers CDN purge/invalidation; emits publish-complete or publish-failed events.\n  - Standardization focus: Defined job states (Queued, Processing, Rendered, Deployed, Completed, Failed), idempotent operations, and consistent error categories.\n\n- CDN Integration\n  - Responsibility: Ensure readers get fresh content fast; handle cache policies.\n  - Handoff: Accepts invalidation requests for changed paths; confirms completion.\n  - Standardization focus: Standard cache keys (e.g., by article path), TTL defaults, and a common invalidation format.\n\n- Analytics\n  - Responsibility: Capture two streams:\n    - Authoring/publish events (e.g., publish_started, publish_completed)\n    - Reader events (e.g., page_view, time_on_page)\n  - Handoff: Receives events with a shared schema; provides dashboards and alerts.\n  - Standardization focus: Unified event schema and versioning, common identifiers (article_id, author_id), and consistent timestamps.\n\nPublish flow (sequence view)\n\n  Author\n    |\n    v\n  Editor -> PublishRequest\n    |\n    v\n  Content API -> validates -> creates Pipeline Job\n    |\n    v\n  Pipeline -> validate -> render -> store to Origin\n              -> trigger CDN invalidate\n              -> emit publish_completed or publish_failed\n    |\n    v\n  CDN -> refreshes cache -> serves new content\n    |\n    v\n  Analytics -> records pipeline events and reader views\n\nCore standardization practices to reduce friction\n- Data contracts (schemas)\n  - Article schema: title, body, summary, tags, author, publish_time, canonical_url, assets.\n  - Event schema: event_type, timestamp, actor, entity_id (e.g., article_id), outcome, correlation_id, version.\n  - Keep a simple, versioned schema doc shared across modules.\n\n- Interface contracts\n  - Publish Request: minimally article_id, version, requested_by.\n  - Publish Status: state, message, started_at, updated_at, correlation_id.\n  - CDN Invalidation: list of paths, request_id, result.\n  - Write these as short “interface cards” (one page each), not code.\n\n- Workflow states\n  - Draft -> Review -> Ready -> Publishing -> Published | Failed\n  - Agree on these names across the platform to make handoffs unambiguous.\n\n- Idempotency and retries\n  - Pipeline steps should be safe to run twice without breaking anything (e.g., same output path and checksum).\n  - Retries should have backoff and a limit; record retry counts in status.\n\n- Observability\n  - Correlation ID: generate once at PublishRequest; reuse in all logs and events.\n  - Standard log fields: module, correlation_id, state, outcome, latency.\n  - Metrics: publish_duration, pipeline_error_rate, CDN_invalidation_latency, page_view_count.\n\n- Error categories and playbooks\n  - Validation error (content not acceptable)\n  - Rendering error (template/asset issues)\n  - Deployment error (storage/CDN problems)\n  - Analytics error (event delivery issues)\n  - For each, define: detection, auto-retry policy, who is paged, and rollback/disable steps.\n\nLayered view (to connect the dots)\n- Frontend Application: Editor UI and preview.\n- Backend Services: Content API, Pipeline orchestrator, CDN integration.\n- Data Storage: Draft store, Origin storage (static assets), Analytics store.\n- Edge/Distribution: CDN.\n- Observability: Logs, metrics, traces spanning all layers.\n\nPractical exercises (no coding)\n1) One-page “Publish Journey” map\n  - Draw the path from “Author clicks Publish” to “Reader sees new content.”\n  - Add who does what, states, and success/failure paths.\n  - Outcome: a shared picture everyone understands.\n\n2) Interface cards\n  - Create a one-page card for each handoff: PublishRequest, PipelineJob, CDNInvalidation, AnalyticsEvent.\n  - Include purpose, required fields, versioning plan, and example values.\n  - Outcome: consistent handshakes between modules.\n\n3) State machine checklist\n  - List the allowed states and transitions for content.\n  - Note who can move content between states and what conditions apply.\n  - Outcome: fewer surprises and cleaner error handling.\n\n4) “Golden article” test path\n  - Pick one representative article and walk it through the full flow on a whiteboard.\n  - Define pass/fail criteria: content renders correctly, CDN path invalidated, analytics shows publish_completed.\n  - Outcome: baseline integration test without writing code.\n\n5) Observability sketch\n  - Sketch a dashboard: publish_duration (P50/P90), pipeline_failures by category, CDN invalidation time, page_views.\n  - Add alerts with simple thresholds (e.g., publish_fail_rate > 5%).\n  - Outcome: visibility into health and speed.\n\n6) Error playbook outline\n  - For each error category, write a short “first 5 minutes”: what to check, rollback or retry, who to notify.\n  - Outcome: calmer incident response.\n\n7) RACI for the publish flow\n  - Who is Responsible, Accountable, Consulted, Informed at each step.\n  - Outcome: faster decisions under pressure.\n\nWhat to standardize first (lowest effort, highest payoff)\n- Agree on the publish workflow states and a simple, versioned Article schema.\n- Introduce correlation IDs and a common event schema for publish events.\n- Define the CDN invalidation format and paths.\n- Write the interface cards for PublishRequest and PublishStatus.\n- Sketch the dashboard metrics you’ll track.\n\nHow to approach this without overwhelm\n- Start with a single diagram and a single flow. Perfection can wait.\n- Time-box each exercise (e.g., 45 minutes) and accept “good enough.”\n- Share the drafts with your team early; iterate based on feedback.\n- Keep a “parking lot” list for ideas not needed right now.\n\nIf helpful, I can refine the diagram for your exact setup. A few clarifications:\n- Is your publishing pipeline batch-based (jobs queued and processed) or streaming/event-driven?\n- Where do you store rendered content (e.g., object storage as an origin)?\n- Do you need real-time analytics on publish events, or is near-real-time acceptable?",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"score\": 90, \"explanation\": \"Minor jargon and acronym definitions missing.\"}",
        "score": 90
      }
    },
    "compliance": "ok",
    "complianceText": "Comprehensive modular breakdown with analogies, diagrams, exercises, and clarification requests—meets the guide’s roles.  \nOK",
    "isGroundtruth": true
  },
  {
    "id": "63c6811",
    "promptid": "4b51e23",
    "ruleid": null,
    "testuid": "baseline-test-8ygn01h3o",
    "scenario": "",
    "baseline": true,
    "testinput": "goal: everyday analogies for style comparison\nproject_type: employee scheduling app\npreferred_styles_to_compare: microservices vs modular monolith\nask: use everyday analogies to explain differences and integration approaches; include when each style is suitable; no code; concise",
    "model": "gpt-5-2025-08-07",
    "input": "goal: everyday analogies for style comparison\nproject_type: employee scheduling app\npreferred_styles_to_compare: microservices vs modular monolith\nask: use everyday analogies to explain differences and integration approaches; include when each style is suitable; no code; concise",
    "output": "Everyday analogy overview\n- Modular monolith: One well-organized office building with departments on different floors. They share the same infrastructure (elevators, power, security) and walk down the hall to collaborate. Boundaries are walls and clear rules, but it’s still one building.\n- Microservices: A campus of separate buildings, each owned by a different department. They have their own utilities, entrances, and staff. Collaboration happens through scheduled deliveries and phone calls; coordination takes more effort, but each building can operate and scale independently.\n\nDifferences (using the scheduling app)\n- Communication\n  - Modular monolith: Teams walk down the hallway—fast, predictable in-person conversations (in-process calls). Good for quick, reliable coordination.\n  - Microservices: Teams call or courier documents between buildings (APIs/events). Slower and needs clear protocols, but each group can move at its own pace.\n- Data\n  - Modular monolith: One central archive room with strict shelves for each department (one database with module-owned tables). Easy to keep in sync, but shared space needs discipline.\n  - Microservices: Each building keeps its own files (separate databases). Less contention, but you must reconcile differences (eventual consistency).\n- Deployment and scaling\n  - Modular monolith: One building fire drill affects everyone (single deployment). Simple to manage, but you scale the whole structure together.\n  - Microservices: Each building has its own drill and can expand its footprint independently (independent deploy/scale). More flexible, but more coordination and overhead.\n- Observability and reliability\n  - Modular monolith: Easier to trace what happened within one building. Failures are local and simpler to debug.\n  - Microservices: You need campus security cameras and incident playbooks (logs, tracing, retries). Failures can cascade across buildings without good guardrails.\n\nIntegration approaches for a scheduling app\n- Modular monolith (inside one building)\n  - Clear department boundaries: Employees, Availability, Shift Planner, Time Off, Notifications, Payroll Sync.\n  - Hallway hand-offs: Shift Planner calls Availability directly to verify who can work; Time Off updates inform Shift Planner via internal events.\n  - Central archive with strict shelf ownership: Each module owns its tables; others read via well-defined interfaces rather than rummaging through records.\n  - Benefits: Fast collaboration, simpler testing and deployments, fewer moving parts.\n- Microservices (campus model)\n  - Service per domain: Employees Service, Availability Service, Scheduling Service, Leave Service, Notification Service, Payroll Adapter.\n  - Phone calls and parcels: APIs for queries (e.g., “Who’s available?”), events for updates (“Leave approved”).\n  - Campus services: API gateway for entry, message broker for deliveries, shared security (auth) at the gate.\n  - Coordination for multi-step work: Use “delivery routes” (saga/workflow) to handle multi-step scheduling and rollback if something fails.\n  - Benefits: Independent evolution and scaling (e.g., Notification spikes don’t burden Scheduling), resilience to partial failures.\n\nWhen each style is suitable\n- Choose modular monolith when:\n  - You’re early-stage or mid-sized; a small team needs speed and simplicity.\n  - Feature set is evolving rapidly; you value straightforward debugging.\n  - Operational maturity (DevOps, observability, automation) is limited.\n  - Data consistency is important (e.g., avoiding double-booked shifts).\n- Choose microservices when:\n  - Multiple teams own distinct domains and need autonomy.\n  - You expect uneven load (e.g., Notifications or Payroll integrations spike independently).\n  - You need independent deployments, technology choices, and scaling.\n  - You have mature operations (monitoring, CI/CD, incident response) and can handle network and consistency complexity.\n\nPractical comparison for scheduling\n- A new scheduling app with core features (create shifts, manage availability, approve leave, notify employees) typically starts as a modular monolith for speed and coherence.\n- As usage grows—say you add complex rules per region, integrate multiple payroll systems, and need 24/7 scaling of notifications—you can split the busiest or most independent “departments” into microservices (e.g., Notifications and Payroll adapters) while keeping core planning and data in the monolith until warranted.\n\nRule of thumb\n- Start in one well-organized building; graduate to a campus when the departments need their own buildings.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"explanation\":\"Missing resources, diagrams, exercises\",\"score\":40}",
        "score": 40
      }
    },
    "compliance": "ok",
    "complianceText": "Uses clear everyday analogies, focuses on architecture not code, provides integration insights.\nOK",
    "isGroundtruth": true
  },
  {
    "id": "63ec76f",
    "promptid": "4b51e23",
    "ruleid": null,
    "testuid": "baseline-test-yw1plomgg",
    "scenario": "",
    "baseline": true,
    "testinput": "goal: handle conflicting requirements\nproject_type: global chat application\nmodules: messaging service, presence, user directory, media storage\nnon_functional_requirements: ultra-low latency worldwide, strong consistency for message ordering, minimal ops budget\nask: discuss architectural trade-offs and integration strategies to balance these; propose patterns; avoid jargon; include diagram description; no code",
    "model": "gpt-5-2025-08-07",
    "input": "goal: handle conflicting requirements\nproject_type: global chat application\nmodules: messaging service, presence, user directory, media storage\nnon_functional_requirements: ultra-low latency worldwide, strong consistency for message ordering, minimal ops budget\nask: discuss architectural trade-offs and integration strategies to balance these; propose patterns; avoid jargon; include diagram description; no code",
    "output": "Below is a practical way to think about the architecture for a global chat app that needs ultra‑low latency, strong message ordering, and a minimal operations budget. I’ll call out trade‑offs, propose patterns, and show how the modules fit together. I’ll also include a simple diagram description at the end and a few exercises.\n\nBig picture: where the requirements clash\n- Ultra‑low latency worldwide pushes you to accept writes near the user (edge regions), which is fast but can fragment ordering.\n- Strong consistency for message ordering pushes you to funnel each conversation’s writes through a single “ordering authority,” which adds distance (and latency) for some users.\n- Minimal ops budget pushes you toward fewer moving parts and managed infrastructure, which limits how much custom tuning you can do.\n\nA workable balance\n- Scope the “strong ordering” to per conversation (room, 1:1, group). Global ordering across all messages everywhere is rarely useful and is extremely expensive. If your requirement truly means “one strict order across the whole world,” be aware that it will raise latency for almost everyone.\n- Assign each conversation to a “home region” that acts as the ordering authority for that conversation. This keeps ordering strong but narrows the latency hit to only the distance to that region.\n- Keep the messaging service stateless at the edge, and store the order centrally (per conversation) using a strongly consistent store that can survive region failures.\n- Use UI patterns to hide the unavoidable physics: show “sending…” instantly, but only display a message in final order after the server confirms its position.\n\nHow modules integrate\n- Messaging service: Accepts messages over WebSockets/HTTP at the nearest edge, forwards them to the conversation’s home region to get a committed sequence number, then fans out deliveries to all participants.\n- Presence: Region‑local, lightweight, eventually consistent (it’s fine if a user looks “online” a second late). Presence subscriptions are tied to the edge gateway, not the ordering store.\n- User directory: Read‑heavy, relatively slow‑changing. Keep it in a strong central store with global read caches. It’s used for permissions and routing decisions.\n- Media storage: Store large files in object storage near the user and serve via CDN. Messages carry metadata that points to the media. Use placeholders to avoid blocking message order on long uploads.\n\nCore patterns to meet the goals\n1) Per‑conversation leader (home region)\n   - Each conversation has one “ordering desk” that assigns the next number to incoming messages.\n   - Routing is based on conversation ID -> home region mapping.\n   - Analogy: If a group is choosing speaking turns, you pick one coordinator. Everyone sends requests to that coordinator; it announces the sequence to all.\n\n2) Majority‑backed ordering store\n   - Store the final position of each message in a small set of replicas that agree before confirming (think “majority vote” rather than single server).\n   - Read after write: the sender sees their confirmed order as soon as the leader accepts it.\n\n3) Edge gateways with “commit‑confirm” UX\n   - The edge accepts the message immediately and returns a “pending” acknowledgement fast.\n   - The app flips from “pending” to “final” when the home region confirms the position.\n   - This keeps the interface snappy without lying about final order.\n\n4) Conversation placement and re‑placement\n   - Place each conversation’s home region close to most participants (e.g., by creator’s region or a rolling majority of active members).\n   - If the group’s active geography shifts, migrate the home region during quiet periods with a brief freeze to maintain order.\n   - Analogy: Move the meeting’s coordinator desk closer to where most people are sitting.\n\n5) Fan‑out from near the participants\n   - After order is confirmed, replicate to regional caches near participants and deliver from there.\n   - The slow path is only on write ordering; reads/deliveries are fast and local.\n\n6) Cached presence and directory reads\n   - Presence: Keep it fast and local with quick expirations (seconds), update from gateways.\n   - Directory: Cache reads at the edge; push permission changes promptly from the central store.\n\n7) Media placeholder pattern\n   - On send: create a message that includes a placeholder for the media.\n   - Upload media directly to nearby storage; when ready, the message shows the media.\n   - This preserves the strict order of messages without making users wait for a large upload.\n\nTrade‑offs and knobs you can tune\n- Latency vs. ordering:\n  - Tighter ordering means you may wait the round‑trip to the home region for final display.\n  - You can reduce perceived delay with pending states, optimistic previews, and quick feedback.\n- Home region selection:\n  - Static per conversation: simple, predictable; can be sub‑optimal as participants move.\n  - Dynamic: better latency, more complexity to migrate safely. Do it during low activity windows.\n- Number of replicas (for the ordering store):\n  - More replicas increase durability and availability but add agreement time.\n  - With a small, well‑placed majority, you keep agreement fast while surviving failures.\n- Minimal ops:\n  - Fewer services: combine presence and messaging gateway in one edge service to cut ops overhead.\n  - Managed multi‑region database: offload the agreement and failover to a provider; trade cost for simplicity.\n\nFailure handling\n- Region outage:\n  - Each conversation’s data is in replicas across multiple regions. If the home region fails, switch leadership to another replica.\n  - Freeze new messages briefly during leadership handover; once the new leader is active, resume with order continuity.\n- Network partitions:\n  - Don’t accept message commits from isolated replicas. If the majority can’t agree, pause committing for affected conversations and surface a clear UI state (“temporarily delayed”).\n- Client disconnects:\n  - Maintain connection state at the edge; resubscribe on reconnect; deliver missed messages from the regional cache.\n\nHow the modules talk to each other (integration strategy)\n- Standard message envelope:\n  - Fields include conversation ID, sender ID, a client‑side temporary ID, and server‑assigned final sequence when confirmed. This standardization simplifies retries and deduplication.\n- Routing:\n  - Edge gateway looks up the conversation’s home region using a small, strongly consistent mapping table.\n- Delivery:\n  - Once confirmed, the ordering store publishes the new sequence to a per‑conversation stream that edge gateways subscribe to. Gateways push to clients on open connections.\n- Presence:\n  - Gateways update presence in a local, fast store with short time‑to‑live. Clients subscribe to presence updates for the conversations they are viewing.\n- Directory:\n  - Permission checks on send go to the central directory. Cache membership lists at the edge for quick read checks; refresh on changes.\n- Media:\n  - Client uploads to object storage near them. The message includes a pointer and status. When storage confirms, update the message view.\n\nArchitecture styles that fit the constraints\n- Modular core with a thin edge layer:\n  - Keep the heavy logic (ordering, directory) in a central managed store and a small set of stateless services.\n  - Use lightweight edge gateways for connection handling, routing, presence, and delivery.\n- Partitioned streams per conversation:\n  - Treat each conversation as an independent stream with its own ordered sequence. This avoids cross‑conversation interference and simplifies scaling.\n- Event‑driven fan‑out:\n  - Confirmed messages trigger notifications to delivery services subscribed to that conversation. This keeps write ordering and read distribution decoupled.\n\nOps budget minimization tips\n- Prefer managed multi‑region databases for strong ordering and failover.\n- Use one global load balancer and a small number of edge gateway pools.\n- Consolidate presence into the edge gateway process rather than running a separate cluster.\n- Automate: blue/green deploys, health checks, and simple alerts (connection counts, commit latency, failed leadership transfers).\n- Keep observability simple: a few key metrics—commit latency per conversation, delivery latency per region, reconnect rate, presence update lag.\n\nDiagram description (text)\n- Box: Clients worldwide\n  -> Arrow to Box: Nearest Edge Gateway (WebSocket/HTTP, presence, routing)\n  -> Arrow (for sends) to Box: Conversation Router (at edge; looks up home region)\n  -> Arrow to Box: Conversation Home Region Leader (per‑conversation ordering desk)\n  -> Arrow to Box: Strong Store (replicated across 3–5 regions; stores message sequences)\n  -> Arrow (publish) to Box: Regional Delivery Caches (near participants)\n  -> Arrow to Clients: Push deliveries from Edge Gateways\n- Side path:\n  - Clients -> Edge Gateway -> Presence Store (local, short TTL) -> Presence Subscriptions\n  - Edge Gateway -> User Directory Service -> Strong Store (directory data) -> Edge Read Cache\n  - Clients -> Media Upload -> Object Storage (regional) -> CDN; Messages reference media via metadata\n\nConcrete patterns to adopt\n- Commit‑confirm message flow: instant “pending,” final display on commit.\n- Per‑conversation home region: one leader per conversation; fast agreement across a small set of replicas.\n- Idempotent send with client temporary IDs: safe retries without duplicates.\n- Read‑your‑writes at the sender: return the final sequence to the sender as soon as it’s committed so their view never “jumps.”\n- Backpressure at the edge: slow senders get paced to avoid overflowing delivery queues.\n\nWhat to standardize between modules\n- Conversation ID rules (stable, unique, immutable).\n- Message envelope fields (temporary ID, sender, timestamp, final sequence).\n- Delivery contracts (what “pending,” “confirmed,” “failed” mean).\n- Presence signals (online, typing, last seen) and their timeouts.\n- Directory roles and permissions, and how changes propagate to edge caches.\n\nReal‑world expectations\n- For cross‑continent rooms, final commit may be ~150–250 ms due to speed‑of‑light limits, even with excellent networks.\n- Most everyday use feels instant with pending states and local delivery caches.\n- Strong ordering is per conversation; reads happen fast from nearby caches.\n\nExercises to deepen understanding\n1) Latency budgeting: Pick three user locations (e.g., Tokyo, London, São Paulo), place the conversation home region, and estimate end‑to‑end commit time. Try moving the home region; note the impact.\n2) Failure drill: Simulate the home region going down for a busy conversation. Plan the leadership transfer steps and what the UI shows during it.\n3) Cache strategy: Design the edge cache invalidation rules for directory membership changes and measure how long stale permissions might remain.\n4) Media placeholder flow: Map the steps from “send photo” to “photo visible,” including failure scenarios (upload stalls, CDN lag).\n5) Capacity planning: Estimate per‑conversation throughput limits before you need to split very large rooms into sub‑streams (e.g., “stage” vs “chat”).\n\nIf you need to enforce a truly global single order across all messages everywhere (not just per conversation), call that out explicitly—latency will rise sharply and you’ll need a single worldwide ordering desk with very strict agreement, which conflicts with “ultra‑low latency.” If per‑conversation ordering is acceptable, the approach above gives a practical balance with strong guarantees and reasonable latency, while keeping the operations footprint small.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"score\":60,\"explanation\":\"Missing layering, resources, styles\"}",
        "score": 60
      }
    },
    "compliance": "ok",
    "complianceText": "Provides clear modular architecture, integration insights, analogies, exercises, diagrams, no code—fits the Architect Guide requirements.  \nOK",
    "isGroundtruth": true
  },
  {
    "id": "5a1ec75",
    "promptid": "4b51e23",
    "ruleid": null,
    "testuid": "baseline-test-1tlc8k268",
    "scenario": "",
    "baseline": true,
    "testinput": "goal: mobile app with offline-first features\nproject_type: field data collection app\nlayers: Mobile Frontend, Sync Backend, Data Storage\nmodules: local cache, sync engine, conflict resolver, reporting backend\nask: suggest an architecture and module communication patterns for offline sync; include practical exercises; no code; clear explanations; diagram preferred\nrequest_diagram: yes",
    "model": "gpt-5-2025-08-07",
    "input": "goal: mobile app with offline-first features\nproject_type: field data collection app\nlayers: Mobile Frontend, Sync Backend, Data Storage\nmodules: local cache, sync engine, conflict resolver, reporting backend\nask: suggest an architecture and module communication patterns for offline sync; include practical exercises; no code; clear explanations; diagram preferred\nrequest_diagram: yes",
    "output": "Below is a practical offline-first architecture tailored for a field data collection app, with clear module roles, communication patterns, and a diagram. It emphasizes reliable syncing in poor connectivity, predictable behavior for users in the field, and clean separation of concerns across Mobile Frontend, Sync Backend, and Data Storage.\n\nArchitecture overview\n- Goal: Let users create, edit, and review records offline; sync safely when connectivity is available; keep data consistent across many devices.\n- Style: Event-driven, “outbox/inbox” synchronization with eventual consistency and explicit conflict resolution.\n\nLayered view and modules\n- Mobile Frontend (device)\n  - Local Cache: Embedded database storing records, metadata, and sync state. Acts as the source of truth for the app UI.\n  - Sync Engine: Manages push/pull of changes, connectivity, retries, and checkpoints. Implements the outbox/inbox pattern.\n  - Conflict Resolver (client side): Applies automatic, domain-specific rules locally; defers complex cases to server or user review.\n\n- Sync Backend (server)\n  - Sync API + Ingestion Service: Receives batched changes from devices, validates, deduplicates (idempotency), and persists to canonical store.\n  - Conflict Resolver (server side): Detects and resolves conflicts using rules; returns resolution outcomes to clients.\n  - Change Feed: Streams ordered changes (by sequence number) to clients; supports checkpoint tokens for incremental pull.\n  - Reporting Backend: Subscribes to the canonical change log to build analytics, dashboards, and exports without interfering with sync.\n\n- Data Storage\n  - Device: Embedded DB (e.g., SQLite/Realm) with tables/collections for records, outbox, inbox, and sync metadata.\n  - Backend: Canonical store (e.g., Postgres/CockroachDB with event/change tables, or document DB with revision tracking). Separate read-optimized store for reporting (e.g., warehouse or materialized views).\n\nDiagram (high-level data flow)\n\n           [Mobile Frontend]\n                   |\n            (UI interacts with)\n                   v\n             [Local Cache]\n              /        \\\n   writes -> Outbox    Inbox <- applies remote changes\n              \\        /\n               \\      /\n                [Sync Engine]\n                  /     \\\n            Push (up)   Pull (down)\n                |         |\n           [Sync API]   [Change Feed]\n                \\         /\n                 \\       /\n                [Conflict Resolver]\n                      |\n               [Canonical Store]\n                      |\n              [Reporting Backend]\n              (ETL/materialized views)\n\nCore communication patterns\n1) Outbox/Inbox on device (postal analogy)\n- Outbox: When users create or edit data, the app writes to Local Cache and also appends an “envelope” to Outbox describing the operation: record ID, operation type (create/update/delete), fields changed, local version, timestamps, and an idempotency key.\n- Inbox: When the device pulls remote updates, the Sync Engine places them into Inbox and merges them into Local Cache. The UI observes Local Cache to update views.\n\n2) Checkpoints and change feed (subscription analogy)\n- Each device maintains a checkpoint token (e.g., last-seen sequence number). The backend emits a strictly ordered change feed. The device requests “changes since token” to get incremental updates.\n- This avoids full resyncs and supports resumable, partial downloads.\n\n3) State machine-based sync engine (traffic controller analogy)\n- States: Idle, Backoff, Pushing, Pulling, Resolving, Paused (battery/airplane mode).\n- Transitions triggered by events: connectivity regained, outbox not empty, server responds with conflicts, throttling needed.\n- Ensures predictable behavior, retries with exponential backoff, and avoids UI blocking.\n\n4) Idempotency and deduplication (receipt numbers analogy)\n- Each outbox entry carries an idempotency key. Backend stores processed keys to prevent double-apply if a request is retried.\n- Backend responds with a stable acknowledgment that the client records to mark items as “sent.”\n\n5) Versioning and conflict detection (ticket numbers analogy)\n- Every record carries a version/timestamp and an origin device ID.\n- Backend compares incoming change version against canonical version:\n  - If no divergence: apply and increment version.\n  - If divergence: flag conflict; run resolver; return resolution decision and new canonical version.\n\n6) Field-level merges and domain rules (household chores analogy)\n- Prefer merging at the field level when safe (e.g., additive lists, counters).\n- For mutually exclusive fields (e.g., status), use domain rules: priority by role, freshness, or explicit workflow state.\n- When rules cannot decide, mark conflict for user review; sync engine places a “conflict task” in the app UI.\n\n7) Pull-first on reconnect (tidy before you add)\n- On reconnect, pull recent changes before pushing local ones. This reduces conflicts by aligning the device with the latest canonical state.\n- After pull, push the outbox, re-check conflicts with most up-to-date server state.\n\n8) Connectivity-aware batching and compression\n- Batch outbox entries and compress payloads for slow networks.\n- Limit batch sizes to keep memory stable and speed up retries.\n\nBackend responsibilities\n- Canonical change log: Append-only record of changes with sequence numbers and metadata. Drives the change feed and reporting ETL.\n- Conflict resolution service: Applies deterministic rules and logs outcomes for auditability.\n- Per-client checkpoint registry: Stores last delivered sequence per device for efficient recovery.\n- Validation: Enforce server-side constraints (e.g., unique IDs) and return actionable errors the client can handle offline (e.g., rename/merge operations on next push).\n- Reporting backend: Consumes the change log asynchronously to build reports. Never blocks sync.\n\nData consistency strategy\n- Model: Eventual consistency with user-friendly local immediacy.\n- Guarantees:\n  - Local operations are instant and durable on device.\n  - Server eventually reflects all accepted changes.\n  - Devices converge via change feed and conflict resolution.\n- When to block locally:\n  - Operations that require global uniqueness or scarce resources (e.g., assigning a unique asset tag). Use local reservation followed by server confirmation; if rejected, surface a gentle correction flow.\n\nModule responsibilities and boundaries\n- Local Cache\n  - Stores records, sync metadata, outbox, inbox, and conflict flags.\n  - Exposes an observable API to the UI for changes.\n  - Supports lightweight migrations (versioned schema).\n\n- Sync Engine\n  - Reads from Outbox; packages changes with idempotency keys and local versions.\n  - Manages connectivity checks, backoff, retries, checkpoint tokens.\n  - Applies incoming changes to Inbox, triggers merge into Local Cache.\n  - Delegates conflict decisions to Conflict Resolver and surfaces unresolved conflicts to UI.\n\n- Conflict Resolver\n  - Client: Automatic policies for common cases; prompts user when needed.\n  - Server: Deterministic rules, audit log, and consistent outcomes across clients.\n  - Policy examples: Last-writer-wins for notes, union-merge for tag sets, max-value for counters, role-priority for status fields.\n\n- Sync Backend\n  - Validates, deduplicates, persists changes, maintains change feed, and enforces access control.\n  - Exposes pagination, checkpoints, and partial download of large attachments.\n\n- Reporting Backend\n  - Listens to canonical changes; builds read-optimized projections.\n  - Isolated from write path to avoid impacting sync latency.\n\nOperational concerns\n- Telemetry: Track sync successes/failures, queue sizes, time-to-sync, conflict frequency.\n- Privacy/Security: Encrypt local storage; rotate tokens; ensure least-privilege access on backend.\n- Offline-safe UX: Clear status indicators (queued, syncing, conflicted), visible last-sync time, manual “sync now,” and conflict resolution screens.\n- Migrations: Version schemas and change contracts; support backward compatibility during phased rollouts.\n\nPractical exercises (hands-on architecture tasks, no code)\n1) Define your domain records and operations\n- List record types (e.g., inspection, sample, photo) and typical operations (create/update/delete).\n- For each field, classify merge strategy: safe auto-merge, rule-based merge, or user review.\n- Exercise outcome: A conflict resolution matrix per field.\n\n2) Design outbox envelopes and server responses\n- Specify mandatory metadata: record ID, operation, local version, timestamp, device ID, idempotency key.\n- Specify server acknowledgment schema: success/failure, canonical version, conflict details, updated checkpoint.\n- Exercise outcome: A communication contract document for client-server change batches.\n\n3) Create sync state machine\n- Draw states and transitions: Idle, Pulling, Pushing, Resolving, Backoff, Paused.\n- Define triggers: connectivity changed, outbox not empty, server 409 conflict, battery low.\n- Exercise outcome: A state diagram and rules for retry/backoff.\n\n4) Plan checkpoints and change feed\n- Decide sequence numbering strategy (global monotonic sequence vs per-collection).\n- Define checkpoint token format and retention policy.\n- Exercise outcome: A change feed specification with pagination and recovery behavior.\n\n5) Run conflict scenarios\n- Simulate: Two devices edit the same record field differently; one deletes while the other edits; attachments added concurrently.\n- For each, determine expected outcome (auto-merge vs user prompt) and how the app communicates this to the user.\n- Exercise outcome: A playbook of conflict handling flows with mock screens.\n\n6) Offline UX audit\n- Specify UI markers for: queued changes, last sync, errors, conflicts.\n- Define user actions: retry now, resolve conflict, view change history.\n- Exercise outcome: A UX checklist and annotated screens.\n\n7) Reporting isolation plan\n- Define which fields feed reports and how often ETL runs.\n- Decide how to handle late-arriving changes (e.g., backfill).\n- Exercise outcome: An ETL plan and data freshness SLO.\n\n8) Resilience drills\n- Network: Toggle between offline/slow/unstable; observe batch sizes and retries.\n- Storage: Simulate local DB write failure and recovery.\n- Exercise outcome: Operational runbook for common failure modes.\n\nOptions to explore (choose based on team skills and constraints)\n- Document replication (e.g., CouchDB-style): Built-in revisioning and replication; faster start but less control over domain-specific conflicts.\n- Event-sourced backend: Fine-grained audit trail with change feed; more flexibility in resolution and reporting; requires disciplined modeling.\n- REST with ETags and merge-patch: Simpler API surface; good for smaller projects; add outbox/inbox and sequence tokens to avoid full scans.\n\nKey design standards (keep things consistent across modules)\n- Standard change envelope and idempotency policy.\n- Consistent versioning: record version + server sequence number.\n- Clear sync error categories: retryable vs non-retryable.\n- Observable local cache with uniform event names for UI updates.\n- Time synchronization policy: use server time for ordering where needed; capture local timestamps for context.\n\nIf you can share a bit more (types of data collected, need for attachments/photos, expected volume, platform targets, and any hard uniqueness constraints), I can tailor the conflict policies and storage choices for your context.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"score\":85,\"explanation\":\"Missing learning resource; no definitions\"}",
        "score": 85
      }
    },
    "compliance": "ok",
    "complianceText": "Provides detailed architectural guidance, uses diagrams, analogies, practical exercises, no code, asks clarifications—fits description.\nOK",
    "isGroundtruth": true
  },
  {
    "id": "594e467",
    "promptid": "4b51e23",
    "ruleid": null,
    "testuid": "baseline-test-7lh9wllxc",
    "scenario": "",
    "baseline": true,
    "testinput": "goal: IoT architecture guidance\nproject_type: smart agriculture sensors\nlayers: Device Firmware, Edge Gateway, Cloud Ingestion, Data Storage, Analytics Dashboard\nmodules: sensor data collector, MQTT gateway, stream processor, alerting service, visualization\nask: explain inter-module communication and standardization (schemas, contracts); provide a diagram description; discuss security considerations; no code; approachable tone",
    "model": "gpt-5-2025-08-07",
    "input": "goal: IoT architecture guidance\nproject_type: smart agriculture sensors\nlayers: Device Firmware, Edge Gateway, Cloud Ingestion, Data Storage, Analytics Dashboard\nmodules: sensor data collector, MQTT gateway, stream processor, alerting service, visualization\nask: explain inter-module communication and standardization (schemas, contracts); provide a diagram description; discuss security considerations; no code; approachable tone",
    "output": "Here’s a clear, project-wide view of how your smart agriculture IoT stack fits together, how modules should talk to each other, and what to standardize so the whole system is reliable and secure.\n\nOverview: Layers and module mapping\n- Device Firmware (sensor data collector): Reads measurements, timestamps them, and prepares telemetry messages.\n- Edge Gateway (MQTT gateway): Terminates local device connections, normalizes messages, buffers when offline, and publishes to cloud MQTT.\n- Cloud Ingestion: Receives MQTT, validates and enriches messages, and forwards them into a stream (e.g., Kafka/Kinesis) for processing.\n- Data Storage: Time-series and relational stores for raw/processed telemetry, alerts, and device metadata.\n- Analytics Dashboard (visualization): Presents metrics, trends, alerts; drives command/control back to devices. Alerting service runs alongside analytics, subscribing to processed events.\n\nInter-module communication: the “postal system” of your project\nThink of each module as a post office. Communication works best when:\n- Everyone speaks the same language (schemas).\n- Everyone uses the same addressing rules (topics/paths).\n- Everyone follows the same delivery and receipt etiquette (QoS, retries, acks).\n\nFlow and contracts (what each hand-off should look like):\n1) Device Firmware → Edge Gateway\n   - Transport: Local MQTT, serial/Modbus, BLE, or Zigbee, normalized at the gateway to MQTT.\n   - Message type: Telemetry.\n   - Contract essentials:\n     - deviceId (stable identity tied to certificate)\n     - timestamp (UTC ISO8601)\n     - metric name (e.g., soilMoisture, airTemp)\n     - value and unit (SI units, e.g., %, °C)\n     - quality flags (calibrationApplied, batteryLevel, signalStrength)\n     - schemaVersion\n   - Notes: Include a sequence number for ordering and a correlationId for tracing.\n\n2) Edge Gateway → Cloud Ingestion (MQTT broker)\n   - Transport: MQTT over TLS with mutual auth (mTLS).\n   - Topic taxonomy (consistent “addresses”):\n     - telemetry: farms/{farmId}/devices/{deviceId}/telemetry\n     - status/health: farms/{farmId}/devices/{deviceId}/status\n     - commands (downlink): farms/{farmId}/devices/{deviceId}/commands\n   - Delivery: QoS 1 or 2 for important telemetry; retained messages for last-known status; Last Will for offline detection.\n   - Contract: Same canonical telemetry schema as devices, plus gateway metadata (gatewayId, receivedAt).\n\n3) Cloud Ingestion → Stream Processor\n   - Transport: MQTT-to-stream bridge or connector (e.g., to Kafka).\n   - Contract: Canonical telemetry event with a normalized envelope:\n     - envelope: eventType (Telemetry|Alert|DeviceStatus), schemaVersion, traceId\n     - payload: the telemetry fields above\n   - Notes: Validate schema at ingestion and reject/route-to-dead-letter if invalid.\n\n4) Stream Processor → Data Storage\n   - Writes to Time-Series DB (raw and aggregated) and to relational store for devices, thresholds, alerts.\n   - Contract distinctions:\n     - raw telemetry table: deviceId, timestamp, metric, value, unit, quality\n     - aggregates (e.g., 5-min avg): metric, deviceId, windowStart, windowEnd, aggType, value\n     - alerts: alertId, deviceId, ruleId, status (Raised|Cleared), severity, threshold, observedValue, timestamp\n\n5) Stream Processor → Alerting Service\n   - Transport: Pub/sub on “alerts” topic or queue.\n   - Contract: Alert events with idempotencyKey, alert lifecycle (Raised/Cleared/Updated), and human-friendly message.\n\n6) Analytics Dashboard ↔ Data Storage\n   - Dashboard queries aggregations and raw series; the visualization module reads well-defined views/APIs.\n   - Contract: Query parameters must include time range, devices/fields, granularity; returned data uses the same canonical units.\n   - Commands flow back via Cloud Ingestion to devices:\n     - Command contract: commandType (SetSamplingRate, Restart), target deviceId, parameters, requestedBy, requestId, schemaVersion.\n     - Acknowledgment contract: ack with status (Accepted|Rejected|Executed), reason, timestamp.\n\nStandardization: schemas and contracts you should write down and version\n- Canonical telemetry schema: One spec all sensors adhere to, with clear names, units, timestamp rules, and optional fields (batteryLevel, location).\n- Event envelope: Common fields across all messages (eventType, schemaVersion, traceId/correlationId, producedAt).\n- Topic naming conventions: Predictable, hierarchical, and environment-aware (e.g., env/prod/farms/{farmId}/…).\n- Versioning and compatibility:\n  - Include schemaVersion in every message.\n  - Use additive changes whenever possible; treat deletions/renames as major versions.\n  - Maintain at least N-1 compatibility in processors and dashboards.\n- Error and status conventions:\n  - Standard error events with code, message, and affected deviceId.\n  - Device status heartbeat format; offline/online transitions via Last Will.\n- Units and measurement standards:\n  - Prefer SI units; define enumerations for metric names.\n  - Document calibration methods and include calibration metadata where relevant.\n- Ordering, duplicates, idempotency:\n  - sequenceNumber on telemetry; processors handle out-of-order arrivals.\n  - idempotencyKey for alerts and commands to avoid duplicates.\n- Time discipline:\n  - Devices sync clock (NTP/GPS); timestamp at source when possible.\n  - Cloud uses receivedAt for fallback; record both producedAt and receivedAt.\n- Observability:\n  - correlationId/traceId through all layers.\n  - Include gatewayId and edgeReceivedAt for provenance.\n- Access contracts and RBAC:\n  - Define who can publish/subscribe to which topics and which data fields are visible to dashboards or tenants.\n\nDiagram description (how the architecture looks)\n- Left: Sensor Nodes (Device Firmware)\n  - Each device has sensors and a small agent that formats telemetry messages.\n- Arrow to: Edge Gateway (MQTT gateway)\n  - Collects local messages, normalizes, buffers, and publishes to cloud MQTT.\n- Arrow to: Cloud Ingestion (MQTT Broker + Validator/Bridge)\n  - Authenticates devices, validates schemas, forwards to a streaming backbone.\n- Arrow to: Stream Processor\n  - Applies rules (thresholds, anomaly detection), creates aggregates, emits alerts.\n- Two arrows out:\n  - To Data Storage (Time-Series DB + Relational metadata) for telemetry and aggregates.\n  - To Alerting Service, which notifies via email/SMS/webhooks and maintains alert state.\n- Right: Analytics Dashboard (Visualization)\n  - Reads from Data Storage to display charts and maps; subscribes to alert status.\n- Control path (dashed arrow) from Dashboard/Management back through Cloud Ingestion to Edge/Devices\n  - Sends commands (e.g., change sampling rate), receives acknowledgments.\n- Security boundary around each link (TLS/mTLS), and RBAC around broker, stream, storage, and dashboard.\n\nSecurity considerations (layer-by-layer and cross-cutting)\nDevice Firmware\n- Secure identity: Per-device X.509 certificates stored in a secure element; no shared keys.\n- Secure boot and signed firmware: Prevent unauthorized code; anti-rollback on updates.\n- Least privilege: Only the topics and commands that device needs; command allowlist.\n- Local hardening: Validate sensor inputs and command parameters; rate-limit command handling.\n\nEdge Gateway\n- mTLS to cloud; firewall and network segmentation between local device network and upstream.\n- OS hardening: Disable unused services, apply patches, restrict physical ports, encrypt local buffer storage.\n- Credential hygiene: Automatic key rotation, short-lived tokens for cloud connectors, secure secrets vault.\n- Offline behavior: Spool telemetry with bounds and back-pressure; protect buffer with encryption.\n\nCloud Ingestion\n- Broker isolation: Per-tenant/topics permissions; prevent devices publishing outside allowed paths.\n- DDoS and abuse protection: Rate limits, connection quotas, anomaly detection for high-volume devices.\n- Schema validation: Reject malformed payloads; quarantine into dead-letter topics for investigation.\n- Kill switch: Ability to disconnect or ban misbehaving devices/gateways quickly.\n\nStream Processor and Alerting\n- Service accounts with least privilege to topics and databases.\n- Input sanitation: Trust schemas, verify ranges and units; handle outliers intentionally.\n- Idempotency and replay safety: Ensure reprocessing does not duplicate alerts or aggregates.\n- Secure configs and secrets: Managed in KMS/secret manager; audited access.\n\nData Storage\n- Encryption at rest; backups encrypted and tested.\n- Row/tenant-level access control; audit logging of reads/writes.\n- Retention policies by data type (raw vs aggregates vs alerts); deletion workflows for compliance.\n\nAnalytics Dashboard\n- Strong auth (SSO, MFA), role-based permissions for viewing and commanding.\n- Secure APIs: Input validation, rate limiting, CSRF protection for session-based flows.\n- Audit trails: Who sent which command and when; alert acknowledgment history.\n\nCross-cutting practices\n- End-to-end encryption: TLS everywhere; mTLS on device/gateway-to-cloud paths.\n- Vulnerability management: Regular scans, patch cadence, and dependency auditing.\n- Monitoring and incident response: Centralized logs, metrics, traces; alerting on unusual device behavior.\n- Supply chain and provisioning: Secure manufacturing or onboarding flow, unique identities, proof-of-ownership.\n\nPractical checks you can run quickly\n- Create a one-page canonical telemetry schema and have every module validate against it.\n- Publish a topic naming and RBAC policy; test a device’s inability to publish to forbidden topics.\n- Add sequenceNumber and correlationId to messages; trace a sample reading from device to dashboard.\n- Simulate offline edge gateway and confirm buffered messages replay correctly without duplicate alerts.\n- Perform a “command dry run” end-to-end with ack/timeout semantics documented.\n\nIf you have preferences for specific protocols (e.g., Modbus vs BLE on the device side, or Kafka vs Kinesis in the cloud), share them and I can tailor the contracts and security details accordingly.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"explanation\":\"Acronyms not defined\",\"score\":90}",
        "score": 90
      }
    },
    "compliance": "ok",
    "complianceText": "Extensive modular overview with clear architecture focus, analogies, exercises, and no code.\nOK",
    "isGroundtruth": true
  },
  {
    "id": "c3273e9",
    "promptid": "4b51e23",
    "ruleid": null,
    "testuid": "baseline-test-n29tyhilz",
    "scenario": "",
    "baseline": true,
    "testinput": "goal: data pipeline architecture\nproject_type: customer analytics\nmodules: event collector, ETL, stream processing, data warehouse, BI tool\nconstraints: mixed batch and real-time needs\nask: recommend architectural styles (Lambda/Kappa); explain integration; provide resources for learning more; avoid jargon; no code; include diagram description",
    "model": "gpt-5-2025-08-07",
    "input": "goal: data pipeline architecture\nproject_type: customer analytics\nmodules: event collector, ETL, stream processing, data warehouse, BI tool\nconstraints: mixed batch and real-time needs\nask: recommend architectural styles (Lambda/Kappa); explain integration; provide resources for learning more; avoid jargon; no code; include diagram description",
    "output": "Below is a clear, practical guide to choosing between Lambda and Kappa for a customer analytics data pipeline, how your modules fit together, and where to learn more. I’ll also include a diagram description you can hand to a designer and some exercises to help you apply the ideas. I’ll keep the language simple and focus on how parts work together.\n\nRecommendation: Lambda vs. Kappa for mixed batch and real-time\n- Lambda Architecture (two paths: batch + real-time)\n  - Best when: you have strong batch needs (historic backfills, complex transformations, heavy joins), but you also need fast dashboards or alerts.\n  - Strengths: reliable historical accuracy from batch jobs; fast views from stream jobs; easy to reprocess history when business logic changes.\n  - Trade-offs: two code paths to maintain; slightly more operational overhead.\n- Kappa Architecture (stream-first, one path)\n  - Best when: most data starts as events; you can treat “batch” as replaying streams from storage; you want simplicity and one path for logic.\n  - Strengths: one set of transformations; simpler operations; faster iteration.\n  - Trade-offs: large historical recalculations can be more complex; some warehouse-centric transformations (like huge dimension builds) can be trickier directly in streams.\n\nGiven your mixed batch and real-time needs, I recommend:\n- If analysts rely heavily on curated, historical reports and you expect frequent backfills or logic changes, choose Lambda.\n- If your events are the ground truth and you want to minimize duplicated logic, choose Kappa with a strong event storage/replay strategy and a warehouse fed from the stream.\n\nPractical integration (how modules fit together)\nYour modules: event collector, ETL, stream processing, data warehouse, BI tool.\n\nCommon integration rules (applies to both Lambda and Kappa):\n- Event standardization: define a clear event “contract” (name, version, timestamp, customer_id, source, payload). Use consistent time (UTC), stable IDs, and versioning. This reduces breakage across modules.\n- Schema management: keep a registry or catalog of event types and their fields, with evolution rules (adding fields is okay; removing or renaming needs a plan).\n- Keys and partitions: choose a routing key (often customer_id) to keep related events together. This cuts down on out-of-order issues.\n- Idempotency: design transforms so rerunning them doesn’t double-count (e.g., use upserts keyed by customer_id+event_id).\n- Late and bad data: handle late arrivals and errors via a “parking lot” (dead-letter queue) and scheduled cleanup/backfill jobs.\n\nIntegration flow for Lambda (two paths)\n- Real-time path:\n  1) Event Collector: receives clickstream, app events, and CRM changes; publishes events to a streaming bus.\n  2) Stream Processing: applies lightweight enrichments (attach geo, device, campaign), aggregates quick KPIs (e.g., sessions per minute), and writes to a fast store (real-time cache or “speed layer” table in the warehouse).\n  3) BI tool: powers live dashboards and alerts from these fast tables (with caveats on roughness vs. final accuracy).\n- Batch path:\n  1) Raw Event Storage: events are also landed to durable storage (object store or warehouse staging).\n  2) ETL: nightly/hourly jobs perform deeper transformations (deduping, identity resolution, sessionization, dimension building).\n  3) Data Warehouse: curated tables (facts and dimensions) are built for reliable reporting.\n  4) BI tool: analysts use curated tables for accurate reporting; the BI model can blend curated batch with selected real-time metrics.\n\nDiagram description for Lambda\n- Boxes and arrows, left-to-right:\n  - Box: “Event Collector”\n    -> Arrow to “Streaming Bus”\n    -> Arrow splits:\n      - Top path: “Stream Processing (enrich + fast aggregates)” -> “Speed Layer (fast tables/cache)” -> “BI (real-time dashboards)”\n      - Bottom path: “Raw Event Storage (staging)” -> “Batch ETL (clean + model)” -> “Data Warehouse (curated tables)” -> “BI (accurate reports)”\n  - Thin arrow from “Raw Event Storage” back to “Stream Processing” labeled “replay if needed” (optional).\n  - Side box: “Schema Registry” connected to Event Collector, Stream Processing, ETL, and Warehouse.\n\nIntegration flow for Kappa (single path with replay)\n- Stream-first path:\n  1) Event Collector: sends all events to the streaming bus.\n  2) Stream Processing: performs both real-time enrichments and core transformations; outputs to two places:\n     - Real-time tables for dashboards,\n     - Append-only store or warehouse staging for durable history.\n  3) Warehouse Build: incremental jobs read the transformed stream (or its durable log) to maintain curated tables.\n  4) BI tool: uses curated tables for most reporting; real-time tables for live views.\n  5) Replay: when business rules change, re-run stream transforms from the stored log to recompute history.\n\nDiagram description for Kappa\n- Boxes and arrows, left-to-right:\n  - Box: “Event Collector”\n    -> “Streaming Bus (with long retention)”\n    -> “Stream Processing (main transformations)”\n    -> Two outputs:\n       - “Real-time Tables” -> “BI (live dashboards)”\n       - “Durable Log/Stage” -> “Warehouse Build (incremental)” -> “Data Warehouse (curated tables)” -> “BI (reports)”\n  - Curved arrow from “Durable Log/Stage” back to “Stream Processing” labeled “replay for backfills.”\n  - Side box: “Schema Registry” connected to Collector, Stream, Warehouse Build.\n\nChoosing between them: decision checklist\n- Backfill frequency: frequent changes to business rules or identity resolution → Lambda easier.\n- Operational simplicity: fewer code paths and faster iteration → Kappa simpler.\n- Team skills: strong streaming skills → Kappa; strong SQL/warehouse/ETL skills → Lambda or a hybrid.\n- Latency goals: sub-second alerts → either style works; ensure the stream path is optimized.\n- Data volumes: very high event rates → Kappa with long-retention streams is often cleaner.\n\nEveryday analogy\n- Think of the streaming bus as a conveyor belt moving packages (events) continuously.\n- The speed layer is a display shelf showing “what’s happening right now.”\n- Batch ETL is the overnight crew that opens packages, reorganizes the warehouse, and updates the inventory system for accurate stock counts.\n- The data warehouse is the neatly organized storeroom.\n- The BI tool is the storefront where customers (analysts) browse and compare.\n\nStandards to keep modules aligned\n- Event naming: product.viewed, cart.added, order.completed; include a version number (e.g., order.completed.v2).\n- Common fields: event_id, event_time_utc, customer_id, session_id, source_app, schema_version.\n- Time windows: define standard windows (session = 30 minutes idle) so stream and batch agree.\n- Quality rules: document how to handle duplicates, missing customer_id, and out-of-order events.\n- Data contracts: producers must announce schema changes; consumers validate before accepting.\n\nOperational practices\n- Monitoring: track lag on streams, job runtimes, error rates, and freshness of BI tables.\n- Dead-letter handling: capture bad events with reasons; schedule weekly review and fixes.\n- Access control: separate raw, curated, and sandbox areas in the warehouse; limit direct writes to curated tables.\n- Cost control: keep raw events compressed; use incremental loads; prune old aggregates not used by BI.\n\nPractical exercises\n1) Event contract workshop: list 10 core event types for customer analytics; define required fields and a versioning plan.\n2) Latency tiers: write down which metrics must be live (e.g., active users, checkout failures) vs. batch (e.g., monthly cohort retention).\n3) Backfill drill: design steps to reprocess last 30 days after a logic change; note how you prevent double counting.\n4) Data quality playbook: choose rules for deduplication and late arrivals; define dashboards to track them.\n5) BI model mapping: sketch star schemas for Sales (facts) and Customers (dimensions) and which fields are fed by stream vs. batch.\n\nResources for learning more\n- “Questioning the Lambda Architecture” by Jay Kreps: https://www.oreilly.com/radar/questioning-the-lambda-architecture/\n- Designing Data-Intensive Applications by Martin Kleppmann (chapters on streams and batch): https://dataintensive.net/\n- Streaming Systems (Tyler Akidau et al.): https://www.oreilly.com/library/view/streaming-systems/9781491983867/\n- Confluent/Apache Kafka design guides on event schemas and replay: https://www.confluent.io/resources/\n- Google Cloud and AWS data pipeline reference architectures (even if you use other tools, the diagrams help):\n  - GCP: https://cloud.google.com/architecture#data-analytics\n  - AWS: https://docs.aws.amazon.com/whitepapers/latest/data-analytics-options/data-analytics-options.html\n- Dimensional modeling for analytics (Kimball): https://www.kimballgroup.com/\n\nA few clarifying questions (optional, to refine the design)\n- What are your target latencies (e.g., alerts within 5 seconds; dashboards within 1 minute)?\n- Do you expect frequent business rule changes (e.g., new sessionization or attribution logic)?\n- What’s your expected peak event rate and how long do you need to retain raw events for replay?\n- Which BI use cases truly need live data vs. curated, consistent data?\n\nIf you share those details, I can tailor the final architecture choice and integration conventions to your exact needs.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"score\":95,\"explanation\":\"Excellent content; missing acronym definitions\"}",
        "score": 95
      }
    },
    "compliance": "ok",
    "complianceText": "Thoroughly matches architecture guidance: basics, integration, exercises, diagrams, clarity, no coding, analogies, clarification questions.\nOK",
    "isGroundtruth": true
  },
  {
    "id": "bfc9db9",
    "promptid": "4b51e23",
    "ruleid": null,
    "testuid": "baseline-test-kboiiqy9g",
    "scenario": "",
    "baseline": true,
    "testinput": "goal: AI/ML project architecture\nproject_type: recommendation system\nlayers: Data Lake, Feature Store, Model Training, Model Serving, Monitoring\nmodules: data ingestion, feature computation, training orchestrator, serving API, drift detector\nask: analyze the multi-layer architecture, module interactions, and deployment considerations; provide practical exercises; no code; clear language; diagram requested\nrequest_diagram: yes",
    "model": "gpt-5-2025-08-07",
    "input": "goal: AI/ML project architecture\nproject_type: recommendation system\nlayers: Data Lake, Feature Store, Model Training, Model Serving, Monitoring\nmodules: data ingestion, feature computation, training orchestrator, serving API, drift detector\nask: analyze the multi-layer architecture, module interactions, and deployment considerations; provide practical exercises; no code; clear language; diagram requested\nrequest_diagram: yes",
    "output": "Below is a clear, end‑to‑end architecture for a recommendation system using your layers and modules. Think of the system like a restaurant: the Data Lake is your pantry, the Feature Store is the prep station with ready-to-use ingredients, Model Training is the kitchen where recipes are honed, Model Serving is the waiter delivering dishes, and Monitoring is the health inspector ensuring quality and safety.\n\nArchitecture diagram (ASCII)\n\n+------------------------------------------------------------------------------------+\n|                                  Monitoring Layer                                  |\n|      Drift Detector + Metrics (data quality, drift, CTR, latency, errors)          |\n|   Taps into: Ingestion, Feature Store (offline/online), Serving API, Data Lake     |\n|                Triggers retraining via Training Orchestrator when needed           |\n+------------------------------------------------------------------------------------+\n\n+----------------------------------- Model Serving Layer ----------------------------+\n|                                                                                   |\n|   +-----------------------+         +---------------------+         +----------+   |\n|   | Model Runtime         |<--------| Model Registry      |<--------| Training |   |\n|   | (deployed model)      |         | (versions, metadata)|         | Orchestr.|   |\n|   +-----------+-----------+         +---------------------+         +----------+   |\n|               ^                                                         ^         |\n|               |                                                         |         |\n|   +-----------+------------+                 +---------------------+    |         |\n|   | Serving API             |--(online lookup)--> Online Feature Store |  |         |\n|   | (HTTP/gRPC, cache)      |<-- prediction logs --+------------------+  |         |\n|   +-----------+-------------+                           |               |         |\n|               |                                         v               |         |\n|               +---------------------> Users (responses, logs)           |         |\n+------------------------------------------------------------------------------------+\n\n+----------------------------------- Model Training Layer ---------------------------+\n|                                                                                   |\n|   +----------------------+        +--------------------------+                    |\n|   | Training Orchestrator|<-------| Feature Store (offline)  |<-----+             |\n|   | (DAG, scheduling,    |        | (historical features)    |      |             |\n|   | eval, registry push) |        +--------------------------+      |             |\n|   +----------------------+                              ^           |             |\n|                                                         |           |             |\n|                                         +---------------------------+             |\n|                                         | Feature Computation (batch/stream)      |\n+------------------------------------------------------------------------------------+\n\n+----------------------------------- Feature Store Layer ----------------------------+\n|                                                                                   |\n|   +---------------------------+             +---------------------------+         |\n|   | Feature Store (offline)   |<----------- | Feature Computation       |         |\n|   | (parquet/tables)          |             | (ETL/stream transforms)   |         |\n|   +---------------------------+             +---------------------------+         |\n|                   |                                        |                      |\n|                   +---------- materialize ----------> Online Feature Store        |\n+------------------------------------------------------------------------------------+\n\n+----------------------------------- Data Lake Layer --------------------------------+\n|                                                                                   |\n|   +-------------------+     +------------------------------------+                |\n|   | Data Ingestion    |---->| Data Lake (raw/curated, partitioned)|               |\n|   | (batch/stream)    |     | click/view/purchase events, labels  |               |\n|   +-------------------+     +------------------------------------+                |\n+------------------------------------------------------------------------------------+\n\nLayer-by-layer analysis\n\n1) Data Lake\n- Responsibility: Central, immutable store for raw and curated data (events, user/items, labels such as clicks or purchases). Acts as source of truth.\n- Inputs/Outputs:\n  - Input: Data Ingestion writes raw events; optional enrichment and curation produce cleaned tables.\n  - Output: Curated tables feed Feature Computation; logs and predictions from Serving also land back here for offline analysis.\n- Key decisions:\n  - Batch vs streaming ingestion, partitioning by time/user/item, retention and governance.\n  - Data contracts: define schemas (required fields, types, nullability) and change policies.\n- Pitfalls:\n  - Inconsistent timestamps or IDs causing misaligned features and labels.\n  - Lack of lineage and reproducibility (hard to re-train same model).\n\n2) Feature Store (offline and online)\n- Responsibility: Standardized, documented features computed from the lake. Offline store used for training; online store used at inference.\n- Inputs/Outputs:\n  - Feature Computation reads from the Data Lake, writes to the offline store, materializes a subset to the online store.\n- Key decisions:\n  - Entity design (user_id, item_id); feature definitions with owners, freshness/TTL.\n  - Consistency: same logic for offline and online to avoid training-serving skew.\n- Pitfalls:\n  - Feature leakage (using future information in training); stale or mismatched online features; unbounded cardinality features.\n\n3) Model Training\n- Responsibility: Orchestrate training pipelines, track experiments, evaluate, register models.\n- Inputs/Outputs:\n  - Input: Offline Feature Store + labels from Data Lake.\n  - Output: Validated model artifact in a Model Registry with metadata (version, data slice, metrics).\n- Key decisions:\n  - Retraining triggers (schedule vs drift), train/validation/test split consistent with temporal data.\n  - Candidate generation vs ranking architecture (for recsys often two-stage; if you use single-stage, note latency/cost trade-offs).\n- Pitfalls:\n  - Not aligning training window with feature timestamps; unverified performance across segments (new users, long-tail items).\n\n4) Model Serving\n- Responsibility: Real-time inference via the Serving API, fetching online features, caching, A/B tests, canary/shadow deployments.\n- Inputs/Outputs:\n  - Input: Online Feature Store, model artifact from Registry.\n  - Output: Recommendations + logs (requests, features used, predictions).\n- Key decisions:\n  - Latency budget (e.g., p95 < 100ms), rate limits, autoscaling for traffic spikes.\n  - Rollout strategy (canary, A/B, shadow) and fallback behavior (use popular items if features missing).\n- Pitfalls:\n  - Cold-start for new users/items; caching inconsistencies; insufficient observability for failures.\n\n5) Monitoring and Drift Detection\n- Responsibility: Watch data quality, drift, model performance, and system health; trigger retraining.\n- Inputs/Outputs:\n  - Inputs: Online features, predictions, outcomes, ingestion stats; offline distributions from Data Lake.\n  - Output: Alerts, dashboards, and retrain requests to the Training Orchestrator.\n- Key decisions:\n  - Metrics: feature freshness, missingness, distribution shift (data drift), label delay, CTR/Conversion, latency, error rate.\n  - Action thresholds: when to retrain, when to pause a model, when to roll back.\n- Pitfalls:\n  - Silent degradation due to label delay or partial outages; noisy drift signals leading to unnecessary retrains.\n\nModule interactions (how things talk to each other)\n\n- Data Ingestion -> Data Lake:\n  - Writes raw events with strict schema/versioning. Idempotent and replayable for backfills.\n- Data Lake -> Feature Computation:\n  - Feature jobs (batch or stream) read curated event tables; compute features with time-aware windows; publish to offline Feature Store.\n  - Materialization publishes a smaller, latency-sensitive subset to the Online Feature Store.\n- Offline Feature Store -> Training Orchestrator:\n  - Orchestrator builds training datasets using entity keys and aligned timestamps; runs experiments; tracks metrics; registers artifacts in Model Registry.\n- Model Registry -> Serving API:\n  - Serving pulls latest approved model version; hot-swaps via canary/shadow; keeps a fallback model.\n- Serving API -> Online Feature Store:\n  - Resolves features by entity IDs; caches hot features; records prediction logs.\n- Serving API -> Monitoring (and Data Lake):\n  - Streams predictions and request metadata to Monitoring and Data Lake for later evaluation.\n- Drift Detector -> Training Orchestrator:\n  - When drift or performance drops exceed thresholds, creates retraining jobs; may gate promotion of new models until online checks pass.\n\nDeployment considerations\n\n- Architecture style:\n  - Event-driven, dual offline/online (“lambda-like”) is common for recommender systems: batch for completeness, stream for freshness.\n  - Microservices for modularity (ingestion, feature service, serving API, monitoring) vs a smaller, orchestrated monolith for simpler stacks.\n- Environments:\n  - Dev/Staging/Prod separation. Use sampled/synthetic data in non-prod; strict secrets management; separate registries and stores per env.\n- Scalability and performance:\n  - Autoscale serving pods based on CPU/QPS/latency. Precompute/caching to keep p95 latency within target.\n  - Throughput planning for ingestion and feature materialization; backpressure handling.\n- Reliability:\n  - Idempotent ingestion and feature jobs; retries with exponential backoff; dead-letter queues for bad events.\n  - Training-serving consistency: shared feature definitions; versioned transformations; reproducible snapshots.\n- CI/CD:\n  - Pipelines for data (schemas, validation), features (definitions, tests), models (training, evaluation, promotion gates), and infra (IaC).\n  - Canary deployments with automatic rollback on errors or metric regressions.\n- Security & privacy:\n  - PII protection in the lake and features; encryption at rest/in transit; RBAC; audit logs; data retention policies; anonymization for training.\n- Observability:\n  - Centralized logging, metrics, tracing. Dashboards for latency, errors, drift, feature freshness, model performance.\n  - Data lineage: track dataset versions and feature computation graphs.\n- Cost & efficiency:\n  - Time-partitioned storage; tiered storage; compact features; TTL for online features; caching strategies.\n- Data contracts:\n  - Versioned schemas with backward-compatible changes; contract tests to prevent breaking downstream consumers.\n\nPractical exercises (no code)\n\n- Map the data flow:\n  - Draw a sequence of artifacts: event -> raw table -> curated table -> feature view (offline) -> feature view (online) -> training set -> model -> serving logs.\n- Define feature contracts:\n  - For 10 key features, document entity key, window, TTL, freshness requirement, null behavior, owners, and test checks.\n- Consistency check plan:\n  - Describe how you’ll ensure the same transformation logic is used offline and online; include a periodic offline-online distribution comparison.\n- Training dataset reproducibility:\n  - Specify snapshot strategy (date partition + feature version) and how you can reconstruct any past training run.\n- Latency budget:\n  - Allocate a 100ms budget across serving steps: network, feature lookup, model scoring, serialization; plan caches to meet p95.\n- Rollout strategy:\n  - Design a canary plan: 5% traffic for 24 hours, monitor CTR, latency, errors; rollback criteria; shadowing first if risk is high.\n- Drift monitoring playbook:\n  - Pick 5 top features; define drift metrics and thresholds; outline actions (alert, retrain, pause model).\n- Label delay handling:\n  - Document how late conversions/clicks arrive and how you align labels with training windows; define minimum delay before evaluating online performance.\n- SLOs and alerts:\n  - Set SLOs for serving latency, feature freshness, ingestion lag, training cadence; map each SLO to alerts and dashboards.\n- Incident runbook:\n  - Write steps for “online feature store outage” and “CTR drop by 20%” including fallbacks (popular items), on-call roles, and recovery.\n- README outline:\n  - Include architecture diagram, module responsibilities, data contracts, feature catalog, training/serving versioning, deployment process, SLOs/alerts, and runbooks.\n\nCommon pitfalls and how to avoid them\n\n- Training-serving skew: Use shared feature definitions and validate offline vs online distributions regularly.\n- Feature leakage: Ensure features only use data available up to the prediction time; audit windows and joins.\n- Cold-start: Define fallbacks (popular items, content-based features) and bootstrap strategies (exploration traffic).\n- Inconsistent IDs/timestamps: Centralize ID generation and clock handling; enforce schemas.\n- Uncontrolled schema changes: Use versioned contracts and contract tests; deprecate features with timelines.\n- Unclear ownership: Assign owners to each feature and module; publish contacts and SLAs.\n- Overfitting to offline metrics: Require online A/B or canary validation before full promotion.\n\nBrief style comparison\n\n- Monolith: Faster to start, simpler ops, but harder to scale and evolve; good for small teams and MVP.\n- Microservices: Clear boundaries (ingestion, features, serving, monitoring), independent scaling; needs strong observability and DevOps maturity.\n- Event-driven pipeline: Natural fit for recsys freshness; adds complexity in stream processing and event semantics.\n- Dual offline/online (lambda-like): Balances completeness (batch) and freshness (stream); accept complexity; keep transformations consistent.\n\nClarifications that would improve accuracy\n\n- Latency target (p95/p99) and peak QPS for the Serving API.\n- Retraining cadence vs drift-triggered retraining policy.\n- Data volume/velocity (events per day) and label delays.\n- Must-have features online vs acceptable to compute offline only.\n- Regulatory constraints (PII, data locality) and compliance requirements.\n- Deployment environment (cloud/on-prem), container orchestration, and managed services available.\n- Rollout strategy preference (canary, A/B, shadow) and business KPIs (CTR, conversion).\n\nIf you’d like, I can refine the diagram and the module responsibilities based on your answers to the clarifications above, or tailor the exercises to your specific stack and team size.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"score\":70,\"explanation\":\"No resource; acronyms undefined\"}",
        "score": 70
      }
    },
    "compliance": "ok",
    "complianceText": "Comprehensive architecture guide with analogy, diagram, layers, exercises, style comparison, clarification: matches description.  \nOK",
    "isGroundtruth": true
  },
  {
    "id": "20bae5f",
    "promptid": "4b51e23",
    "ruleid": null,
    "testuid": "baseline-test-omo0dyogn",
    "scenario": "",
    "baseline": true,
    "testinput": "goal: compliance-focused architecture\nproject_type: healthcare appointment platform\nmodules: patient portal, scheduling, EHR integration, notifications, audit trail\nnon_functional_requirements: HIPAA compliance, data privacy, availability\nask: advise on architecture, inter-module contracts, and data storage practices under compliance; include real-world examples; request clarifications if needed; provide a diagram description; no code",
    "model": "gpt-5-2025-08-07",
    "input": "goal: compliance-focused architecture\nproject_type: healthcare appointment platform\nmodules: patient portal, scheduling, EHR integration, notifications, audit trail\nnon_functional_requirements: HIPAA compliance, data privacy, availability\nask: advise on architecture, inter-module contracts, and data storage practices under compliance; include real-world examples; request clarifications if needed; provide a diagram description; no code",
    "output": "Thanks for the clear goals and module list. Below is an architecture-focused guide tailored for a HIPAA-compliant healthcare appointment platform. I’ll cover inter-module contracts, data storage practices under compliance, give real-world examples, and include a diagram description. I’ll also ask a few clarifying questions at the end to ensure we fine-tune for your context.\n\nArchitecture overview\n- Architectural style: A modular, service-oriented backend with an event-driven spine works well here. Each module is its own bounded context: Patient Portal, Scheduling, EHR Integration, Notifications, and Audit Trail. Use synchronous APIs for “read/decide” interactions and asynchronous events for “notify/react” interactions. This separation helps with compliance by minimizing unnecessary data sharing and isolating PHI.\n- High-level layers:\n  - Frontend Application: Web and mobile Patient Portal, plus a staff/admin UI behind stricter access controls.\n  - Backend Services: One service per module, decoupled via an API gateway and a secure event bus.\n  - Data Storage: Separate datastores per module, with strict PHI boundaries, encryption, and immutable auditing.\n  - Cross-cutting: Identity and Access Management (IAM), Secrets/Key Management, Observability, and Security Operations.\n\nInter-module communication and contracts\nThink of contracts like rules for what each team is allowed to bring to a shared table. Keep the table clean—only what’s needed.\n\n- Common standards and metadata:\n  - IDs: Use a stable internal patient UUID and a separate external EHR patient identifier. Never expose raw EHR identifiers to end users.\n  - Correlation: Every request and event includes correlation_id and requestor_id to trace who did what, when, and why.\n  - Purpose-of-use: Include a “purpose_of_use” tag (e.g., treatment, payment, operations) in requests/events for HIPAA alignment.\n  - Data classification: Tag payloads as PHI, PII, operational, or non-sensitive. Contracts enforce what classifications are allowed per channel.\n\n- Synchronous APIs via API Gateway:\n  - Patient Portal Service:\n    - Auth flows with OAuth2/OIDC and MFA.\n    - Read-only PHI endpoints gated by patient identity and consent.\n    - Write endpoints limited to actions like booking/canceling appointments and updating preferences.\n  - Scheduling Service:\n    - Endpoints to query availability, hold a timeslot, book/reschedule/cancel.\n    - Receives minimal PHI—often just patient_id and visit_type. It does not store clinical notes.\n  - EHR Integration Service:\n    - FHIR-based endpoints to fetch or push Appointment, Patient, Practitioner, Location resources.\n    - Maps internal IDs to EHR IDs. Handles data minimization, transformation, and consent checks.\n  - Notifications Service:\n    - Templates and delivery endpoints. Strict rule: no PHI in messages; use links to the portal for details.\n  - Audit Trail Service:\n    - Ingest-only API for structured audit entries; append-only design; rejects edits/deletes.\n\n- Asynchronous events on a secure event bus:\n  - Scheduling emits: AppointmentHeld, AppointmentBooked, AppointmentRescheduled, AppointmentCanceled.\n  - Patient Portal emits: PatientAuthenticated, PatientConsentUpdated, ContactMethodUpdated.\n  - EHR Integration emits: EHRSyncStarted, EHRSyncCompleted, EHRSyncFailed.\n  - Notifications emits: ReminderQueued, MessageDelivered, DeliveryFailed.\n  - Audit Trail subscribes to all and writes immutable entries.\n  - Event payload rules: include event_id, correlation_id, actor_id, timestamp, purpose_of_use, data_classification, resource references (e.g., appointment_id, patient_id). Keep PHI out of events unless strictly necessary; prefer referencing IDs and fetching PHI synchronously when needed with proper authorization.\n\nData storage practices under compliance\nThink of PHI like valuables in a bank: separate vaults, strong keys, cameras everywhere, and a strict ledger of every access.\n\n- Segregate data by module and sensitivity:\n  - Patient Portal DB: user profiles, consents, non-clinical preferences, minimal PHI necessary for display. Encrypt columns that hold PHI.\n  - Scheduling DB: appointment metadata and calendars; store only what’s needed to schedule (patient_id, provider_id, timeslot, visit_type). No clinical notes.\n  - EHR Integration DB: mapping tables, sync states, FHIR resource references; limit cached PHI, prefer just-in-time retrieval from EHR when feasible and allowed.\n  - Notifications DB: message templates, delivery receipts, and contact channel tokens. Store no PHI in bodies. Use indirect identifiers and short-lived tokens for deep links.\n  - Audit Trail Store: append-only, write-once style storage. Consider immutable/WORM storage with retention locks.\n\n- Encryption, keys, and secrets:\n  - Encryption at rest for all stores with per-tenant or per-module keys, managed via KMS/HSM. Rotate keys regularly.\n  - Field-level encryption for PHI fields, especially contact data and DOB. Use deterministic encryption or tokenization if exact-match queries are needed.\n  - TLS everywhere in transit; mutual TLS or signed JWTs between services.\n  - Vault for secrets; enforce short-lived credentials.\n\n- Minimization and redaction:\n  - Contracts explicitly prohibit PHI in logs, traces, and notifications. Use redaction filters in logging pipelines.\n  - Cache PHI sparingly and with TTLs. Prefer pull-on-demand with strict authorization and purpose-of-use checks.\n\n- Access control:\n  - RBAC/ABAC with least privilege. Example: Scheduling service can access appointment metadata but cannot read clinical notes from EHR.\n  - Break-glass workflow for emergency access with enhanced auditing and post-incident review.\n\n- Retention and deletion:\n  - HIPAA requires retaining audit logs; set retention policies per data type. Appointments and billing data have longer retention than notification receipts.\n  - Deletion: Patients typically have rights to access and request amendments. Implement controlled updates rather than hard deletes for clinical data; soft-delete with audit entries for operational data.\n\n- Backups and data residency:\n  - Encrypted backups with tested restores. Define RPO/RTO; support geo-redundant but region-constrained storage to honor residency requirements.\n\nAvailability, resilience, and operations\n- High availability:\n  - API gateway and services deployed across multiple zones; managed DB with read replicas; event bus with durable storage and dead-letter queues.\n  - Transactional outbox pattern for reliable event publishing after DB commits.\n  - Idempotent handlers for event processing to avoid duplicates.\n\n- EHR Integration resilience:\n  - Queue outbound writes; backoff and retry on EHR downtime. Provide a “sync status” visible to staff.\n  - Separate inbound/outbound connectors to contain failures.\n\n- Observability and security operations:\n  - Centralized logging, metrics, and traces with PHI redaction. Forward audit logs and security events to SIEM.\n  - Continuous compliance checks, vulnerability scans, and patch pipelines.\n  - WAF, DDoS protection, and anomaly detection. Threat modeling for each module.\n\nReal-world examples and practices\n- FHIR-based EHR integration: Many providers expose FHIR R4 endpoints (e.g., Epic, Cerner). Use Appointment, Patient, Practitioner, and Location resources, and map internal IDs carefully. Keep a “link table” for internal-to-EHR identifiers to avoid leaking EHR IDs.\n- Immutable audit: Organizations use S3 Object Lock or similar WORM storage to ensure audit logs cannot be altered. Pair with a SIEM (e.g., Splunk, Elastic) for alerting. Some teams create hash-chains for audit entries to detect tampering.\n- HIPAA-ready notifications: Teams often use vendors that will sign a BAA (e.g., Twilio’s healthcare programs, certain email providers with HIPAA offerings). Even with a BAA, keep messages PHI-free and drive patients to the portal.\n- Consent capture and purpose-of-use: Systems log consent versions and purpose tags per access request. Staff access is audited by role and purpose, often with periodic attestation.\n- Data minimization in scheduling: Real deployments store only appointment essentials and rely on EHR for clinical context. This limits PHI exposure and simplifies compliance reviews.\n\nDiagram description\nImagine the system as a secure campus with guarded gates and clearly marked buildings:\n- Client Layer:\n  - Patient Web/Mobile Apps\n  - Staff/Admin Portal\n- Edge:\n  - API Gateway with WAF and rate limiting\n  - IAM/OIDC Provider with MFA\n- Services (each in its own secure building):\n  - Patient Portal Service\n  - Scheduling Service\n  - EHR Integration Service (with FHIR adapter)\n  - Notifications Service\n  - Audit Trail Service\n- Integration Spine:\n  - Secure Event Bus (topics: scheduling, notifications, audit, ehr-sync)\n  - Transactional Outbox in each service for reliable event publishing\n- Data Stores (vault rooms per building):\n  - Patient Portal DB (encrypted; PHI columns encrypted)\n  - Scheduling DB (encrypted; minimal PHI)\n  - EHR Integration DB (encrypted; ID mappings and sync state)\n  - Notifications DB (encrypted; templates and receipts; no PHI)\n  - Audit Store (immutable/WORM; long retention)\n- External:\n  - EHR System via secure VPN/private link (FHIR/HL7)\n  - Notification Channels (SMS, email, push) behind BAA and PHI-free policy\n- Security and Ops:\n  - KMS/HSM for key management\n  - SIEM for logs and alerts\n  - Backup/DR environment with encrypted, tested restores\n\nPrimary flows:\n1. Patient logs in -> API Gateway -> Patient Portal Service -> IAM -> session with MFA -> reads appointment data via Scheduling, with PHI fetch gated by consent.\n2. Patient books appointment -> Patient Portal calls Scheduling API -> Scheduling commits -> publishes AppointmentBooked -> Notifications queues reminder -> Audit Trail records the action.\n3. Scheduling triggers EHR sync -> EHR Integration pushes FHIR Appointment -> emits EHRSyncCompleted -> Audit Trail logs the exchange.\n4. Reminder sends via Notifications -> link to portal -> Audit entry saved; no PHI in message body.\n\nPractical exercises\n- Draw a data flow diagram marking where PHI exists and which module touches it. Remove PHI from any unnecessary path.\n- Define event payloads for AppointmentBooked and MessageDelivered, listing mandatory metadata (event_id, correlation_id, purpose_of_use, data_classification) and what is explicitly excluded (PHI).\n- Create a data classification matrix: fields by sensitivity, storage location, encryption method, retention period, and access roles.\n- Write an “audit story” for a suspicious access: identify the entries you’d expect to see across Audit Trail, SIEM, and service logs, then verify your architecture produces them.\n- Define RPO/RTO and run a tabletop DR scenario: EHR outage, event bus backlog, notifications vendor downtime, and a regional failover.\n\nClarifications to tailor the design\n- Cloud and region: Which cloud provider(s) and data residency constraints apply?\n- EHR vendors: Which EHRs and integration methods (FHIR only, HL7 v2, CCD, SMART on FHIR)?\n- Tenancy: Single-tenant per provider or multi-tenant across providers?\n- Scale targets: Peak appointments per day and concurrent users?\n- Notifications: Which channels do you plan to use, and do you have BAAs with those vendors?\n- Consent: How will patient consent be captured and versioned? Any special use cases like minors or proxies?\n- Availability goals: Specific RPO/RTO and uptime SLA?\n- Staff workflows: Do you need separate staff/admin portals with role-based access and break-glass procedures?\n\nIf you share those details, I can refine the contracts, storage boundaries, and resilience patterns to match your constraints while staying compliant.",
    "metrics": {
      "groundtruth-eval with azure:o4-mini_2025-04-16": {
        "content": "{\"score\":75,\"explanation\":\"Good content but misses style comparison, learning resources, ASCII diagram, and some definitions.\"}",
        "score": 75
      }
    },
    "compliance": "ok",
    "complianceText": "Thorough architecture focus, modules, clarity, examples, exercises, and questions.  \nOK",
    "isGroundtruth": true
  }
]