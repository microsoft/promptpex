Abstract: We evaluate deep convolutional architectures for large-scale image classification. Our proposed approach builds on ResNet-50 and improves training stability via stochastic depth, yielding strong performance on ImageNet. We further compare against VGG-16 and Inception-v3 baselines on CIFAR-10 and report consistent gains. A detailed ablation isolates the contribution of the residual blocks and squeeze-and-excitation modules when integrated into ResNet-50.
===
Abstract: We introduce GraphFormer, a transformer-style architecture for learning on graphs that unifies message passing and attention. On citation networks and molecular property prediction benchmarks, GraphFormer outperforms GraphSAGE, GAT, and GCN under identical training budgets. We analyze scaling behavior and show that GraphFormer benefits from increased depth without oversmoothing.
===
Abstract: Deep learning has achieved remarkable progress across perception and language tasks. In this study, we investigate training dynamics under different data curricula and optimization strategies, but we do not introduce or evaluate any specific named models. Our analysis focuses on generalizable phenomena across architectures, offering insights into representation learning without committing to a particular design.
===
Abstract: Multimodal vision-language learning benefits from large pretraining corpora. We fine-tune CLIP and ViT-L/14 encoders for zero-shot retrieval and evaluate cross-attention decoders initialized from BLIP-2. For text-to-image synthesis, we benchmark adapters built on top of DALL·E 2 and analyze the trade-off between fidelity and diversity on MS-COCO. Our findings highlight the importance of contrastive alignment in CLIP for downstream tasks.
===
Abstract: We study distributional word representations for low-resource NLP. We compare classical embeddings like word2vec, fastText, and GloVe with contextual representations from ELMo under limited supervision. Our results show that subword modeling in fastText provides robust performance in morphologically rich languages, while ELMo remains competitive when fine-tuned.
===
Abstract: This paper benchmarks traditional machine learning methods for tabular classification across 45 datasets. We include SVM with RBF kernels, k-NN, Random Forest, XGBoost, and LightGBM, carefully tuning hyperparameters via cross-validation. We report that boosted tree models such as XGBoost and LightGBM consistently dominate on medium-sized tabular data, while SVM performs well in high-dimensional sparse settings.
===
Abstract: We systematically evaluate instruction-following large language models on compositional reasoning. Models considered include GPT-3.5 Turbo, Llama 2-70B, Mistral-7B-Instruct, and Phi-3-mini. We propose a calibration technique that reduces hallucinations without sacrificing fluency, and demonstrate improved factuality across open-domain QA benchmarks.
===
Abstract: We investigate the effect of pretraining objectives on encoder-only transformers for sentence understanding. We fine-tune BERT-Base and BERT-Large (cased) with span masking and compare against RoBERTa-large and ALBERT-xxlarge. Our results indicate that dynamic masking combined with larger batch sizes yields consistent gains, particularly for RoBERTa-large on GLUE tasks.
===
Abstract: Efficient convolutional networks remain strong baselines for mobile applications. We revisit ResNet (He et al., 2016) with modern training tricks and compare to DenseNet-121, MobileNetV3-Large, and EfficientNet-B3 on ImageNet and ImageNet-ReaL. A latency-aware evaluation shows MobileNetV3-Large provides the best accuracy-throughput trade-off on edge devices.
===
Abstract: We present a comprehensive study of annotation artifacts in benchmark datasets spanning visual question answering and reading comprehension. While we evaluate various training regimes and metrics, we intentionally refrain from introducing a new model and instead focus on dataset design, protocol standardization, and error taxonomies for robust evaluation across COCO and SQuAD.
===
Abstract: Long-context modeling remains challenging for autoregressive systems. We compare Transformer-XL and XLNet as baselines for language modeling with extended context windows, and adapt sequence-to-sequence pretraining with T5, ByT5, and mT5 for long-form question answering. Our approach combines relative positional encodings with memory caching to improve coherence over thousands of tokens.
===
Abstract: We benchmark object detection frameworks for real-time applications. We evaluate YOLOv5 under varying input resolutions and compare it with two-stage detectors such as Faster R-CNN and Mask R-CNN, as well as transformer-based DETR and keypoint-based CenterNet. Our unified training recipe reveals that DETR benefits substantially from longer training schedules.
===
Abstract: Semantic segmentation in medical imaging requires precise boundary delineation. We propose a hybrid decoder that augments U-Net with multi-scale attention, and we compare against DeepLabv3+ and PSPNet using Dice and Hausdorff metrics. On two public MRI datasets, our approach yields consistent improvements without increasing the parameter count relative to U-Net.
===
Abstract: We examine structured prediction models for sequence labeling in noisy domains. Baselines include linear-chain CRF, HMM, and a neural BiLSTM-CRF. We further compare with Naive Bayes for token classification under distant supervision. Our experiments suggest that CRF-based models exhibit superior robustness to label noise when augmented with posterior regularization.
===
Abstract: Single image super-resolution has benefited from adversarial training, yet stability remains an issue. We revisit ESRGAN and Real-ESRGAN and propose a perceptual discriminator that reduces over-sharpening artifacts. Comparisons to SRGAN, EDSR, and SwinIR on DIV2K and RealSR indicate that Real-ESRGAN with our modification provides a better fidelity-perception trade-off.
===
Abstract: We present a unified framework for text-to-speech and speech recognition. For generation, we train Tacotron 2 and compare with the autoregressive WaveNet vocoder and neural source-filter baselines. For recognition, we fine-tune wav2vec 2.0 and HuBERT, and include DeepSpeech as a non-self-supervised baseline. Cross-task transfer improves both ASR WER and TTS MOS.
===
Abstract: We study planning and control with model-based and model-free reinforcement learning. Our evaluation spans AlphaZero and MuZero on board games, as well as off-policy DQN and on-policy PPO on Atari and MuJoCo. We propose a hybrid value-consistency loss that stabilizes MuZero’s training without degrading sample efficiency.
===
Abstract: We investigate multilingual machine translation under constrained compute. We compare MarianMT fine-tuned per language pair against massively multilingual models mBART50 and M2M-100, and evaluate the scaling behavior of NLLB-200 in low-resource regimes. Our curriculum sampling strategy yields improved BLEU in zero-shot directions without increasing model capacity.