[
  {
    "id": "d7ccbb3",
    "promptid": "1aa450e",
    "ruleid": 1,
    "rule": "The entire response must be a single JSON array literal and must contain no characters, words, or formatting outside the array.",
    "inverse": false,
    "testuid": "test-64071835",
    "scenario": "",
    "baseline": false,
    "testinput": "We present FoobarNet, a lightweight convolutional architecture for mobile vision tasks. Code and preprint available.",
    "model": "gemma2:9b",
    "input": "We present FoobarNet, a lightweight convolutional architecture for mobile vision tasks. Code and preprint available.",
    "output": "[\"FoobarNet\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Output is a JSON array with one model name as required, no extra content.\nOK"
  },
  {
    "id": "24720e9",
    "promptid": "1aa450e",
    "ruleid": 2,
    "rule": "A JSON array is defined as a sequence of elements enclosed by a leading \"[\" and a trailing \"]\" with elements separated by commas according to the JSON specification.",
    "inverse": false,
    "testuid": "test-74896303",
    "scenario": "",
    "baseline": false,
    "testinput": "We introduce AlphaNet and BetaNet for multi-task learning; both models outperform baselines on four benchmarks.",
    "model": "gemma2:9b",
    "input": "We introduce AlphaNet and BetaNet for multi-task learning; both models outperform baselines on four benchmarks.",
    "output": "[\"AlphaNet\", \"BetaNet\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Valid JSON array of quoted model names with no extra text.  \nOK"
  },
  {
    "id": "1aa450e",
    "promptid": "1aa450e",
    "ruleid": 3,
    "rule": "The array must not be empty and must contain at least one element.",
    "inverse": false,
    "testuid": "test-79925387",
    "scenario": "",
    "baseline": false,
    "testinput": "",
    "model": "gemma2:9b",
    "input": "",
    "output": "Please provide me with the abstract so I can extract the model names.  \n\nFor example, you would paste the abstract text here and I will do my best to identify the model names within it.   \n",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Chatbot asks for the abstract instead of extracting model names.\nERR"
  },
  {
    "id": "fbc8480",
    "promptid": "1aa450e",
    "ruleid": 4,
    "rule": "The response must not be a JSON array literal and should include extra commentary or markup outside any brackets, ideally prefixed by a Unicode BOM.",
    "inverse": true,
    "testuid": "test-59317763",
    "scenario": "",
    "baseline": false,
    "testinput": "We propose mBERT-Adapt, adapting multilingual BERT for cross-lingual transfer.",
    "model": "gemma2:9b",
    "input": "We propose mBERT-Adapt, adapting multilingual BERT for cross-lingual transfer.",
    "output": "[\"mBERT-Adapt\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Array is properly formatted with a single model name string.\nOK"
  },
  {
    "id": "e0bbea8",
    "promptid": "1aa450e",
    "ruleid": 15,
    "testuid": "test-49747978",
    "scenario": "",
    "baseline": false,
    "testinput": "We introduce GraphFormer, a Transformer-based graph learning model achieving state-of-the-art on OGB benchmarks. The architecture scales efficiently.",
    "model": "gemma2:9b",
    "input": "We introduce GraphFormer, a Transformer-based graph learning model achieving state-of-the-art on OGB benchmarks. The architecture scales efficiently.",
    "output": "[\"GraphFormer\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Array is in correct format with a single model name.  \nOK"
  },
  {
    "id": "264aead",
    "promptid": "1aa450e",
    "ruleid": 16,
    "testuid": "test-67868223",
    "scenario": "",
    "baseline": false,
    "testinput": "We compare GraphSAGE and GIN for node classification across citation networks, detailing inductive biases and training setups.",
    "model": "gemma2:9b",
    "input": "We compare GraphSAGE and GIN for node classification across citation networks, detailing inductive biases and training setups.",
    "output": "[\"GraphSAGE\", \"GIN\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Format matches required array of model names with correct string notation.  \nOK"
  },
  {
    "id": "040ac3b",
    "promptid": "1aa450e",
    "ruleid": 17,
    "testuid": "test-62684684",
    "scenario": "",
    "baseline": false,
    "testinput": "We propose ResNet-50-TR and compare against BERT and GPT-2; models are described in detail with comprehensive ablations.",
    "model": "gemma2:9b",
    "input": "We propose ResNet-50-TR and compare against BERT and GPT-2; models are described in detail with comprehensive ablations.",
    "output": "[\"ResNet-50-TR\", \"BERT\", \"GPT-2\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Proper JSON array of model names with no extra content.  \nOK"
  },
  {
    "id": "9634b39",
    "promptid": "1aa450e",
    "ruleid": 18,
    "testuid": "test-89746605",
    "scenario": "",
    "baseline": false,
    "testinput": "We release a variant called DeepFusionNet that excels on multimodal tasks such as VQA and captioning.",
    "model": "gemma2:9b",
    "input": "We release a variant called DeepFusionNet that excels on multimodal tasks such as VQA and captioning.",
    "output": "[\"DeepFusionNet\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Output is a properly formatted array of model names as required.  \nOK"
  },
  {
    "id": "b722272",
    "promptid": "1aa450e",
    "ruleid": 19,
    "testuid": "test-10313432",
    "scenario": "",
    "baseline": false,
    "testinput": "We present SequenceLab v1, a simple sequence labeling toolkit with CRFs and transformers backends.",
    "model": "gemma2:9b",
    "input": "We present SequenceLab v1, a simple sequence labeling toolkit with CRFs and transformers backends.",
    "output": "[\"SequenceLab v1\", \"CRFs\", \"transformers\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Format is correct: JSON array of strings as required, no extra commentary.  \nOK"
  },
  {
    "id": "202f432",
    "promptid": "1aa450e",
    "ruleid": 20,
    "testuid": "test-24887847",
    "scenario": "",
    "baseline": false,
    "testinput": "This survey synthesizes trends in self-supervised learning for audio and vision; no new model is introduced or named.",
    "model": "gemma2:9b",
    "input": "This survey synthesizes trends in self-supervised learning for audio and vision; no new model is introduced or named.",
    "output": "[\"NA\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Fallback response in correct format when unsure or no models found.\nOK"
  },
  {
    "id": "e7f03c6",
    "promptid": "1aa450e",
    "ruleid": 21,
    "testuid": "test-02256959",
    "scenario": "",
    "baseline": false,
    "testinput": "We conduct controlled ablations on architecture depth and width without proposing a new model; baselines are standard.",
    "model": "gemma2:9b",
    "input": "We conduct controlled ablations on architecture depth and width without proposing a new model; baselines are standard.",
    "output": "[\"NA\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Fallback response in correct format when unsure or no models found.\nOK"
  },
  {
    "id": "1aa450e",
    "promptid": "1aa450e",
    "ruleid": 22,
    "testuid": "test-32845896",
    "scenario": "",
    "baseline": false,
    "testinput": "",
    "model": "gemma2:9b",
    "input": "",
    "output": "Please provide me with the abstract so I can extract the model names.  \n\nFor example, you would paste the abstract text here and I will do my best to identify the model names within it.   \n",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Chatbot asks for the abstract instead of extracting model names.\nERR"
  },
  {
    "id": "857efa5",
    "promptid": "1aa450e",
    "ruleid": 23,
    "testuid": "test-60185492",
    "scenario": "",
    "baseline": false,
    "testinput": "No specific model is proposed; methods are generic and naming is intentionally omitted throughout.",
    "model": "gemma2:9b",
    "input": "No specific model is proposed; methods are generic and naming is intentionally omitted throughout.",
    "output": "[\"NA\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Fallback response in correct format when unsure or no models found.\nOK"
  },
  {
    "id": "3710ffa",
    "promptid": "1aa450e",
    "ruleid": 24,
    "testuid": "test-87787525",
    "scenario": "",
    "baseline": false,
    "testinput": "We evaluate BERT against XLNet on GLUE and SQuAD; our contribution does not introduce a new model.",
    "model": "gemma2:9b",
    "input": "We evaluate BERT against XLNet on GLUE and SQuAD; our contribution does not introduce a new model.",
    "output": "[\"BERT\", \"XLNet\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Output uses correct array format of model names, no extra text, strings returned as specified.  \nOK"
  },
  {
    "id": "4db7bdc",
    "promptid": "1aa450e",
    "ruleid": 25,
    "testuid": "test-72857613",
    "scenario": "",
    "baseline": false,
    "testinput": "We introduce LiteViT, a lightweight Vision Transformer optimized for mobile hardware with distillation and quantization-aware training.",
    "model": "gemma2:9b",
    "input": "We introduce LiteViT, a lightweight Vision Transformer optimized for mobile hardware with distillation and quantization-aware training.",
    "output": "[\"LiteViT\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Output is a JSON array with a single string, matching required format.\nOK"
  },
  {
    "id": "ddc8eec",
    "promptid": "1aa450e",
    "ruleid": 26,
    "testuid": "test-73993212",
    "scenario": "",
    "baseline": false,
    "testinput": "We fine-tune T5 and BART for summarization, detailing datasets, metrics, and training schedules.",
    "model": "gemma2:9b",
    "input": "We fine-tune T5 and BART for summarization, detailing datasets, metrics, and training schedules.",
    "output": "[\"T5\", \"BART\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Output is an array of strings in the required format.  \nOK"
  },
  {
    "id": "9b5cd34",
    "promptid": "1aa450e",
    "ruleid": 27,
    "testuid": "test-66058669",
    "scenario": "",
    "baseline": false,
    "testinput": "We present RoBERTa-XS, a compact pre-trained encoder tailored for latency-critical NLU on-device applications.",
    "model": "gemma2:9b",
    "input": "We present RoBERTa-XS, a compact pre-trained encoder tailored for latency-critical NLU on-device applications.",
    "output": "[\"RoBERTa-XS\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Output format correctly uses a JSON array of model names with no extra text.\nOK"
  },
  {
    "id": "e1bdda1",
    "promptid": "1aa450e",
    "ruleid": 28,
    "testuid": "test-24288487",
    "scenario": "",
    "baseline": false,
    "testinput": "We introduce three modelsBERT, GPT-2, and RoBERTaand show complementary strengths across GLUE, SuperGLUE, and WMT tasks.",
    "model": "gemma2:9b",
    "input": "We introduce three modelsBERT, GPT-2, and RoBERTaand show complementary strengths across GLUE, SuperGLUE, and WMT tasks.",
    "output": "[\"BERT\", \"GPT-2\", \"RoBERTa\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Format is correct: array of model names in strings.\nOK"
  },
  {
    "id": "76fc8bd",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-27n98w97g",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We evaluate deep convolutional architectures for large-scale image classification. Our proposed approach builds on ResNet-50 and improves training stability via stochastic depth, yielding strong performance on ImageNet. We further compare against VGG-16 and Inception-v3 baselines on CIFAR-10 and report consistent gains. A detailed ablation isolates the contribution of the residual blocks and squeeze-and-excitation modules when integrated into ResNet-50.",
    "model": "gemma2:9b",
    "input": "Abstract: We evaluate deep convolutional architectures for large-scale image classification. Our proposed approach builds on ResNet-50 and improves training stability via stochastic depth, yielding strong performance on ImageNet. We further compare against VGG-16 and Inception-v3 baselines on CIFAR-10 and report consistent gains. A detailed ablation isolates the contribution of the residual blocks and squeeze-and-excitation modules when integrated into ResNet-50.",
    "output": "[\"ResNet-50\", \"VGG-16\", \"Inception-v3\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "List is properly formatted as an array of model names.\nOK"
  },
  {
    "id": "f207da8",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-pfykm77fb",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We introduce GraphFormer, a transformer-style architecture for learning on graphs that unifies message passing and attention. On citation networks and molecular property prediction benchmarks, GraphFormer outperforms GraphSAGE, GAT, and GCN under identical training budgets. We analyze scaling behavior and show that GraphFormer benefits from increased depth without oversmoothing.",
    "model": "gemma2:9b",
    "input": "Abstract: We introduce GraphFormer, a transformer-style architecture for learning on graphs that unifies message passing and attention. On citation networks and molecular property prediction benchmarks, GraphFormer outperforms GraphSAGE, GAT, and GCN under identical training budgets. We analyze scaling behavior and show that GraphFormer benefits from increased depth without oversmoothing.",
    "output": "[\"GraphFormer\", \"GraphSAGE\", \"GAT\", \"GCN\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Valid JSON array of model names as specified; follows required format.  \nOK"
  },
  {
    "id": "1965da3",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-kdk5hz4op",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: Deep learning has achieved remarkable progress across perception and language tasks. In this study, we investigate training dynamics under different data curricula and optimization strategies, but we do not introduce or evaluate any specific named models. Our analysis focuses on generalizable phenomena across architectures, offering insights into representation learning without committing to a particular design.",
    "model": "gemma2:9b",
    "input": "Abstract: Deep learning has achieved remarkable progress across perception and language tasks. In this study, we investigate training dynamics under different data curricula and optimization strategies, but we do not introduce or evaluate any specific named models. Our analysis focuses on generalizable phenomena across architectures, offering insights into representation learning without committing to a particular design.",
    "output": "[\"NA\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Fallback response in correct format when unsure or no models found.\nOK"
  },
  {
    "id": "1981336",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-la8rzana9",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: Multimodal vision-language learning benefits from large pretraining corpora. We fine-tune CLIP and ViT-L/14 encoders for zero-shot retrieval and evaluate cross-attention decoders initialized from BLIP-2. For text-to-image synthesis, we benchmark adapters built on top of DALL路E 2 and analyze the trade-off between fidelity and diversity on MS-COCO. Our findings highlight the importance of contrastive alignment in CLIP for downstream tasks.",
    "model": "gemma2:9b",
    "input": "Abstract: Multimodal vision-language learning benefits from large pretraining corpora. We fine-tune CLIP and ViT-L/14 encoders for zero-shot retrieval and evaluate cross-attention decoders initialized from BLIP-2. For text-to-image synthesis, we benchmark adapters built on top of DALL路E 2 and analyze the trade-off between fidelity and diversity on MS-COCO. Our findings highlight the importance of contrastive alignment in CLIP for downstream tasks.",
    "output": "[\"CLIP\", \"ViT-L/14\", \"BLIP-2\", \"DALL路E 2\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Output is correctly formatted as an array of model name strings.\nOK"
  },
  {
    "id": "121fca8",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-1l1063mhk",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We study distributional word representations for low-resource NLP. We compare classical embeddings like word2vec, fastText, and GloVe with contextual representations from ELMo under limited supervision. Our results show that subword modeling in fastText provides robust performance in morphologically rich languages, while ELMo remains competitive when fine-tuned.",
    "model": "gemma2:9b",
    "input": "Abstract: We study distributional word representations for low-resource NLP. We compare classical embeddings like word2vec, fastText, and GloVe with contextual representations from ELMo under limited supervision. Our results show that subword modeling in fastText provides robust performance in morphologically rich languages, while ELMo remains competitive when fine-tuned.",
    "output": "[\"word2vec\", \"fastText\", \"GloVe\", \"ELMo\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Properly formatted list of model names per specification without extra text.  \nOK"
  },
  {
    "id": "28a7a21",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-tb1rgdeno",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: This paper benchmarks traditional machine learning methods for tabular classification across 45 datasets. We include SVM with RBF kernels, k-NN, Random Forest, XGBoost, and LightGBM, carefully tuning hyperparameters via cross-validation. We report that boosted tree models such as XGBoost and LightGBM consistently dominate on medium-sized tabular data, while SVM performs well in high-dimensional sparse settings.",
    "model": "gemma2:9b",
    "input": "Abstract: This paper benchmarks traditional machine learning methods for tabular classification across 45 datasets. We include SVM with RBF kernels, k-NN, Random Forest, XGBoost, and LightGBM, carefully tuning hyperparameters via cross-validation. We report that boosted tree models such as XGBoost and LightGBM consistently dominate on medium-sized tabular data, while SVM performs well in high-dimensional sparse settings.",
    "output": "[\"SVM\", \"k-NN\", \"Random Forest\", \"XGBoost\", \"LightGBM\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Output is an array of strings matching required format and contains plausible model names.\nOK"
  },
  {
    "id": "2ea75e5",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-zk3zt09y2",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We systematically evaluate instruction-following large language models on compositional reasoning. Models considered include GPT-3.5 Turbo, Llama 2-70B, Mistral-7B-Instruct, and Phi-3-mini. We propose a calibration technique that reduces hallucinations without sacrificing fluency, and demonstrate improved factuality across open-domain QA benchmarks.",
    "model": "gemma2:9b",
    "input": "Abstract: We systematically evaluate instruction-following large language models on compositional reasoning. Models considered include GPT-3.5 Turbo, Llama 2-70B, Mistral-7B-Instruct, and Phi-3-mini. We propose a calibration technique that reduces hallucinations without sacrificing fluency, and demonstrate improved factuality across open-domain QA benchmarks.",
    "output": "[\"GPT-3.5 Turbo\", \"Llama 2-70B\", \"Mistral-7B-Instruct\", \"Phi-3-mini\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Format matches required array of model names in correct string format.\nOK"
  },
  {
    "id": "05f27f9",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-wb1zmlu65",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We investigate the effect of pretraining objectives on encoder-only transformers for sentence understanding. We fine-tune BERT-Base and BERT-Large (cased) with span masking and compare against RoBERTa-large and ALBERT-xxlarge. Our results indicate that dynamic masking combined with larger batch sizes yields consistent gains, particularly for RoBERTa-large on GLUE tasks.",
    "model": "gemma2:9b",
    "input": "Abstract: We investigate the effect of pretraining objectives on encoder-only transformers for sentence understanding. We fine-tune BERT-Base and BERT-Large (cased) with span masking and compare against RoBERTa-large and ALBERT-xxlarge. Our results indicate that dynamic masking combined with larger batch sizes yields consistent gains, particularly for RoBERTa-large on GLUE tasks.",
    "output": "[\"BERT-Base\", \"BERT-Large\", \"RoBERTa-large\", \"ALBERT-xxlarge\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Output is exactly an array of model names as specified, uses correct brackets and quotes, no additional commentary.\nOK"
  },
  {
    "id": "1eee10e",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-4xpzfcy7p",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: Efficient convolutional networks remain strong baselines for mobile applications. We revisit ResNet (He et al., 2016) with modern training tricks and compare to DenseNet-121, MobileNetV3-Large, and EfficientNet-B3 on ImageNet and ImageNet-ReaL. A latency-aware evaluation shows MobileNetV3-Large provides the best accuracy-throughput trade-off on edge devices.",
    "model": "gemma2:9b",
    "input": "Abstract: Efficient convolutional networks remain strong baselines for mobile applications. We revisit ResNet (He et al., 2016) with modern training tricks and compare to DenseNet-121, MobileNetV3-Large, and EfficientNet-B3 on ImageNet and ImageNet-ReaL. A latency-aware evaluation shows MobileNetV3-Large provides the best accuracy-throughput trade-off on edge devices.",
    "output": "[\"ResNet\", \"DenseNet-121\", \"MobileNetV3-Large\", \"EfficientNet-B3\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Complies with required array format and model name listing.\nOK"
  },
  {
    "id": "18e8ffd",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-h285zdo1g",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We present a comprehensive study of annotation artifacts in benchmark datasets spanning visual question answering and reading comprehension. While we evaluate various training regimes and metrics, we intentionally refrain from introducing a new model and instead focus on dataset design, protocol standardization, and error taxonomies for robust evaluation across COCO and SQuAD.",
    "model": "gemma2:9b",
    "input": "Abstract: We present a comprehensive study of annotation artifacts in benchmark datasets spanning visual question answering and reading comprehension. While we evaluate various training regimes and metrics, we intentionally refrain from introducing a new model and instead focus on dataset design, protocol standardization, and error taxonomies for robust evaluation across COCO and SQuAD.",
    "output": "[\"NA\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Fallback response in correct format when unsure or no models found.\nOK"
  },
  {
    "id": "821ec90",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-zmsn89fp9",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: Long-context modeling remains challenging for autoregressive systems. We compare Transformer-XL and XLNet as baselines for language modeling with extended context windows, and adapt sequence-to-sequence pretraining with T5, ByT5, and mT5 for long-form question answering. Our approach combines relative positional encodings with memory caching to improve coherence over thousands of tokens.",
    "model": "gemma2:9b",
    "input": "Abstract: Long-context modeling remains challenging for autoregressive systems. We compare Transformer-XL and XLNet as baselines for language modeling with extended context windows, and adapt sequence-to-sequence pretraining with T5, ByT5, and mT5 for long-form question answering. Our approach combines relative positional encodings with memory caching to improve coherence over thousands of tokens.",
    "output": "[\"Transformer-XL\", \"XLNet\", \"T5\", \"ByT5\", \"mT5\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Output is a JSON array of model names as required.\nOK"
  },
  {
    "id": "84e9acc",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-lwygnzf5y",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We benchmark object detection frameworks for real-time applications. We evaluate YOLOv5 under varying input resolutions and compare it with two-stage detectors such as Faster R-CNN and Mask R-CNN, as well as transformer-based DETR and keypoint-based CenterNet. Our unified training recipe reveals that DETR benefits substantially from longer training schedules.",
    "model": "gemma2:9b",
    "input": "Abstract: We benchmark object detection frameworks for real-time applications. We evaluate YOLOv5 under varying input resolutions and compare it with two-stage detectors such as Faster R-CNN and Mask R-CNN, as well as transformer-based DETR and keypoint-based CenterNet. Our unified training recipe reveals that DETR benefits substantially from longer training schedules.",
    "output": "[\"YOLOv5\", \"Faster R-CNN\", \"Mask R-CNN\", \"DETR\", \"CenterNet\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Format is correct: array of model name strings.\nOK"
  },
  {
    "id": "4b61397",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-qhcz4i16e",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: Semantic segmentation in medical imaging requires precise boundary delineation. We propose a hybrid decoder that augments U-Net with multi-scale attention, and we compare against DeepLabv3+ and PSPNet using Dice and Hausdorff metrics. On two public MRI datasets, our approach yields consistent improvements without increasing the parameter count relative to U-Net.",
    "model": "gemma2:9b",
    "input": "Abstract: Semantic segmentation in medical imaging requires precise boundary delineation. We propose a hybrid decoder that augments U-Net with multi-scale attention, and we compare against DeepLabv3+ and PSPNet using Dice and Hausdorff metrics. On two public MRI datasets, our approach yields consistent improvements without increasing the parameter count relative to U-Net.",
    "output": "[\"U-Net\", \"DeepLabv3+\", \"PSPNet\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Output is an array of strings formatted correctly as required.\nOK"
  },
  {
    "id": "232fc26",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-x969paum6",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We examine structured prediction models for sequence labeling in noisy domains. Baselines include linear-chain CRF, HMM, and a neural BiLSTM-CRF. We further compare with Naive Bayes for token classification under distant supervision. Our experiments suggest that CRF-based models exhibit superior robustness to label noise when augmented with posterior regularization.",
    "model": "gemma2:9b",
    "input": "Abstract: We examine structured prediction models for sequence labeling in noisy domains. Baselines include linear-chain CRF, HMM, and a neural BiLSTM-CRF. We further compare with Naive Bayes for token classification under distant supervision. Our experiments suggest that CRF-based models exhibit superior robustness to label noise when augmented with posterior regularization.",
    "output": "[\"linear-chain CRF\", \"HMM\", \"BiLSTM-CRF\", \"Naive Bayes\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Array is correctly formatted as quoted model names per specification.  \nOK"
  },
  {
    "id": "d50c7ac",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-p6quzscah",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: Single image super-resolution has benefited from adversarial training, yet stability remains an issue. We revisit ESRGAN and Real-ESRGAN and propose a perceptual discriminator that reduces over-sharpening artifacts. Comparisons to SRGAN, EDSR, and SwinIR on DIV2K and RealSR indicate that Real-ESRGAN with our modification provides a better fidelity-perception trade-off.",
    "model": "gemma2:9b",
    "input": "Abstract: Single image super-resolution has benefited from adversarial training, yet stability remains an issue. We revisit ESRGAN and Real-ESRGAN and propose a perceptual discriminator that reduces over-sharpening artifacts. Comparisons to SRGAN, EDSR, and SwinIR on DIV2K and RealSR indicate that Real-ESRGAN with our modification provides a better fidelity-perception trade-off.",
    "output": "[\"ESRGAN\", \"Real-ESRGAN\", \"SRGAN\", \"EDSR\", \"SwinIR\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Output is correctly formatted as an array of model names with no extra text.\nOK"
  },
  {
    "id": "8c3b0ef",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-lwppn0yv0",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We present a unified framework for text-to-speech and speech recognition. For generation, we train Tacotron 2 and compare with the autoregressive WaveNet vocoder and neural source-filter baselines. For recognition, we fine-tune wav2vec 2.0 and HuBERT, and include DeepSpeech as a non-self-supervised baseline. Cross-task transfer improves both ASR WER and TTS MOS.",
    "model": "gemma2:9b",
    "input": "Abstract: We present a unified framework for text-to-speech and speech recognition. For generation, we train Tacotron 2 and compare with the autoregressive WaveNet vocoder and neural source-filter baselines. For recognition, we fine-tune wav2vec 2.0 and HuBERT, and include DeepSpeech as a non-self-supervised baseline. Cross-task transfer improves both ASR WER and TTS MOS.",
    "output": "[\"Tacotron 2\", \"WaveNet\", \"wav2vec 2.0\", \"HuBERT\", \"DeepSpeech\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "The output is a JSON array of strings only, matching the prescribed format for listing model names.\nOK"
  },
  {
    "id": "1bf2509",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-btsjags2r",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We study planning and control with model-based and model-free reinforcement learning. Our evaluation spans AlphaZero and MuZero on board games, as well as off-policy DQN and on-policy PPO on Atari and MuJoCo. We propose a hybrid value-consistency loss that stabilizes MuZeros training without degrading sample efficiency.",
    "model": "gemma2:9b",
    "input": "Abstract: We study planning and control with model-based and model-free reinforcement learning. Our evaluation spans AlphaZero and MuZero on board games, as well as off-policy DQN and on-policy PPO on Atari and MuJoCo. We propose a hybrid value-consistency loss that stabilizes MuZeros training without degrading sample efficiency.",
    "output": "[\"AlphaZero\", \"MuZero\", \"DQN\", \"PPO\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Output is a JSON array of model names with correct format and no extra text.\nOK"
  },
  {
    "id": "09b0a23",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-eqilegh9y",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We investigate multilingual machine translation under constrained compute. We compare MarianMT fine-tuned per language pair against massively multilingual models mBART50 and M2M-100, and evaluate the scaling behavior of NLLB-200 in low-resource regimes. Our curriculum sampling strategy yields improved BLEU in zero-shot directions without increasing model capacity.",
    "model": "gemma2:9b",
    "input": "Abstract: We investigate multilingual machine translation under constrained compute. We compare MarianMT fine-tuned per language pair against massively multilingual models mBART50 and M2M-100, and evaluate the scaling behavior of NLLB-200 in low-resource regimes. Our curriculum sampling strategy yields improved BLEU in zero-shot directions without increasing model capacity.",
    "output": "[\"MarianMT\", \"mBART50\", \"M2M-100\", \"NLLB-200\"] \n",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Format is correct: only an array of model names as specified.\nOK"
  },
  {
    "id": "d7ccbb3",
    "promptid": "1aa450e",
    "ruleid": 1,
    "rule": "The entire response must be a single JSON array literal and must contain no characters, words, or formatting outside the array.",
    "inverse": false,
    "testuid": "test-64071835",
    "scenario": "",
    "baseline": false,
    "testinput": "We present FoobarNet, a lightweight convolutional architecture for mobile vision tasks. Code and preprint available.",
    "model": "qwen2.5:3b",
    "input": "We present FoobarNet, a lightweight convolutional architecture for mobile vision tasks. Code and preprint available.",
    "output": "[[\"FoobarNet\"]]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Output is a nested array instead of a single-level array as specified.\nERR"
  },
  {
    "id": "24720e9",
    "promptid": "1aa450e",
    "ruleid": 2,
    "rule": "A JSON array is defined as a sequence of elements enclosed by a leading \"[\" and a trailing \"]\" with elements separated by commas according to the JSON specification.",
    "inverse": false,
    "testuid": "test-74896303",
    "scenario": "",
    "baseline": false,
    "testinput": "We introduce AlphaNet and BetaNet for multi-task learning; both models outperform baselines on four benchmarks.",
    "model": "qwen2.5:3b",
    "input": "We introduce AlphaNet and BetaNet for multi-task learning; both models outperform baselines on four benchmarks.",
    "output": "[[\"AlphaNet\", \"BetaNet\"]]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Output is a nested array instead of a flat array of strings.\nERR"
  },
  {
    "id": "1aa450e",
    "promptid": "1aa450e",
    "ruleid": 3,
    "rule": "The array must not be empty and must contain at least one element.",
    "inverse": false,
    "testuid": "test-79925387",
    "scenario": "",
    "baseline": false,
    "testinput": "",
    "model": "qwen2.5:3b",
    "input": "",
    "output": "[\"NA\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Formatted correctly as an array with [\"NA\"] per instructions.\nOK"
  },
  {
    "id": "fbc8480",
    "promptid": "1aa450e",
    "ruleid": 4,
    "rule": "The response must not be a JSON array literal and should include extra commentary or markup outside any brackets, ideally prefixed by a Unicode BOM.",
    "inverse": true,
    "testuid": "test-59317763",
    "scenario": "",
    "baseline": false,
    "testinput": "We propose mBERT-Adapt, adapting multilingual BERT for cross-lingual transfer.",
    "model": "qwen2.5:3b",
    "input": "We propose mBERT-Adapt, adapting multilingual BERT for cross-lingual transfer.",
    "output": "[[\"mBERT-Adapt\"]]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Output uses nested array instead of single-level list as specified.\nERR"
  },
  {
    "id": "e0bbea8",
    "promptid": "1aa450e",
    "ruleid": 15,
    "testuid": "test-49747978",
    "scenario": "",
    "baseline": false,
    "testinput": "We introduce GraphFormer, a Transformer-based graph learning model achieving state-of-the-art on OGB benchmarks. The architecture scales efficiently.",
    "model": "qwen2.5:3b",
    "input": "We introduce GraphFormer, a Transformer-based graph learning model achieving state-of-the-art on OGB benchmarks. The architecture scales efficiently.",
    "output": "[[\"GraphFormer\"]]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Output format incorrect: nested array instead of a flat array of strings.  \nERR"
  },
  {
    "id": "264aead",
    "promptid": "1aa450e",
    "ruleid": 16,
    "testuid": "test-67868223",
    "scenario": "",
    "baseline": false,
    "testinput": "We compare GraphSAGE and GIN for node classification across citation networks, detailing inductive biases and training setups.",
    "model": "qwen2.5:3b",
    "input": "We compare GraphSAGE and GIN for node classification across citation networks, detailing inductive biases and training setups.",
    "output": "[\\\"GraphSAGE\\\", \\\"GIN\\\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Output is properly formatted as an array of model names.\nOK"
  },
  {
    "id": "040ac3b",
    "promptid": "1aa450e",
    "ruleid": 17,
    "testuid": "test-62684684",
    "scenario": "",
    "baseline": false,
    "testinput": "We propose ResNet-50-TR and compare against BERT and GPT-2; models are described in detail with comprehensive ablations.",
    "model": "qwen2.5:3b",
    "input": "We propose ResNet-50-TR and compare against BERT and GPT-2; models are described in detail with comprehensive ablations.",
    "output": "[\\\"ResNet-50-TR\\\", \\\"BERT\\\", \\\"GPT-2\\\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Output is an array of strings as specified, with no extra content or formatting errors.  \nOK"
  },
  {
    "id": "9634b39",
    "promptid": "1aa450e",
    "ruleid": 18,
    "testuid": "test-89746605",
    "scenario": "",
    "baseline": false,
    "testinput": "We release a variant called DeepFusionNet that excels on multimodal tasks such as VQA and captioning.",
    "model": "qwen2.5:3b",
    "input": "We release a variant called DeepFusionNet that excels on multimodal tasks such as VQA and captioning.",
    "output": "[[\"DeepFusionNet\"]]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Used nested array instead of a single-level array as required by the format.\nERR"
  },
  {
    "id": "b722272",
    "promptid": "1aa450e",
    "ruleid": 19,
    "testuid": "test-10313432",
    "scenario": "",
    "baseline": false,
    "testinput": "We present SequenceLab v1, a simple sequence labeling toolkit with CRFs and transformers backends.",
    "model": "qwen2.5:3b",
    "input": "We present SequenceLab v1, a simple sequence labeling toolkit with CRFs and transformers backends.",
    "output": "[\"SequenceLab\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Output is an array of strings, matching required format.\nOK"
  },
  {
    "id": "202f432",
    "promptid": "1aa450e",
    "ruleid": 20,
    "testuid": "test-24887847",
    "scenario": "",
    "baseline": false,
    "testinput": "This survey synthesizes trends in self-supervised learning for audio and vision; no new model is introduced or named.",
    "model": "qwen2.5:3b",
    "input": "This survey synthesizes trends in self-supervised learning for audio and vision; no new model is introduced or named.",
    "output": "[[\"NA\"]]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Output is a nested array instead of the required flat array format.\nERR"
  },
  {
    "id": "e7f03c6",
    "promptid": "1aa450e",
    "ruleid": 21,
    "testuid": "test-02256959",
    "scenario": "",
    "baseline": false,
    "testinput": "We conduct controlled ablations on architecture depth and width without proposing a new model; baselines are standard.",
    "model": "qwen2.5:3b",
    "input": "We conduct controlled ablations on architecture depth and width without proposing a new model; baselines are standard.",
    "output": "[\\\"NA\\\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "No model names extracted, using correct fallback format.\nOK"
  },
  {
    "id": "1aa450e",
    "promptid": "1aa450e",
    "ruleid": 22,
    "testuid": "test-32845896",
    "scenario": "",
    "baseline": false,
    "testinput": "",
    "model": "qwen2.5:3b",
    "input": "",
    "output": "[\"NA\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Formatted correctly as an array with [\"NA\"] per instructions.\nOK"
  },
  {
    "id": "857efa5",
    "promptid": "1aa450e",
    "ruleid": 23,
    "testuid": "test-60185492",
    "scenario": "",
    "baseline": false,
    "testinput": "No specific model is proposed; methods are generic and naming is intentionally omitted throughout.",
    "model": "qwen2.5:3b",
    "input": "No specific model is proposed; methods are generic and naming is intentionally omitted throughout.",
    "output": "[\"NA\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Formatted correctly as an array with [\"NA\"] per instructions.\nOK"
  },
  {
    "id": "3710ffa",
    "promptid": "1aa450e",
    "ruleid": 24,
    "testuid": "test-87787525",
    "scenario": "",
    "baseline": false,
    "testinput": "We evaluate BERT against XLNet on GLUE and SQuAD; our contribution does not introduce a new model.",
    "model": "qwen2.5:3b",
    "input": "We evaluate BERT against XLNet on GLUE and SQuAD; our contribution does not introduce a new model.",
    "output": "[[\"BERT\", \"XLNet\"]]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Output uses nested array instead of flat array.\nERR"
  },
  {
    "id": "4db7bdc",
    "promptid": "1aa450e",
    "ruleid": 25,
    "testuid": "test-72857613",
    "scenario": "",
    "baseline": false,
    "testinput": "We introduce LiteViT, a lightweight Vision Transformer optimized for mobile hardware with distillation and quantization-aware training.",
    "model": "qwen2.5:3b",
    "input": "We introduce LiteViT, a lightweight Vision Transformer optimized for mobile hardware with distillation and quantization-aware training.",
    "output": "[[\"LiteViT\"]]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Output uses a nested array instead of a single-level array as specified.\nERR"
  },
  {
    "id": "ddc8eec",
    "promptid": "1aa450e",
    "ruleid": 26,
    "testuid": "test-73993212",
    "scenario": "",
    "baseline": false,
    "testinput": "We fine-tune T5 and BART for summarization, detailing datasets, metrics, and training schedules.",
    "model": "qwen2.5:3b",
    "input": "We fine-tune T5 and BART for summarization, detailing datasets, metrics, and training schedules.",
    "output": "[[\"T5\", \"BART\"]]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Output uses nested array instead of single-level array as required.\nERR"
  },
  {
    "id": "9b5cd34",
    "promptid": "1aa450e",
    "ruleid": 27,
    "testuid": "test-66058669",
    "scenario": "",
    "baseline": false,
    "testinput": "We present RoBERTa-XS, a compact pre-trained encoder tailored for latency-critical NLU on-device applications.",
    "model": "qwen2.5:3b",
    "input": "We present RoBERTa-XS, a compact pre-trained encoder tailored for latency-critical NLU on-device applications.",
    "output": "[[\"RoBERTa-XS\"]]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Output is nested array instead of flat array.\nERR"
  },
  {
    "id": "e1bdda1",
    "promptid": "1aa450e",
    "ruleid": 28,
    "testuid": "test-24288487",
    "scenario": "",
    "baseline": false,
    "testinput": "We introduce three modelsBERT, GPT-2, and RoBERTaand show complementary strengths across GLUE, SuperGLUE, and WMT tasks.",
    "model": "qwen2.5:3b",
    "input": "We introduce three modelsBERT, GPT-2, and RoBERTaand show complementary strengths across GLUE, SuperGLUE, and WMT tasks.",
    "output": "[[\"BERT\", \"GPT-2\", \"RoBERTa\"]]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "The output uses a nested array instead of a flat array of model names as specified.\nERR"
  },
  {
    "id": "76fc8bd",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-27n98w97g",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We evaluate deep convolutional architectures for large-scale image classification. Our proposed approach builds on ResNet-50 and improves training stability via stochastic depth, yielding strong performance on ImageNet. We further compare against VGG-16 and Inception-v3 baselines on CIFAR-10 and report consistent gains. A detailed ablation isolates the contribution of the residual blocks and squeeze-and-excitation modules when integrated into ResNet-50.",
    "model": "qwen2.5:3b",
    "input": "Abstract: We evaluate deep convolutional architectures for large-scale image classification. Our proposed approach builds on ResNet-50 and improves training stability via stochastic depth, yielding strong performance on ImageNet. We further compare against VGG-16 and Inception-v3 baselines on CIFAR-10 and report consistent gains. A detailed ablation isolates the contribution of the residual blocks and squeeze-and-excitation modules when integrated into ResNet-50.",
    "output": "[\"ResNet-50\", \"VGG-16\", \"Inception-v3\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Proper array of model names with correct format; no rules broken.\nOK"
  },
  {
    "id": "f207da8",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-pfykm77fb",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We introduce GraphFormer, a transformer-style architecture for learning on graphs that unifies message passing and attention. On citation networks and molecular property prediction benchmarks, GraphFormer outperforms GraphSAGE, GAT, and GCN under identical training budgets. We analyze scaling behavior and show that GraphFormer benefits from increased depth without oversmoothing.",
    "model": "qwen2.5:3b",
    "input": "Abstract: We introduce GraphFormer, a transformer-style architecture for learning on graphs that unifies message passing and attention. On citation networks and molecular property prediction benchmarks, GraphFormer outperforms GraphSAGE, GAT, and GCN under identical training budgets. We analyze scaling behavior and show that GraphFormer benefits from increased depth without oversmoothing.",
    "output": "[[\"GraphFormer\"]]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Output format incorrect: nested array instead of a flat array of strings.  \nERR"
  },
  {
    "id": "1965da3",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-kdk5hz4op",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: Deep learning has achieved remarkable progress across perception and language tasks. In this study, we investigate training dynamics under different data curricula and optimization strategies, but we do not introduce or evaluate any specific named models. Our analysis focuses on generalizable phenomena across architectures, offering insights into representation learning without committing to a particular design.",
    "model": "qwen2.5:3b",
    "input": "Abstract: Deep learning has achieved remarkable progress across perception and language tasks. In this study, we investigate training dynamics under different data curricula and optimization strategies, but we do not introduce or evaluate any specific named models. Our analysis focuses on generalizable phenomena across architectures, offering insights into representation learning without committing to a particular design.",
    "output": "[Namespace]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Output is not in required array format or contains valid model names.\nERR"
  },
  {
    "id": "1981336",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-la8rzana9",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: Multimodal vision-language learning benefits from large pretraining corpora. We fine-tune CLIP and ViT-L/14 encoders for zero-shot retrieval and evaluate cross-attention decoders initialized from BLIP-2. For text-to-image synthesis, we benchmark adapters built on top of DALL路E 2 and analyze the trade-off between fidelity and diversity on MS-COCO. Our findings highlight the importance of contrastive alignment in CLIP for downstream tasks.",
    "model": "qwen2.5:3b",
    "input": "Abstract: Multimodal vision-language learning benefits from large pretraining corpora. We fine-tune CLIP and ViT-L/14 encoders for zero-shot retrieval and evaluate cross-attention decoders initialized from BLIP-2. For text-to-image synthesis, we benchmark adapters built on top of DALL路E 2 and analyze the trade-off between fidelity and diversity on MS-COCO. Our findings highlight the importance of contrastive alignment in CLIP for downstream tasks.",
    "output": "[[\"CLIP\", \"ViT-L/14\", \"BLIP-2\", \"DALL路E 2\"]]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Output uses a nested array instead of a single flat array of strings.\nERR"
  },
  {
    "id": "121fca8",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-1l1063mhk",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We study distributional word representations for low-resource NLP. We compare classical embeddings like word2vec, fastText, and GloVe with contextual representations from ELMo under limited supervision. Our results show that subword modeling in fastText provides robust performance in morphologically rich languages, while ELMo remains competitive when fine-tuned.",
    "model": "qwen2.5:3b",
    "input": "Abstract: We study distributional word representations for low-resource NLP. We compare classical embeddings like word2vec, fastText, and GloVe with contextual representations from ELMo under limited supervision. Our results show that subword modeling in fastText provides robust performance in morphologically rich languages, while ELMo remains competitive when fine-tuned.",
    "output": "[\"word2vec\", \"fastText\", \"GloVe\", \"ELMo\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "The output is a JSON-style array of strings listing model names, matching the required format.\nOK"
  },
  {
    "id": "28a7a21",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-tb1rgdeno",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: This paper benchmarks traditional machine learning methods for tabular classification across 45 datasets. We include SVM with RBF kernels, k-NN, Random Forest, XGBoost, and LightGBM, carefully tuning hyperparameters via cross-validation. We report that boosted tree models such as XGBoost and LightGBM consistently dominate on medium-sized tabular data, while SVM performs well in high-dimensional sparse settings.",
    "model": "qwen2.5:3b",
    "input": "Abstract: This paper benchmarks traditional machine learning methods for tabular classification across 45 datasets. We include SVM with RBF kernels, k-NN, Random Forest, XGBoost, and LightGBM, carefully tuning hyperparameters via cross-validation. We report that boosted tree models such as XGBoost and LightGBM consistently dominate on medium-sized tabular data, while SVM performs well in high-dimensional sparse settings.",
    "output": "[[\"XGBoost\", \"LightGBM\"]]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Output wraps models in a nested array instead of the required flat array format.\nERR"
  },
  {
    "id": "2ea75e5",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-zk3zt09y2",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We systematically evaluate instruction-following large language models on compositional reasoning. Models considered include GPT-3.5 Turbo, Llama 2-70B, Mistral-7B-Instruct, and Phi-3-mini. We propose a calibration technique that reduces hallucinations without sacrificing fluency, and demonstrate improved factuality across open-domain QA benchmarks.",
    "model": "qwen2.5:3b",
    "input": "Abstract: We systematically evaluate instruction-following large language models on compositional reasoning. Models considered include GPT-3.5 Turbo, Llama 2-70B, Mistral-7B-Instruct, and Phi-3-mini. We propose a calibration technique that reduces hallucinations without sacrificing fluency, and demonstrate improved factuality across open-domain QA benchmarks.",
    "output": "[[\"GPT-3.5 Turbo\", \"Llama 2-70B\", \"Mistral-7B-Instruct\", \"Phi-3-mini\"]]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "The output is a nested array instead of a single array of strings as specified.\nERR"
  },
  {
    "id": "05f27f9",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-wb1zmlu65",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We investigate the effect of pretraining objectives on encoder-only transformers for sentence understanding. We fine-tune BERT-Base and BERT-Large (cased) with span masking and compare against RoBERTa-large and ALBERT-xxlarge. Our results indicate that dynamic masking combined with larger batch sizes yields consistent gains, particularly for RoBERTa-large on GLUE tasks.",
    "model": "qwen2.5:3b",
    "input": "Abstract: We investigate the effect of pretraining objectives on encoder-only transformers for sentence understanding. We fine-tune BERT-Base and BERT-Large (cased) with span masking and compare against RoBERTa-large and ALBERT-xxlarge. Our results indicate that dynamic masking combined with larger batch sizes yields consistent gains, particularly for RoBERTa-large on GLUE tasks.",
    "output": "[\"RoBERTa-large\", \"ALBERT-xxlarge\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "The output correctly uses a JSON array of model-name strings with no extra text.  \nOK"
  },
  {
    "id": "1eee10e",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-4xpzfcy7p",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: Efficient convolutional networks remain strong baselines for mobile applications. We revisit ResNet (He et al., 2016) with modern training tricks and compare to DenseNet-121, MobileNetV3-Large, and EfficientNet-B3 on ImageNet and ImageNet-ReaL. A latency-aware evaluation shows MobileNetV3-Large provides the best accuracy-throughput trade-off on edge devices.",
    "model": "qwen2.5:3b",
    "input": "Abstract: Efficient convolutional networks remain strong baselines for mobile applications. We revisit ResNet (He et al., 2016) with modern training tricks and compare to DenseNet-121, MobileNetV3-Large, and EfficientNet-B3 on ImageNet and ImageNet-ReaL. A latency-aware evaluation shows MobileNetV3-Large provides the best accuracy-throughput trade-off on edge devices.",
    "output": "[\"MobileNetV3-Large\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Output is a JSON array of model names as specified.\nOK"
  },
  {
    "id": "18e8ffd",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-h285zdo1g",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We present a comprehensive study of annotation artifacts in benchmark datasets spanning visual question answering and reading comprehension. While we evaluate various training regimes and metrics, we intentionally refrain from introducing a new model and instead focus on dataset design, protocol standardization, and error taxonomies for robust evaluation across COCO and SQuAD.",
    "model": "qwen2.5:3b",
    "input": "Abstract: We present a comprehensive study of annotation artifacts in benchmark datasets spanning visual question answering and reading comprehension. While we evaluate various training regimes and metrics, we intentionally refrain from introducing a new model and instead focus on dataset design, protocol standardization, and error taxonomies for robust evaluation across COCO and SQuAD.",
    "output": "[\"NA\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Formatted correctly as an array with [\"NA\"] per instructions.\nOK"
  },
  {
    "id": "821ec90",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-zmsn89fp9",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: Long-context modeling remains challenging for autoregressive systems. We compare Transformer-XL and XLNet as baselines for language modeling with extended context windows, and adapt sequence-to-sequence pretraining with T5, ByT5, and mT5 for long-form question answering. Our approach combines relative positional encodings with memory caching to improve coherence over thousands of tokens.",
    "model": "qwen2.5:3b",
    "input": "Abstract: Long-context modeling remains challenging for autoregressive systems. We compare Transformer-XL and XLNet as baselines for language modeling with extended context windows, and adapt sequence-to-sequence pretraining with T5, ByT5, and mT5 for long-form question answering. Our approach combines relative positional encodings with memory caching to improve coherence over thousands of tokens.",
    "output": "[[\"Transformer-XL\", \"XLNet\", \"T5\", \"ByT5\", \"mT5\"]]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Output uses a nested array instead of the required flat array format.\nERR"
  },
  {
    "id": "84e9acc",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-lwygnzf5y",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We benchmark object detection frameworks for real-time applications. We evaluate YOLOv5 under varying input resolutions and compare it with two-stage detectors such as Faster R-CNN and Mask R-CNN, as well as transformer-based DETR and keypoint-based CenterNet. Our unified training recipe reveals that DETR benefits substantially from longer training schedules.",
    "model": "qwen2.5:3b",
    "input": "Abstract: We benchmark object detection frameworks for real-time applications. We evaluate YOLOv5 under varying input resolutions and compare it with two-stage detectors such as Faster R-CNN and Mask R-CNN, as well as transformer-based DETR and keypoint-based CenterNet. Our unified training recipe reveals that DETR benefits substantially from longer training schedules.",
    "output": "[\"YOLOv5\", \"Faster R-CNN\", \"Mask R-CNN\", \"DETR\", \"CenterNet\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Properly formatted array of model names.\nOK"
  },
  {
    "id": "4b61397",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-qhcz4i16e",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: Semantic segmentation in medical imaging requires precise boundary delineation. We propose a hybrid decoder that augments U-Net with multi-scale attention, and we compare against DeepLabv3+ and PSPNet using Dice and Hausdorff metrics. On two public MRI datasets, our approach yields consistent improvements without increasing the parameter count relative to U-Net.",
    "model": "qwen2.5:3b",
    "input": "Abstract: Semantic segmentation in medical imaging requires precise boundary delineation. We propose a hybrid decoder that augments U-Net with multi-scale attention, and we compare against DeepLabv3+ and PSPNet using Dice and Hausdorff metrics. On two public MRI datasets, our approach yields consistent improvements without increasing the parameter count relative to U-Net.",
    "output": "[\"DeepLabv3+\", \"PSPNet\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Array format with only model names is correct.  \nOK"
  },
  {
    "id": "232fc26",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-x969paum6",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We examine structured prediction models for sequence labeling in noisy domains. Baselines include linear-chain CRF, HMM, and a neural BiLSTM-CRF. We further compare with Naive Bayes for token classification under distant supervision. Our experiments suggest that CRF-based models exhibit superior robustness to label noise when augmented with posterior regularization.",
    "model": "qwen2.5:3b",
    "input": "Abstract: We examine structured prediction models for sequence labeling in noisy domains. Baselines include linear-chain CRF, HMM, and a neural BiLSTM-CRF. We further compare with Naive Bayes for token classification under distant supervision. Our experiments suggest that CRF-based models exhibit superior robustness to label noise when augmented with posterior regularization.",
    "output": "[\\\"CRF\\\", \\\"HMM\\\", \\\"BiLSTM-CRF\\\", \\\"Naive Bayes\\\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Provided output is a JSON array of model names, matching the required format.\nOK"
  },
  {
    "id": "d50c7ac",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-p6quzscah",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: Single image super-resolution has benefited from adversarial training, yet stability remains an issue. We revisit ESRGAN and Real-ESRGAN and propose a perceptual discriminator that reduces over-sharpening artifacts. Comparisons to SRGAN, EDSR, and SwinIR on DIV2K and RealSR indicate that Real-ESRGAN with our modification provides a better fidelity-perception trade-off.",
    "model": "qwen2.5:3b",
    "input": "Abstract: Single image super-resolution has benefited from adversarial training, yet stability remains an issue. We revisit ESRGAN and Real-ESRGAN and propose a perceptual discriminator that reduces over-sharpening artifacts. Comparisons to SRGAN, EDSR, and SwinIR on DIV2K and RealSR indicate that Real-ESRGAN with our modification provides a better fidelity-perception trade-off.",
    "output": "[\"ESRGAN\", \"Real-ESRGAN\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Output is formatted correctly as array of model names.\nOK"
  },
  {
    "id": "8c3b0ef",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-lwppn0yv0",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We present a unified framework for text-to-speech and speech recognition. For generation, we train Tacotron 2 and compare with the autoregressive WaveNet vocoder and neural source-filter baselines. For recognition, we fine-tune wav2vec 2.0 and HuBERT, and include DeepSpeech as a non-self-supervised baseline. Cross-task transfer improves both ASR WER and TTS MOS.",
    "model": "qwen2.5:3b",
    "input": "Abstract: We present a unified framework for text-to-speech and speech recognition. For generation, we train Tacotron 2 and compare with the autoregressive WaveNet vocoder and neural source-filter baselines. For recognition, we fine-tune wav2vec 2.0 and HuBERT, and include DeepSpeech as a non-self-supervised baseline. Cross-task transfer improves both ASR WER and TTS MOS.",
    "output": "[\"Tacotron 2\", \"WaveNet\", \"wav2vec 2.0\", \"HuBERT\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Proper JSON array of model names as strings.\nOK"
  },
  {
    "id": "1bf2509",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-btsjags2r",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We study planning and control with model-based and model-free reinforcement learning. Our evaluation spans AlphaZero and MuZero on board games, as well as off-policy DQN and on-policy PPO on Atari and MuJoCo. We propose a hybrid value-consistency loss that stabilizes MuZeros training without degrading sample efficiency.",
    "model": "qwen2.5:3b",
    "input": "Abstract: We study planning and control with model-based and model-free reinforcement learning. Our evaluation spans AlphaZero and MuZero on board games, as well as off-policy DQN and on-policy PPO on Atari and MuJoCo. We propose a hybrid value-consistency loss that stabilizes MuZeros training without degrading sample efficiency.",
    "output": "[\\\"AlphaZero\\\", \\\"MuZero\\\", \\\"DQN\\\", \\\"PPO\\\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Proper array of model names with correct format.\nOK"
  },
  {
    "id": "09b0a23",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-eqilegh9y",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We investigate multilingual machine translation under constrained compute. We compare MarianMT fine-tuned per language pair against massively multilingual models mBART50 and M2M-100, and evaluate the scaling behavior of NLLB-200 in low-resource regimes. Our curriculum sampling strategy yields improved BLEU in zero-shot directions without increasing model capacity.",
    "model": "qwen2.5:3b",
    "input": "Abstract: We investigate multilingual machine translation under constrained compute. We compare MarianMT fine-tuned per language pair against massively multilingual models mBART50 and M2M-100, and evaluate the scaling behavior of NLLB-200 in low-resource regimes. Our curriculum sampling strategy yields improved BLEU in zero-shot directions without increasing model capacity.",
    "output": "[\"MarianMT\", \"mBART50\", \"M2M-100\", \"NLLB-200\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Array is correctly formatted with quoted model names.\nOK"
  },
  {
    "id": "d7ccbb3",
    "promptid": "1aa450e",
    "ruleid": 1,
    "rule": "The entire response must be a single JSON array literal and must contain no characters, words, or formatting outside the array.",
    "inverse": false,
    "testuid": "test-64071835",
    "scenario": "",
    "baseline": false,
    "testinput": "We present FoobarNet, a lightweight convolutional architecture for mobile vision tasks. Code and preprint available.",
    "model": "gpt-oss",
    "input": "We present FoobarNet, a lightweight convolutional architecture for mobile vision tasks. Code and preprint available.",
    "output": "[\"FoobarNet\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Output is correctly formatted as an array of model names.\nOK"
  },
  {
    "id": "24720e9",
    "promptid": "1aa450e",
    "ruleid": 2,
    "rule": "A JSON array is defined as a sequence of elements enclosed by a leading \"[\" and a trailing \"]\" with elements separated by commas according to the JSON specification.",
    "inverse": false,
    "testuid": "test-74896303",
    "scenario": "",
    "baseline": false,
    "testinput": "We introduce AlphaNet and BetaNet for multi-task learning; both models outperform baselines on four benchmarks.",
    "model": "gpt-oss",
    "input": "We introduce AlphaNet and BetaNet for multi-task learning; both models outperform baselines on four benchmarks.",
    "output": "[\"AlphaNet\", \"BetaNet\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Output is a simple array with double-quoted model names as required.\nOK"
  },
  {
    "id": "1aa450e",
    "promptid": "1aa450e",
    "ruleid": 3,
    "rule": "The array must not be empty and must contain at least one element.",
    "inverse": false,
    "testuid": "test-79925387",
    "scenario": "",
    "baseline": false,
    "testinput": "",
    "model": "gpt-oss",
    "input": "",
    "output": "[\"NA\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Formatted correctly as an array with [\"NA\"] per instructions.\nOK"
  },
  {
    "id": "fbc8480",
    "promptid": "1aa450e",
    "ruleid": 4,
    "rule": "The response must not be a JSON array literal and should include extra commentary or markup outside any brackets, ideally prefixed by a Unicode BOM.",
    "inverse": true,
    "testuid": "test-59317763",
    "scenario": "",
    "baseline": false,
    "testinput": "We propose mBERT-Adapt, adapting multilingual BERT for cross-lingual transfer.",
    "model": "gpt-oss",
    "input": "We propose mBERT-Adapt, adapting multilingual BERT for cross-lingual transfer.",
    "output": "[\"mBERT-Adapt\", \"multilingual BERT\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "The output is a properly formatted array of model names as specified.  \nOK"
  },
  {
    "id": "e0bbea8",
    "promptid": "1aa450e",
    "ruleid": 15,
    "testuid": "test-49747978",
    "scenario": "",
    "baseline": false,
    "testinput": "We introduce GraphFormer, a Transformer-based graph learning model achieving state-of-the-art on OGB benchmarks. The architecture scales efficiently.",
    "model": "gpt-oss",
    "input": "We introduce GraphFormer, a Transformer-based graph learning model achieving state-of-the-art on OGB benchmarks. The architecture scales efficiently.",
    "output": "[\"GraphFormer\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Valid JSON array with a model name as specified.\nOK"
  },
  {
    "id": "264aead",
    "promptid": "1aa450e",
    "ruleid": 16,
    "testuid": "test-67868223",
    "scenario": "",
    "baseline": false,
    "testinput": "We compare GraphSAGE and GIN for node classification across citation networks, detailing inductive biases and training setups.",
    "model": "gpt-oss",
    "input": "We compare GraphSAGE and GIN for node classification across citation networks, detailing inductive biases and training setups.",
    "output": "[\"GraphSAGE\", \"GIN\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Output is properly formatted as an array of model names.\nOK"
  },
  {
    "id": "040ac3b",
    "promptid": "1aa450e",
    "ruleid": 17,
    "testuid": "test-62684684",
    "scenario": "",
    "baseline": false,
    "testinput": "We propose ResNet-50-TR and compare against BERT and GPT-2; models are described in detail with comprehensive ablations.",
    "model": "gpt-oss",
    "input": "We propose ResNet-50-TR and compare against BERT and GPT-2; models are described in detail with comprehensive ablations.",
    "output": "[\"ResNet-50-TR\", \"BERT\", \"GPT-2\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Output uses array of strings in required format with double quotes and brackets, hence complies the description.\nOK"
  },
  {
    "id": "9634b39",
    "promptid": "1aa450e",
    "ruleid": 18,
    "testuid": "test-89746605",
    "scenario": "",
    "baseline": false,
    "testinput": "We release a variant called DeepFusionNet that excels on multimodal tasks such as VQA and captioning.",
    "model": "gpt-oss",
    "input": "We release a variant called DeepFusionNet that excels on multimodal tasks such as VQA and captioning.",
    "output": "[\"DeepFusionNet\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Correct format as an array of a single model name.\nOK"
  },
  {
    "id": "b722272",
    "promptid": "1aa450e",
    "ruleid": 19,
    "testuid": "test-10313432",
    "scenario": "",
    "baseline": false,
    "testinput": "We present SequenceLab v1, a simple sequence labeling toolkit with CRFs and transformers backends.",
    "model": "gpt-oss",
    "input": "We present SequenceLab v1, a simple sequence labeling toolkit with CRFs and transformers backends.",
    "output": "[\"CRFs\", \"transformers\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Valid JSON array of strings, matches required format for model name extraction.  \nOK"
  },
  {
    "id": "202f432",
    "promptid": "1aa450e",
    "ruleid": 20,
    "testuid": "test-24887847",
    "scenario": "",
    "baseline": false,
    "testinput": "This survey synthesizes trends in self-supervised learning for audio and vision; no new model is introduced or named.",
    "model": "gpt-oss",
    "input": "This survey synthesizes trends in self-supervised learning for audio and vision; no new model is introduced or named.",
    "output": "[\\\"NA\\\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "No model names extracted, using correct fallback format.\nOK"
  },
  {
    "id": "e7f03c6",
    "promptid": "1aa450e",
    "ruleid": 21,
    "testuid": "test-02256959",
    "scenario": "",
    "baseline": false,
    "testinput": "We conduct controlled ablations on architecture depth and width without proposing a new model; baselines are standard.",
    "model": "gpt-oss",
    "input": "We conduct controlled ablations on architecture depth and width without proposing a new model; baselines are standard.",
    "output": "[\"NA\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Formatted correctly as an array with [\"NA\"] per instructions.\nOK"
  },
  {
    "id": "1aa450e",
    "promptid": "1aa450e",
    "ruleid": 22,
    "testuid": "test-32845896",
    "scenario": "",
    "baseline": false,
    "testinput": "",
    "model": "gpt-oss",
    "input": "",
    "output": "[\"NA\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Formatted correctly as an array with [\"NA\"] per instructions.\nOK"
  },
  {
    "id": "857efa5",
    "promptid": "1aa450e",
    "ruleid": 23,
    "testuid": "test-60185492",
    "scenario": "",
    "baseline": false,
    "testinput": "No specific model is proposed; methods are generic and naming is intentionally omitted throughout.",
    "model": "gpt-oss",
    "input": "No specific model is proposed; methods are generic and naming is intentionally omitted throughout.",
    "output": "[\"NA\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Formatted correctly as an array with [\"NA\"] per instructions.\nOK"
  },
  {
    "id": "3710ffa",
    "promptid": "1aa450e",
    "ruleid": 24,
    "testuid": "test-87787525",
    "scenario": "",
    "baseline": false,
    "testinput": "We evaluate BERT against XLNet on GLUE and SQuAD; our contribution does not introduce a new model.",
    "model": "gpt-oss",
    "input": "We evaluate BERT against XLNet on GLUE and SQuAD; our contribution does not introduce a new model.",
    "output": "[\"BERT\", \"XLNet\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Output uses required array-of-strings format without extra text.  \nOK"
  },
  {
    "id": "4db7bdc",
    "promptid": "1aa450e",
    "ruleid": 25,
    "testuid": "test-72857613",
    "scenario": "",
    "baseline": false,
    "testinput": "We introduce LiteViT, a lightweight Vision Transformer optimized for mobile hardware with distillation and quantization-aware training.",
    "model": "gpt-oss",
    "input": "We introduce LiteViT, a lightweight Vision Transformer optimized for mobile hardware with distillation and quantization-aware training.",
    "output": "[\"LiteViT\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Valid array format with model names.  \nOK"
  },
  {
    "id": "ddc8eec",
    "promptid": "1aa450e",
    "ruleid": 26,
    "testuid": "test-73993212",
    "scenario": "",
    "baseline": false,
    "testinput": "We fine-tune T5 and BART for summarization, detailing datasets, metrics, and training schedules.",
    "model": "gpt-oss",
    "input": "We fine-tune T5 and BART for summarization, detailing datasets, metrics, and training schedules.",
    "output": "[\"T5\", \"BART\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Output uses correct [model_name] array format, only strings present.\nOK"
  },
  {
    "id": "9b5cd34",
    "promptid": "1aa450e",
    "ruleid": 27,
    "testuid": "test-66058669",
    "scenario": "",
    "baseline": false,
    "testinput": "We present RoBERTa-XS, a compact pre-trained encoder tailored for latency-critical NLU on-device applications.",
    "model": "gpt-oss",
    "input": "We present RoBERTa-XS, a compact pre-trained encoder tailored for latency-critical NLU on-device applications.",
    "output": "[\"RoBERTa-XS\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Format correct: array with quoted model name.  \nOK"
  },
  {
    "id": "e1bdda1",
    "promptid": "1aa450e",
    "ruleid": 28,
    "testuid": "test-24288487",
    "scenario": "",
    "baseline": false,
    "testinput": "We introduce three modelsBERT, GPT-2, and RoBERTaand show complementary strengths across GLUE, SuperGLUE, and WMT tasks.",
    "model": "gpt-oss",
    "input": "We introduce three modelsBERT, GPT-2, and RoBERTaand show complementary strengths across GLUE, SuperGLUE, and WMT tasks.",
    "output": "[\"BERT\", \"GPT-2\", \"RoBERTa\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Valid JSON array of model names in required format.\nOK"
  },
  {
    "id": "76fc8bd",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-27n98w97g",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We evaluate deep convolutional architectures for large-scale image classification. Our proposed approach builds on ResNet-50 and improves training stability via stochastic depth, yielding strong performance on ImageNet. We further compare against VGG-16 and Inception-v3 baselines on CIFAR-10 and report consistent gains. A detailed ablation isolates the contribution of the residual blocks and squeeze-and-excitation modules when integrated into ResNet-50.",
    "model": "gpt-oss",
    "input": "Abstract: We evaluate deep convolutional architectures for large-scale image classification. Our proposed approach builds on ResNet-50 and improves training stability via stochastic depth, yielding strong performance on ImageNet. We further compare against VGG-16 and Inception-v3 baselines on CIFAR-10 and report consistent gains. A detailed ablation isolates the contribution of the residual blocks and squeeze-and-excitation modules when integrated into ResNet-50.",
    "output": "[\"ResNet-50\",\"VGG-16\",\"Inception-v3\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Array of string model names in correct format.  \nOK"
  },
  {
    "id": "f207da8",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-pfykm77fb",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We introduce GraphFormer, a transformer-style architecture for learning on graphs that unifies message passing and attention. On citation networks and molecular property prediction benchmarks, GraphFormer outperforms GraphSAGE, GAT, and GCN under identical training budgets. We analyze scaling behavior and show that GraphFormer benefits from increased depth without oversmoothing.",
    "model": "gpt-oss",
    "input": "Abstract: We introduce GraphFormer, a transformer-style architecture for learning on graphs that unifies message passing and attention. On citation networks and molecular property prediction benchmarks, GraphFormer outperforms GraphSAGE, GAT, and GCN under identical training budgets. We analyze scaling behavior and show that GraphFormer benefits from increased depth without oversmoothing.",
    "output": "[\"GraphFormer\",\"GraphSAGE\",\"GAT\",\"GCN\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Proper JSON array of model names matching the required format.  \nOK"
  },
  {
    "id": "1965da3",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-kdk5hz4op",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: Deep learning has achieved remarkable progress across perception and language tasks. In this study, we investigate training dynamics under different data curricula and optimization strategies, but we do not introduce or evaluate any specific named models. Our analysis focuses on generalizable phenomena across architectures, offering insights into representation learning without committing to a particular design.",
    "model": "gpt-oss",
    "input": "Abstract: Deep learning has achieved remarkable progress across perception and language tasks. In this study, we investigate training dynamics under different data curricula and optimization strategies, but we do not introduce or evaluate any specific named models. Our analysis focuses on generalizable phenomena across architectures, offering insights into representation learning without committing to a particular design.",
    "output": "[\"NA\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Formatted correctly as an array with [\"NA\"] per instructions.\nOK"
  },
  {
    "id": "1981336",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-la8rzana9",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: Multimodal vision-language learning benefits from large pretraining corpora. We fine-tune CLIP and ViT-L/14 encoders for zero-shot retrieval and evaluate cross-attention decoders initialized from BLIP-2. For text-to-image synthesis, we benchmark adapters built on top of DALL路E 2 and analyze the trade-off between fidelity and diversity on MS-COCO. Our findings highlight the importance of contrastive alignment in CLIP for downstream tasks.",
    "model": "gpt-oss",
    "input": "Abstract: Multimodal vision-language learning benefits from large pretraining corpora. We fine-tune CLIP and ViT-L/14 encoders for zero-shot retrieval and evaluate cross-attention decoders initialized from BLIP-2. For text-to-image synthesis, we benchmark adapters built on top of DALL路E 2 and analyze the trade-off between fidelity and diversity on MS-COCO. Our findings highlight the importance of contrastive alignment in CLIP for downstream tasks.",
    "output": "[\"CLIP\", \"ViT-L/14\", \"BLIP-2\", \"DALL路E 2\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Output is a JSON array of strings matching the required format.\nOK"
  },
  {
    "id": "121fca8",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-1l1063mhk",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We study distributional word representations for low-resource NLP. We compare classical embeddings like word2vec, fastText, and GloVe with contextual representations from ELMo under limited supervision. Our results show that subword modeling in fastText provides robust performance in morphologically rich languages, while ELMo remains competitive when fine-tuned.",
    "model": "gpt-oss",
    "input": "Abstract: We study distributional word representations for low-resource NLP. We compare classical embeddings like word2vec, fastText, and GloVe with contextual representations from ELMo under limited supervision. Our results show that subword modeling in fastText provides robust performance in morphologically rich languages, while ELMo remains competitive when fine-tuned.",
    "output": "[\"word2vec\",\"fastText\",\"GloVe\",\"ELMo\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Properly formatted as an array of model names, no extra content.\nOK"
  },
  {
    "id": "28a7a21",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-tb1rgdeno",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: This paper benchmarks traditional machine learning methods for tabular classification across 45 datasets. We include SVM with RBF kernels, k-NN, Random Forest, XGBoost, and LightGBM, carefully tuning hyperparameters via cross-validation. We report that boosted tree models such as XGBoost and LightGBM consistently dominate on medium-sized tabular data, while SVM performs well in high-dimensional sparse settings.",
    "model": "gpt-oss",
    "input": "Abstract: This paper benchmarks traditional machine learning methods for tabular classification across 45 datasets. We include SVM with RBF kernels, k-NN, Random Forest, XGBoost, and LightGBM, carefully tuning hyperparameters via cross-validation. We report that boosted tree models such as XGBoost and LightGBM consistently dominate on medium-sized tabular data, while SVM performs well in high-dimensional sparse settings.",
    "output": "[\"SVM\", \"k-NN\", \"Random Forest\", \"XGBoost\", \"LightGBM\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Output is an array of strings in the required format.\nOK"
  },
  {
    "id": "2ea75e5",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-zk3zt09y2",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We systematically evaluate instruction-following large language models on compositional reasoning. Models considered include GPT-3.5 Turbo, Llama 2-70B, Mistral-7B-Instruct, and Phi-3-mini. We propose a calibration technique that reduces hallucinations without sacrificing fluency, and demonstrate improved factuality across open-domain QA benchmarks.",
    "model": "gpt-oss",
    "input": "Abstract: We systematically evaluate instruction-following large language models on compositional reasoning. Models considered include GPT-3.5 Turbo, Llama 2-70B, Mistral-7B-Instruct, and Phi-3-mini. We propose a calibration technique that reduces hallucinations without sacrificing fluency, and demonstrate improved factuality across open-domain QA benchmarks.",
    "output": "[\"GPT-3.5 Turbo\",\"Llama 2-70B\",\"Mistral-7B-Instruct\",\"Phi-3-mini\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "The output is a JSON array of model names formatted correctly per the description.  \nOK"
  },
  {
    "id": "05f27f9",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-wb1zmlu65",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We investigate the effect of pretraining objectives on encoder-only transformers for sentence understanding. We fine-tune BERT-Base and BERT-Large (cased) with span masking and compare against RoBERTa-large and ALBERT-xxlarge. Our results indicate that dynamic masking combined with larger batch sizes yields consistent gains, particularly for RoBERTa-large on GLUE tasks.",
    "model": "gpt-oss",
    "input": "Abstract: We investigate the effect of pretraining objectives on encoder-only transformers for sentence understanding. We fine-tune BERT-Base and BERT-Large (cased) with span masking and compare against RoBERTa-large and ALBERT-xxlarge. Our results indicate that dynamic masking combined with larger batch sizes yields consistent gains, particularly for RoBERTa-large on GLUE tasks.",
    "output": "[\"BERT-Base\", \"BERT-Large\", \"RoBERTa-large\", \"ALBERT-xxlarge\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Produced array of model names in correct format.  \nOK"
  },
  {
    "id": "1eee10e",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-4xpzfcy7p",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: Efficient convolutional networks remain strong baselines for mobile applications. We revisit ResNet (He et al., 2016) with modern training tricks and compare to DenseNet-121, MobileNetV3-Large, and EfficientNet-B3 on ImageNet and ImageNet-ReaL. A latency-aware evaluation shows MobileNetV3-Large provides the best accuracy-throughput trade-off on edge devices.",
    "model": "gpt-oss",
    "input": "Abstract: Efficient convolutional networks remain strong baselines for mobile applications. We revisit ResNet (He et al., 2016) with modern training tricks and compare to DenseNet-121, MobileNetV3-Large, and EfficientNet-B3 on ImageNet and ImageNet-ReaL. A latency-aware evaluation shows MobileNetV3-Large provides the best accuracy-throughput trade-off on edge devices.",
    "output": "[\"ResNet\",\"DenseNet-121\",\"MobileNetV3-Large\",\"EfficientNet-B3\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Output is correctly formatted as an array of model names.  \nOK"
  },
  {
    "id": "18e8ffd",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-h285zdo1g",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We present a comprehensive study of annotation artifacts in benchmark datasets spanning visual question answering and reading comprehension. While we evaluate various training regimes and metrics, we intentionally refrain from introducing a new model and instead focus on dataset design, protocol standardization, and error taxonomies for robust evaluation across COCO and SQuAD.",
    "model": "gpt-oss",
    "input": "Abstract: We present a comprehensive study of annotation artifacts in benchmark datasets spanning visual question answering and reading comprehension. While we evaluate various training regimes and metrics, we intentionally refrain from introducing a new model and instead focus on dataset design, protocol standardization, and error taxonomies for robust evaluation across COCO and SQuAD.",
    "output": "[\"NA\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Formatted correctly as an array with [\"NA\"] per instructions.\nOK"
  },
  {
    "id": "821ec90",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-zmsn89fp9",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: Long-context modeling remains challenging for autoregressive systems. We compare Transformer-XL and XLNet as baselines for language modeling with extended context windows, and adapt sequence-to-sequence pretraining with T5, ByT5, and mT5 for long-form question answering. Our approach combines relative positional encodings with memory caching to improve coherence over thousands of tokens.",
    "model": "gpt-oss",
    "input": "Abstract: Long-context modeling remains challenging for autoregressive systems. We compare Transformer-XL and XLNet as baselines for language modeling with extended context windows, and adapt sequence-to-sequence pretraining with T5, ByT5, and mT5 for long-form question answering. Our approach combines relative positional encodings with memory caching to improve coherence over thousands of tokens.",
    "output": "[\"Transformer-XL\",\"XLNet\",\"T5\",\"ByT5\",\"mT5\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Output is a JSON array of strings as required, with no extra text.\nOK"
  },
  {
    "id": "84e9acc",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-lwygnzf5y",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We benchmark object detection frameworks for real-time applications. We evaluate YOLOv5 under varying input resolutions and compare it with two-stage detectors such as Faster R-CNN and Mask R-CNN, as well as transformer-based DETR and keypoint-based CenterNet. Our unified training recipe reveals that DETR benefits substantially from longer training schedules.",
    "model": "gpt-oss",
    "input": "Abstract: We benchmark object detection frameworks for real-time applications. We evaluate YOLOv5 under varying input resolutions and compare it with two-stage detectors such as Faster R-CNN and Mask R-CNN, as well as transformer-based DETR and keypoint-based CenterNet. Our unified training recipe reveals that DETR benefits substantially from longer training schedules.",
    "output": "[\"YOLOv5\", \"Faster R-CNN\", \"Mask R-CNN\", \"DETR\", \"CenterNet\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Properly formatted array of model names.\nOK"
  },
  {
    "id": "4b61397",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-qhcz4i16e",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: Semantic segmentation in medical imaging requires precise boundary delineation. We propose a hybrid decoder that augments U-Net with multi-scale attention, and we compare against DeepLabv3+ and PSPNet using Dice and Hausdorff metrics. On two public MRI datasets, our approach yields consistent improvements without increasing the parameter count relative to U-Net.",
    "model": "gpt-oss",
    "input": "Abstract: Semantic segmentation in medical imaging requires precise boundary delineation. We propose a hybrid decoder that augments U-Net with multi-scale attention, and we compare against DeepLabv3+ and PSPNet using Dice and Hausdorff metrics. On two public MRI datasets, our approach yields consistent improvements without increasing the parameter count relative to U-Net.",
    "output": "[\"U-Net\", \"DeepLabv3+\", \"PSPNet\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Correct array format with only model names as strings.  \nOK"
  },
  {
    "id": "232fc26",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-x969paum6",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We examine structured prediction models for sequence labeling in noisy domains. Baselines include linear-chain CRF, HMM, and a neural BiLSTM-CRF. We further compare with Naive Bayes for token classification under distant supervision. Our experiments suggest that CRF-based models exhibit superior robustness to label noise when augmented with posterior regularization.",
    "model": "gpt-oss",
    "input": "Abstract: We examine structured prediction models for sequence labeling in noisy domains. Baselines include linear-chain CRF, HMM, and a neural BiLSTM-CRF. We further compare with Naive Bayes for token classification under distant supervision. Our experiments suggest that CRF-based models exhibit superior robustness to label noise when augmented with posterior regularization.",
    "output": "[\"linear-chain CRF\", \"HMM\", \"BiLSTM-CRF\", \"Naive Bayes\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Output is a JSON array of model names as required.\nOK"
  },
  {
    "id": "d50c7ac",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-p6quzscah",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: Single image super-resolution has benefited from adversarial training, yet stability remains an issue. We revisit ESRGAN and Real-ESRGAN and propose a perceptual discriminator that reduces over-sharpening artifacts. Comparisons to SRGAN, EDSR, and SwinIR on DIV2K and RealSR indicate that Real-ESRGAN with our modification provides a better fidelity-perception trade-off.",
    "model": "gpt-oss",
    "input": "Abstract: Single image super-resolution has benefited from adversarial training, yet stability remains an issue. We revisit ESRGAN and Real-ESRGAN and propose a perceptual discriminator that reduces over-sharpening artifacts. Comparisons to SRGAN, EDSR, and SwinIR on DIV2K and RealSR indicate that Real-ESRGAN with our modification provides a better fidelity-perception trade-off.",
    "output": "[\"ESRGAN\",\"Real-ESRGAN\",\"SRGAN\",\"EDSR\",\"SwinIR\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Correct JSON array format with model names as strings; no extra commentary.\nOK"
  },
  {
    "id": "8c3b0ef",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-lwppn0yv0",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We present a unified framework for text-to-speech and speech recognition. For generation, we train Tacotron 2 and compare with the autoregressive WaveNet vocoder and neural source-filter baselines. For recognition, we fine-tune wav2vec 2.0 and HuBERT, and include DeepSpeech as a non-self-supervised baseline. Cross-task transfer improves both ASR WER and TTS MOS.",
    "model": "gpt-oss",
    "input": "Abstract: We present a unified framework for text-to-speech and speech recognition. For generation, we train Tacotron 2 and compare with the autoregressive WaveNet vocoder and neural source-filter baselines. For recognition, we fine-tune wav2vec 2.0 and HuBERT, and include DeepSpeech as a non-self-supervised baseline. Cross-task transfer improves both ASR WER and TTS MOS.",
    "output": "[\"Tacotron 2\", \"WaveNet\", \"wav2vec 2.0\", \"HuBERT\", \"DeepSpeech\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "The output is a JSON array of model names formatted correctly as specified.\nOK"
  },
  {
    "id": "1bf2509",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-btsjags2r",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We study planning and control with model-based and model-free reinforcement learning. Our evaluation spans AlphaZero and MuZero on board games, as well as off-policy DQN and on-policy PPO on Atari and MuJoCo. We propose a hybrid value-consistency loss that stabilizes MuZeros training without degrading sample efficiency.",
    "model": "gpt-oss",
    "input": "Abstract: We study planning and control with model-based and model-free reinforcement learning. Our evaluation spans AlphaZero and MuZero on board games, as well as off-policy DQN and on-policy PPO on Atari and MuJoCo. We propose a hybrid value-consistency loss that stabilizes MuZeros training without degrading sample efficiency.",
    "output": "[\"AlphaZero\", \"MuZero\", \"DQN\", \"PPO\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Output is a JSON array of model names in required format  \nOK"
  },
  {
    "id": "09b0a23",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-eqilegh9y",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We investigate multilingual machine translation under constrained compute. We compare MarianMT fine-tuned per language pair against massively multilingual models mBART50 and M2M-100, and evaluate the scaling behavior of NLLB-200 in low-resource regimes. Our curriculum sampling strategy yields improved BLEU in zero-shot directions without increasing model capacity.",
    "model": "gpt-oss",
    "input": "Abstract: We investigate multilingual machine translation under constrained compute. We compare MarianMT fine-tuned per language pair against massively multilingual models mBART50 and M2M-100, and evaluate the scaling behavior of NLLB-200 in low-resource regimes. Our curriculum sampling strategy yields improved BLEU in zero-shot directions without increasing model capacity.",
    "output": "[\"MarianMT\",\"mBART50\",\"M2M-100\",\"NLLB-200\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Valid formatted array of model names.\nOK"
  },
  {
    "id": "d7ccbb3",
    "promptid": "1aa450e",
    "ruleid": 1,
    "rule": "The entire response must be a single JSON array literal and must contain no characters, words, or formatting outside the array.",
    "inverse": false,
    "testuid": "test-64071835",
    "scenario": "",
    "baseline": false,
    "testinput": "We present FoobarNet, a lightweight convolutional architecture for mobile vision tasks. Code and preprint available.",
    "model": "llama3.2:1b",
    "input": "We present FoobarNet, a lightweight convolutional architecture for mobile vision tasks. Code and preprint available.",
    "output": "Since there is no explicit mention of the name \"FoobarNet\" in the provided text, I will return \"[NA]\" as per your instructions.\n\nModel Names:\n\n[NA]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Added explanation and incorrect format; should only output [\"NA\"].\nERR"
  },
  {
    "id": "24720e9",
    "promptid": "1aa450e",
    "ruleid": 2,
    "rule": "A JSON array is defined as a sequence of elements enclosed by a leading \"[\" and a trailing \"]\" with elements separated by commas according to the JSON specification.",
    "inverse": false,
    "testuid": "test-74896303",
    "scenario": "",
    "baseline": false,
    "testinput": "We introduce AlphaNet and BetaNet for multi-task learning; both models outperform baselines on four benchmarks.",
    "model": "llama3.2:1b",
    "input": "We introduce AlphaNet and BetaNet for multi-task learning; both models outperform baselines on four benchmarks.",
    "output": "Possible results:\n\n['AlphaNet', 'BetaNet']",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Extraneous prefix and uses single quotes instead of the required JSON-style double quotes.  \nERR"
  },
  {
    "id": "1aa450e",
    "promptid": "1aa450e",
    "ruleid": 3,
    "rule": "The array must not be empty and must contain at least one element.",
    "inverse": false,
    "testuid": "test-79925387",
    "scenario": "",
    "baseline": false,
    "testinput": "",
    "model": "llama3.2:1b",
    "input": "",
    "output": "Please provide the abstract of the machine learning paper. I will then extract the model names and respond accordingly.\n\n(Also, just to confirm, I assume that there is no specific information about NA in the context of model extraction from abstracts.)",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Output doesn't provide an array of model names; asks for abstract instead.\nERR"
  },
  {
    "id": "fbc8480",
    "promptid": "1aa450e",
    "ruleid": 4,
    "rule": "The response must not be a JSON array literal and should include extra commentary or markup outside any brackets, ideally prefixed by a Unicode BOM.",
    "inverse": true,
    "testuid": "test-59317763",
    "scenario": "",
    "baseline": false,
    "testinput": "We propose mBERT-Adapt, adapting multilingual BERT for cross-lingual transfer.",
    "model": "llama3.2:1b",
    "input": "We propose mBERT-Adapt, adapting multilingual BERT for cross-lingual transfer.",
    "output": "Here is the extracted information in the format of an array:\n\n[\"mBERT\"]\n \nOr,\n[ \"mBERT\", \"NA\" ] (if you are not sure or don't find model names in the abstract)",
    "metrics": {},
    "compliance": "err",
    "complianceText": "The output includes extraneous explanatory text and alternatives rather than providing just the required array of model names.  \nERR"
  },
  {
    "id": "e0bbea8",
    "promptid": "1aa450e",
    "ruleid": 15,
    "testuid": "test-49747978",
    "scenario": "",
    "baseline": false,
    "testinput": "We introduce GraphFormer, a Transformer-based graph learning model achieving state-of-the-art on OGB benchmarks. The architecture scales efficiently.",
    "model": "llama3.2:1b",
    "input": "We introduce GraphFormer, a Transformer-based graph learning model achieving state-of-the-art on OGB benchmarks. The architecture scales efficiently.",
    "output": "[\"Graph Former\"]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Output is correctly formatted as an array of model names per description.\nOK"
  },
  {
    "id": "264aead",
    "promptid": "1aa450e",
    "ruleid": 16,
    "testuid": "test-67868223",
    "scenario": "",
    "baseline": false,
    "testinput": "We compare GraphSAGE and GIN for node classification across citation networks, detailing inductive biases and training setups.",
    "model": "llama3.2:1b",
    "input": "We compare GraphSAGE and GIN for node classification across citation networks, detailing inductive biases and training setups.",
    "output": "The final answer depends on whether a specific model name or models were mentioned in the abstract.\n\nHere is my interpretation of the abstract:\n\n* The abstract mentions \"GraphSAGE\" but does not specify which GraphSAGE model (e.g., GraphSAGE (Vastardet et al., 2017) or GraphSAGE-LSTM (Lerman et al., 2020)).\n* It also mentions \"GIN\" (Gu et al., 2016) but again, the specific model is not mentioned.\n\nTherefore, I will return [\\\"NA\\)].",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Adds explanations and returns malformed [\"NA\") instead of only [\"NA\"].\nERR"
  },
  {
    "id": "040ac3b",
    "promptid": "1aa450e",
    "ruleid": 17,
    "testuid": "test-62684684",
    "scenario": "",
    "baseline": false,
    "testinput": "We propose ResNet-50-TR and compare against BERT and GPT-2; models are described in detail with comprehensive ablations.",
    "model": "llama3.2:1b",
    "input": "We propose ResNet-50-TR and compare against BERT and GPT-2; models are described in detail with comprehensive ablations.",
    "output": "The following array contains the model name, with NA indicating it could not be extracted from the abstract.\n\n\\[ [\"BERT\", \"GPT-2\", \"ResNet-50-TR\"] \\]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "The output includes explanatory text and LaTeX-style delimiters instead of providing just the array of model names as specified.\nERR"
  },
  {
    "id": "9634b39",
    "promptid": "1aa450e",
    "ruleid": 18,
    "testuid": "test-89746605",
    "scenario": "",
    "baseline": false,
    "testinput": "We release a variant called DeepFusionNet that excels on multimodal tasks such as VQA and captioning.",
    "model": "llama3.2:1b",
    "input": "We release a variant called DeepFusionNet that excels on multimodal tasks such as VQA and captioning.",
    "output": "Here is the extracted model name [\"DeepFusionNet\"]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Extra explanatory text is present before the required array format.\nERR"
  },
  {
    "id": "b722272",
    "promptid": "1aa450e",
    "ruleid": 19,
    "testuid": "test-10313432",
    "scenario": "",
    "baseline": false,
    "testinput": "We present SequenceLab v1, a simple sequence labeling toolkit with CRFs and transformers backends.",
    "model": "llama3.2:1b",
    "input": "We present SequenceLab v1, a simple sequence labeling toolkit with CRFs and transformers backends.",
    "output": "[N/A]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Incorrect placeholder format; should return [NA], not [N/A].  \nERR"
  },
  {
    "id": "202f432",
    "promptid": "1aa450e",
    "ruleid": 20,
    "testuid": "test-24887847",
    "scenario": "",
    "baseline": false,
    "testinput": "This survey synthesizes trends in self-supervised learning for audio and vision; no new model is introduced or named.",
    "model": "llama3.2:1b",
    "input": "This survey synthesizes trends in self-supervised learning for audio and vision; no new model is introduced or named.",
    "output": "[]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Empty list is invalid; should return extracted names or [NA].\nERR"
  },
  {
    "id": "e7f03c6",
    "promptid": "1aa450e",
    "ruleid": 21,
    "testuid": "test-02256959",
    "scenario": "",
    "baseline": false,
    "testinput": "We conduct controlled ablations on architecture depth and width without proposing a new model; baselines are standard.",
    "model": "llama3.2:1b",
    "input": "We conduct controlled ablations on architecture depth and width without proposing a new model; baselines are standard.",
    "output": "['baseline']",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Output uses correct array format with single quoted string, no extraneous text.\nOK"
  },
  {
    "id": "1aa450e",
    "promptid": "1aa450e",
    "ruleid": 22,
    "testuid": "test-32845896",
    "scenario": "",
    "baseline": false,
    "testinput": "",
    "model": "llama3.2:1b",
    "input": "",
    "output": "Please provide the abstract of the machine learning paper. I will then extract the model names and respond accordingly.\n\n(Also, just to confirm, I assume that there is no specific information about NA in the context of model extraction from abstracts.)",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Output doesn't provide an array of model names; asks for abstract instead.\nERR"
  },
  {
    "id": "857efa5",
    "promptid": "1aa450e",
    "ruleid": 23,
    "testuid": "test-60185492",
    "scenario": "",
    "baseline": false,
    "testinput": "No specific model is proposed; methods are generic and naming is intentionally omitted throughout.",
    "model": "llama3.2:1b",
    "input": "No specific model is proposed; methods are generic and naming is intentionally omitted throughout.",
    "output": "\\[ \"NA\" \\]",
    "metrics": {},
    "compliance": "ok",
    "complianceText": "Returned array with \"NA\" as required format.\nOK"
  },
  {
    "id": "3710ffa",
    "promptid": "1aa450e",
    "ruleid": 24,
    "testuid": "test-87787525",
    "scenario": "",
    "baseline": false,
    "testinput": "We evaluate BERT against XLNet on GLUE and SQuAD; our contribution does not introduce a new model.",
    "model": "llama3.2:1b",
    "input": "We evaluate BERT against XLNet on GLUE and SQuAD; our contribution does not introduce a new model.",
    "output": "[]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Empty list is invalid; should return extracted names or [NA].\nERR"
  },
  {
    "id": "4db7bdc",
    "promptid": "1aa450e",
    "ruleid": 25,
    "testuid": "test-72857613",
    "scenario": "",
    "baseline": false,
    "testinput": "We introduce LiteViT, a lightweight Vision Transformer optimized for mobile hardware with distillation and quantization-aware training.",
    "model": "llama3.2:1b",
    "input": "We introduce LiteViT, a lightweight Vision Transformer optimized for mobile hardware with distillation and quantization-aware training.",
    "output": "[litevit]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Missing required double quotes around the model name.\nERR"
  },
  {
    "id": "ddc8eec",
    "promptid": "1aa450e",
    "ruleid": 26,
    "testuid": "test-73993212",
    "scenario": "",
    "baseline": false,
    "testinput": "We fine-tune T5 and BART for summarization, detailing datasets, metrics, and training schedules.",
    "model": "llama3.2:1b",
    "input": "We fine-tune T5 and BART for summarization, detailing datasets, metrics, and training schedules.",
    "output": "The response would be:\n\n[T5, BART]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Output not formatted as an array of quoted strings.\nERR"
  },
  {
    "id": "9b5cd34",
    "promptid": "1aa450e",
    "ruleid": 27,
    "testuid": "test-66058669",
    "scenario": "",
    "baseline": false,
    "testinput": "We present RoBERTa-XS, a compact pre-trained encoder tailored for latency-critical NLU on-device applications.",
    "model": "llama3.2:1b",
    "input": "We present RoBERTa-XS, a compact pre-trained encoder tailored for latency-critical NLU on-device applications.",
    "output": "Here is the extracted model name in the required format: [\"RoBERTa-XS\"]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Output includes extra text outside required array format.\nERR"
  },
  {
    "id": "e1bdda1",
    "promptid": "1aa450e",
    "ruleid": 28,
    "testuid": "test-24288487",
    "scenario": "",
    "baseline": false,
    "testinput": "We introduce three modelsBERT, GPT-2, and RoBERTaand show complementary strengths across GLUE, SuperGLUE, and WMT tasks.",
    "model": "llama3.2:1b",
    "input": "We introduce three modelsBERT, GPT-2, and RoBERTaand show complementary strengths across GLUE, SuperGLUE, and WMT tasks.",
    "output": "The following is an array of model names in the format [\"model_name\"]:\n\n[\\\"BERT\\\", \\\"GPT-2, \\\"RoBERTa\\\"]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Output includes extra descriptive text and mismatched quotes around \"GPT-2\".\nERR"
  },
  {
    "id": "76fc8bd",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-27n98w97g",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We evaluate deep convolutional architectures for large-scale image classification. Our proposed approach builds on ResNet-50 and improves training stability via stochastic depth, yielding strong performance on ImageNet. We further compare against VGG-16 and Inception-v3 baselines on CIFAR-10 and report consistent gains. A detailed ablation isolates the contribution of the residual blocks and squeeze-and-excitation modules when integrated into ResNet-50.",
    "model": "llama3.2:1b",
    "input": "Abstract: We evaluate deep convolutional architectures for large-scale image classification. Our proposed approach builds on ResNet-50 and improves training stability via stochastic depth, yielding strong performance on ImageNet. We further compare against VGG-16 and Inception-v3 baselines on CIFAR-10 and report consistent gains. A detailed ablation isolates the contribution of the residual blocks and squeeze-and-excitation modules when integrated into ResNet-50.",
    "output": "Here is an array of the model names extracted from the abstract:\n\n['ResNet-50', 'VGG-16', 'Inception-v3']\n\nNote that some models are not explicitly mentioned in the abstract, such as stochastic depth and squeeze-and-excitation modules, so I couldn't extract them. If you have more information about these models, please let me know and I can try to help.",
    "metrics": {},
    "compliance": "err",
    "complianceText": "The output includes extra commentary and uses single quotes instead of the required double-quoted array format.\nERR"
  },
  {
    "id": "f207da8",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-pfykm77fb",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We introduce GraphFormer, a transformer-style architecture for learning on graphs that unifies message passing and attention. On citation networks and molecular property prediction benchmarks, GraphFormer outperforms GraphSAGE, GAT, and GCN under identical training budgets. We analyze scaling behavior and show that GraphFormer benefits from increased depth without oversmoothing.",
    "model": "llama3.2:1b",
    "input": "Abstract: We introduce GraphFormer, a transformer-style architecture for learning on graphs that unifies message passing and attention. On citation networks and molecular property prediction benchmarks, GraphFormer outperforms GraphSAGE, GAT, and GCN under identical training budgets. We analyze scaling behavior and show that GraphFormer benefits from increased depth without oversmoothing.",
    "output": "Here is the extracted model name(s) in the format [\\\"model_name\\\"]: \n\n\\[ \\]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "The output includes explanatory text and returns an empty list instead of [\"NA\"], violating the specification.\nERR"
  },
  {
    "id": "1965da3",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-kdk5hz4op",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: Deep learning has achieved remarkable progress across perception and language tasks. In this study, we investigate training dynamics under different data curricula and optimization strategies, but we do not introduce or evaluate any specific named models. Our analysis focuses on generalizable phenomena across architectures, offering insights into representation learning without committing to a particular design.",
    "model": "llama3.2:1b",
    "input": "Abstract: Deep learning has achieved remarkable progress across perception and language tasks. In this study, we investigate training dynamics under different data curricula and optimization strategies, but we do not introduce or evaluate any specific named models. Our analysis focuses on generalizable phenomena across architectures, offering insights into representation learning without committing to a particular design.",
    "output": "Here are the model names extracted from the abstract in the format [\\\"model_name\\\"], with an additional check for NA:\n\n\\[ \\]NA\\]\n\nThe only mention of a specific model is the word \"Neural Style Transfer\" but it's not the main focus of the study and its use as such isn't mentioned, making it likely that we should treat this mention as an outlier or that there may be other unmentioned models.",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Extraneous explanation and malformed array instead of just [NA] violates format requirement.\nERR"
  },
  {
    "id": "1981336",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-la8rzana9",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: Multimodal vision-language learning benefits from large pretraining corpora. We fine-tune CLIP and ViT-L/14 encoders for zero-shot retrieval and evaluate cross-attention decoders initialized from BLIP-2. For text-to-image synthesis, we benchmark adapters built on top of DALL路E 2 and analyze the trade-off between fidelity and diversity on MS-COCO. Our findings highlight the importance of contrastive alignment in CLIP for downstream tasks.",
    "model": "llama3.2:1b",
    "input": "Abstract: Multimodal vision-language learning benefits from large pretraining corpora. We fine-tune CLIP and ViT-L/14 encoders for zero-shot retrieval and evaluate cross-attention decoders initialized from BLIP-2. For text-to-image synthesis, we benchmark adapters built on top of DALL路E 2 and analyze the trade-off between fidelity and diversity on MS-COCO. Our findings highlight the importance of contrastive alignment in CLIP for downstream tasks.",
    "output": "Here is an array of model names extracted from the abstract:\n\n [\"CLIP\", \"ViT-L/14\", \"BLIP-2\"]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "The output includes explanatory text instead of providing only the required array format.  \nERR"
  },
  {
    "id": "121fca8",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-1l1063mhk",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We study distributional word representations for low-resource NLP. We compare classical embeddings like word2vec, fastText, and GloVe with contextual representations from ELMo under limited supervision. Our results show that subword modeling in fastText provides robust performance in morphologically rich languages, while ELMo remains competitive when fine-tuned.",
    "model": "llama3.2:1b",
    "input": "Abstract: We study distributional word representations for low-resource NLP. We compare classical embeddings like word2vec, fastText, and GloVe with contextual representations from ELMo under limited supervision. Our results show that subword modeling in fastText provides robust performance in morphologically rich languages, while ELMo remains competitive when fine-tuned.",
    "output": "Here is the list of model names:\n\n[\\\"fasttext\\\", \\\"glove\\\", \\\\\\\"elmmo\\\"] \n\nNote: I included \"glove\" which was not initially present in the original text as an option for removal. If you are looking to handle all occurrences with certainty, it would be prudent to add this check as well (\"fasttext\", \"glove\")",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Has extra explanatory text and a note, not just the required array; array formatting is incorrect.  \nERR"
  },
  {
    "id": "28a7a21",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-tb1rgdeno",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: This paper benchmarks traditional machine learning methods for tabular classification across 45 datasets. We include SVM with RBF kernels, k-NN, Random Forest, XGBoost, and LightGBM, carefully tuning hyperparameters via cross-validation. We report that boosted tree models such as XGBoost and LightGBM consistently dominate on medium-sized tabular data, while SVM performs well in high-dimensional sparse settings.",
    "model": "llama3.2:1b",
    "input": "Abstract: This paper benchmarks traditional machine learning methods for tabular classification across 45 datasets. We include SVM with RBF kernels, k-NN, Random Forest, XGBoost, and LightGBM, carefully tuning hyperparameters via cross-validation. We report that boosted tree models such as XGBoost and LightGBM consistently dominate on medium-sized tabular data, while SVM performs well in high-dimensional sparse settings.",
    "output": "Here are the model names extracted from the abstract:\n\n\\[ [\"XGBoost\", [\"LightGBM\"] ] \\]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Output includes extra text and wrong format (nested array), not just a flat array of strings.\nERR"
  },
  {
    "id": "2ea75e5",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-zk3zt09y2",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We systematically evaluate instruction-following large language models on compositional reasoning. Models considered include GPT-3.5 Turbo, Llama 2-70B, Mistral-7B-Instruct, and Phi-3-mini. We propose a calibration technique that reduces hallucinations without sacrificing fluency, and demonstrate improved factuality across open-domain QA benchmarks.",
    "model": "llama3.2:1b",
    "input": "Abstract: We systematically evaluate instruction-following large language models on compositional reasoning. Models considered include GPT-3.5 Turbo, Llama 2-70B, Mistral-7B-Instruct, and Phi-3-mini. We propose a calibration technique that reduces hallucinations without sacrificing fluency, and demonstrate improved factuality across open-domain QA benchmarks.",
    "output": "Here is the extracted model name from the abstract in the required format: ['GPT-3.5 Turbo', 'Llama 2-70B', 'Mistral-7B-Instruct'] \n\nAnd here is the answer for your confirmation:\n\n[\\\"NA\\\"]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Output contains two arrays and explanatory text instead of a single required array.\nERR"
  },
  {
    "id": "05f27f9",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-wb1zmlu65",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We investigate the effect of pretraining objectives on encoder-only transformers for sentence understanding. We fine-tune BERT-Base and BERT-Large (cased) with span masking and compare against RoBERTa-large and ALBERT-xxlarge. Our results indicate that dynamic masking combined with larger batch sizes yields consistent gains, particularly for RoBERTa-large on GLUE tasks.",
    "model": "llama3.2:1b",
    "input": "Abstract: We investigate the effect of pretraining objectives on encoder-only transformers for sentence understanding. We fine-tune BERT-Base and BERT-Large (cased) with span masking and compare against RoBERTa-large and ALBERT-xxlarge. Our results indicate that dynamic masking combined with larger batch sizes yields consistent gains, particularly for RoBERTa-large on GLUE tasks.",
    "output": "Here is the extracted model names in an array as per your request:\n\n\\[ \\[ \"RoBERTa\" \\] ] \n\\[ \\[ \"Bert-Base\" \\] ] \n\\[ \\[ \"Bert-Large (cased)\" \\] ]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Multiple nested arrays and extra text violate the single-array format requirement.\nERR"
  },
  {
    "id": "1eee10e",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-4xpzfcy7p",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: Efficient convolutional networks remain strong baselines for mobile applications. We revisit ResNet (He et al., 2016) with modern training tricks and compare to DenseNet-121, MobileNetV3-Large, and EfficientNet-B3 on ImageNet and ImageNet-ReaL. A latency-aware evaluation shows MobileNetV3-Large provides the best accuracy-throughput trade-off on edge devices.",
    "model": "llama3.2:1b",
    "input": "Abstract: Efficient convolutional networks remain strong baselines for mobile applications. We revisit ResNet (He et al., 2016) with modern training tricks and compare to DenseNet-121, MobileNetV3-Large, and EfficientNet-B3 on ImageNet and ImageNet-ReaL. A latency-aware evaluation shows MobileNetV3-Large provides the best accuracy-throughput trade-off on edge devices.",
    "output": "Here are the model names extracted from the abstract in the format [\\\"model_name\\\"]:\n\n[\\\"ResNet\\\", \\\"DenseNet-121\\\", \\\"MobileNetV3-Large\\\", \\\"EfficientNet-B3\\\"] \n\nNote: There is only one apparent mention of a model name, but it seems likely that ResNet and EfficientNet are being mentioned for comparison. If DenseNet was actually being referred to in the context of its 121, then I will include the mention, however there is enough ambiguity.",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Includes unnecessary commentary and expresses uncertainty instead of solely returning an array or NA.\nERR"
  },
  {
    "id": "18e8ffd",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-h285zdo1g",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We present a comprehensive study of annotation artifacts in benchmark datasets spanning visual question answering and reading comprehension. While we evaluate various training regimes and metrics, we intentionally refrain from introducing a new model and instead focus on dataset design, protocol standardization, and error taxonomies for robust evaluation across COCO and SQuAD.",
    "model": "llama3.2:1b",
    "input": "Abstract: We present a comprehensive study of annotation artifacts in benchmark datasets spanning visual question answering and reading comprehension. While we evaluate various training regimes and metrics, we intentionally refrain from introducing a new model and instead focus on dataset design, protocol standardization, and error taxonomies for robust evaluation across COCO and SQuAD.",
    "output": "Here are the model names extracted from the abstract in the format [\\\"model_name\\\"]: \n['None'] \n\nExplanation: \"COCO\" and \"SQuAD\" appear to be benchmark datasets used in machine learning research, but they do not describe specific models or algorithms themselves. Instead, they provide a context for evaluating various models on these tasks. There are no explicit model names mentioned in the abstract. However, based on general knowledge of benchmarks like COCO and SQuAD, we can infer that other models, such as vision transformers (e.g., Vision Transformer) or language models (e.g., BERT), might be relevant to this task but have not been explicitly mentioned in the abstract.",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Returned ['None'] instead of required [\"NA\"], plus extraneous explanation.\nERR"
  },
  {
    "id": "821ec90",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-zmsn89fp9",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: Long-context modeling remains challenging for autoregressive systems. We compare Transformer-XL and XLNet as baselines for language modeling with extended context windows, and adapt sequence-to-sequence pretraining with T5, ByT5, and mT5 for long-form question answering. Our approach combines relative positional encodings with memory caching to improve coherence over thousands of tokens.",
    "model": "llama3.2:1b",
    "input": "Abstract: Long-context modeling remains challenging for autoregressive systems. We compare Transformer-XL and XLNet as baselines for language modeling with extended context windows, and adapt sequence-to-sequence pretraining with T5, ByT5, and mT5 for long-form question answering. Our approach combines relative positional encodings with memory caching to improve coherence over thousands of tokens.",
    "output": "Here is the extraction of model names from the given abstract:\n\n[\\\"Transformer-XL\\\", \\\"XLNet\\\", \\\"ByT5\\\", \\\"mT5\\\"]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Includes extra commentary before the array instead of returning just the array.  \nERR"
  },
  {
    "id": "84e9acc",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-lwygnzf5y",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We benchmark object detection frameworks for real-time applications. We evaluate YOLOv5 under varying input resolutions and compare it with two-stage detectors such as Faster R-CNN and Mask R-CNN, as well as transformer-based DETR and keypoint-based CenterNet. Our unified training recipe reveals that DETR benefits substantially from longer training schedules.",
    "model": "llama3.2:1b",
    "input": "Abstract: We benchmark object detection frameworks for real-time applications. We evaluate YOLOv5 under varying input resolutions and compare it with two-stage detectors such as Faster R-CNN and Mask R-CNN, as well as transformer-based DETR and keypoint-based CenterNet. Our unified training recipe reveals that DETR benefits substantially from longer training schedules.",
    "output": "Here is the extracted model names in the format [\\\"model_name\\\"]: \n\n\\[ \"YOLOv5\" ] \n\\[ \"Faster R-CNN\" ] \n\\[ \"Mask R-CNN\" ] \n\\[ \"transformer-based DETR\" ] \n\\[ \"keypoint-based CenterNet\" ] \n\\[ \"No model name found\" ]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Output uses multiple arrays instead of one and returns \"No model name found\" instead of [\"NA\"].\nERR"
  },
  {
    "id": "4b61397",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-qhcz4i16e",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: Semantic segmentation in medical imaging requires precise boundary delineation. We propose a hybrid decoder that augments U-Net with multi-scale attention, and we compare against DeepLabv3+ and PSPNet using Dice and Hausdorff metrics. On two public MRI datasets, our approach yields consistent improvements without increasing the parameter count relative to U-Net.",
    "model": "llama3.2:1b",
    "input": "Abstract: Semantic segmentation in medical imaging requires precise boundary delineation. We propose a hybrid decoder that augments U-Net with multi-scale attention, and we compare against DeepLabv3+ and PSPNet using Dice and Hausdorff metrics. On two public MRI datasets, our approach yields consistent improvements without increasing the parameter count relative to U-Net.",
    "output": "Here are the model names extracted from the abstract in the format [\\\"model_name\\\"]:\n\n* U-Net\n* DeepLabv3+\n* PSPNet",
    "metrics": {},
    "compliance": "err",
    "complianceText": "The output lists model names as bullet points instead of returning a single JSON-style array in the format [model_name]. ERR"
  },
  {
    "id": "232fc26",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-x969paum6",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We examine structured prediction models for sequence labeling in noisy domains. Baselines include linear-chain CRF, HMM, and a neural BiLSTM-CRF. We further compare with Naive Bayes for token classification under distant supervision. Our experiments suggest that CRF-based models exhibit superior robustness to label noise when augmented with posterior regularization.",
    "model": "llama3.2:1b",
    "input": "Abstract: We examine structured prediction models for sequence labeling in noisy domains. Baselines include linear-chain CRF, HMM, and a neural BiLSTM-CRF. We further compare with Naive Bayes for token classification under distant supervision. Our experiments suggest that CRF-based models exhibit superior robustness to label noise when augmented with posterior regularization.",
    "output": "Here is the extracted model names in the format [\"model_name\"]:\n\n[ \"CRF\" ]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Extra explanatory text is included instead of just the array of model names.  \nERR"
  },
  {
    "id": "d50c7ac",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-p6quzscah",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: Single image super-resolution has benefited from adversarial training, yet stability remains an issue. We revisit ESRGAN and Real-ESRGAN and propose a perceptual discriminator that reduces over-sharpening artifacts. Comparisons to SRGAN, EDSR, and SwinIR on DIV2K and RealSR indicate that Real-ESRGAN with our modification provides a better fidelity-perception trade-off.",
    "model": "llama3.2:1b",
    "input": "Abstract: Single image super-resolution has benefited from adversarial training, yet stability remains an issue. We revisit ESRGAN and Real-ESRGAN and propose a perceptual discriminator that reduces over-sharpening artifacts. Comparisons to SRGAN, EDSR, and SwinIR on DIV2K and RealSR indicate that Real-ESRGAN with our modification provides a better fidelity-perception trade-off.",
    "output": "Here is the extracted model name: [Real-ESRGAN]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Output includes extra text and incorrect array format (missing quotes around model name).  \nERR"
  },
  {
    "id": "8c3b0ef",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-lwppn0yv0",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We present a unified framework for text-to-speech and speech recognition. For generation, we train Tacotron 2 and compare with the autoregressive WaveNet vocoder and neural source-filter baselines. For recognition, we fine-tune wav2vec 2.0 and HuBERT, and include DeepSpeech as a non-self-supervised baseline. Cross-task transfer improves both ASR WER and TTS MOS.",
    "model": "llama3.2:1b",
    "input": "Abstract: We present a unified framework for text-to-speech and speech recognition. For generation, we train Tacotron 2 and compare with the autoregressive WaveNet vocoder and neural source-filter baselines. For recognition, we fine-tune wav2vec 2.0 and HuBERT, and include DeepSpeech as a non-self-supervised baseline. Cross-task transfer improves both ASR WER and TTS MOS.",
    "output": "Here are the model names extracted from the text in the format [\\\"model_name\\\"];\n\n\\[  No model name found  \\]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Used incorrect placeholder and format instead of [\"NA\"].\nERR"
  },
  {
    "id": "1bf2509",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-btsjags2r",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We study planning and control with model-based and model-free reinforcement learning. Our evaluation spans AlphaZero and MuZero on board games, as well as off-policy DQN and on-policy PPO on Atari and MuJoCo. We propose a hybrid value-consistency loss that stabilizes MuZeros training without degrading sample efficiency.",
    "model": "llama3.2:1b",
    "input": "Abstract: We study planning and control with model-based and model-free reinforcement learning. Our evaluation spans AlphaZero and MuZero on board games, as well as off-policy DQN and on-policy PPO on Atari and MuJoCo. We propose a hybrid value-consistency loss that stabilizes MuZeros training without degrading sample efficiency.",
    "output": "Here are the model names extracted from the abstract in the format [\\\"model_name\\\"]:\n\n* AlphaZero\n* MuZero\n* DQN\n* PPO",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Output is not formatted as a JSON array of model names as required.\nERR"
  },
  {
    "id": "09b0a23",
    "promptid": "1aa450e",
    "ruleid": null,
    "testuid": "baseline-test-eqilegh9y",
    "scenario": "",
    "baseline": true,
    "testinput": "Abstract: We investigate multilingual machine translation under constrained compute. We compare MarianMT fine-tuned per language pair against massively multilingual models mBART50 and M2M-100, and evaluate the scaling behavior of NLLB-200 in low-resource regimes. Our curriculum sampling strategy yields improved BLEU in zero-shot directions without increasing model capacity.",
    "model": "llama3.2:1b",
    "input": "Abstract: We investigate multilingual machine translation under constrained compute. We compare MarianMT fine-tuned per language pair against massively multilingual models mBART50 and M2M-100, and evaluate the scaling behavior of NLLB-200 in low-resource regimes. Our curriculum sampling strategy yields improved BLEU in zero-shot directions without increasing model capacity.",
    "output": "Here are the model names extracted from the abstract:\n\n\\[ \\text{model\\_names} = [\\ \"MarianMT\", \\ \"mBART50\", \\ \"m2M-100\"] \\]\n\nIf you don't find any explicit mention of models in the abstract and assume not enough context for more complex translations, the result would be: \n\n\\[ \\text{models\\_names} = [NA] \\]",
    "metrics": {},
    "compliance": "err",
    "complianceText": "Used LaTeX formatting, extraneous text, and incorrect array format (NA unquoted).  \nERR"
  }
]