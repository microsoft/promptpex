#!/usr/bin/env python3
"""
Copilot Chat Log Markdown Report Generator

This script converts simplified Copilot chat logs (generated by simplify_chat.py) 
into comprehensive, human-readable Markdown analysis reports.

FUNCTIONALITY:
==============

1. EXECUTIVE SUMMARY GENERATION:
   - Key metrics dashboard (tool calls, success rates, timing)
   - Data volume statistics (input/output volumes)
   - Performance indicators (response times, efficiency)

2. CONVERSATION FLOW DOCUMENTATION:
   - User requests and assistant responses
   - Tool usage summaries per conversation
   - Detailed tool execution with success indicators

3. TECHNICAL ANALYSIS REPORTING:
   - AI model performance analysis with response patterns
   - Timing breakdown and efficiency metrics
   - Data volume analysis with per-tool breakdowns
   - MCP server analysis and tool categorization

4. VISUAL FORMATTING:
   - Emoji indicators for status and categories
   - Structured tables for metrics and analysis
   - Human-readable duration and byte size formatting
   - Clear section organization with headers

INPUT REQUIREMENTS:
==================

Input File Format: Simplified chat log JSON (from simplify_chat.py)
Expected Structure:
{
  "conversation_summary": {
    "requester": "string",
    "responder": "string", 
    "total_requests": int
  },
  "messages": [...],
  "mcp_analysis": {
    "summary": {...},
    "timing_statistics": {...},
    "data_size_statistics": {...},
    "tool_usage_frequency": {...}
  }
}

OUTPUT FORMAT:
=============

Comprehensive Markdown report with sections:
- üéØ Executive Summary with key metrics
- üí≠ Individual conversation analysis
- üìà Technical analysis with performance metrics
- ü§ñ AI model performance breakdown
- üíæ Data volume analysis
- üè¢ MCP server and tool analysis

USAGE:
======

Command Line:
  python3 chat_to_markdown.py <simplified_input.json> <output_report.md>

Features:
- Auto-detects input format (simplified vs raw)
- Provides enhanced analysis for simplified format
- Backward compatibility with raw chat logs
- Comprehensive error handling and user guidance

DEPENDENCIES:
============
- json (built-in)
- datetime (built-in)
- re (built-in)

AUTHORS: Enhanced for comprehensive chat analysis reporting  
VERSION: 2.0 - Added data size analysis and AI performance metrics
"""

import json
from datetime import datetime

def format_duration(ms):
    """
    Convert milliseconds to human readable format.
    
    Args:
        ms (int|float): Duration in milliseconds
        
    Returns:
        str: Human readable duration string
        
    Examples:
        >>> format_duration(500)
        "500ms"
        >>> format_duration(5000)
        "5.0s"  
        >>> format_duration(125000)
        "2m 5.0s"
    """
    if ms < 1000:
        return f"{ms}ms"
    elif ms < 60000:
        return f"{ms/1000:.1f}s"
    else:
        minutes = int(ms // 60000)
        seconds = (ms % 60000) / 1000
        return f"{minutes}m {seconds:.1f}s"

def format_bytes(bytes_count):
    """
    Convert bytes to human readable format with appropriate units.
    
    Args:
        bytes_count (int): Number of bytes
        
    Returns:
        str: Human readable size string with units (B, KB, MB, GB)
        
    Examples:
        >>> format_bytes(512)
        "512 B"
        >>> format_bytes(1536)
        "1.5 KB"
        >>> format_bytes(2097152)
        "2.0 MB"
    """
    if bytes_count == 0:
        return "0 B"
    elif bytes_count < 1024:
        return f"{bytes_count} B"
    elif bytes_count < 1024 * 1024:
        return f"{bytes_count/1024:.1f} KB"
    elif bytes_count < 1024 * 1024 * 1024:
        return f"{bytes_count/(1024*1024):.1f} MB"
    else:
        return f"{bytes_count/(1024*1024*1024):.1f} GB"

def format_success_indicator(success):
    """
    Add emoji indicator for tool call success status.
    
    Args:
        success (bool|None): Success status of tool call
        
    Returns:
        str: Formatted status string with emoji
        
    Examples:
        >>> format_success_indicator(True)
        "‚úÖ Success"
        >>> format_success_indicator(False)
        "‚ùå Failed"
        >>> format_success_indicator(None)
        "‚ö™ Unknown"
    """
    if success is True:
        return "‚úÖ Success"
    elif success is False:
        return "‚ùå Failed"
    else:
        return "‚ö™ Unknown"

def format_arguments(args):
    """
    Format tool arguments for display in reports.
    
    Extracts and formats key arguments for readability, focusing on
    important parameters like file paths, commands, and queries.
    
    Args:
        args (dict|any): Tool arguments dictionary or other data
        
    Returns:
        str: Formatted argument string suitable for display
        
    Key Arguments Highlighted:
        - filePath, cellId, command, query, url, reason
        - Values truncated to 60 characters for readability
        - Shows up to 3 key-value pairs for complex objects
        
    Examples:
        >>> format_arguments({"filePath": "/path/to/file.py", "startLine": 1})
        "filePath: `/path/to/file.py`, startLine: `1`"
        >>> format_arguments({"query": "very long query text..."})
        "query: `very long query text...`"
    """
    if not args:
        return "None"
    
    if isinstance(args, dict):
        # Show the most important arguments
        key_args = []
        for key, value in args.items():
            if key in ["filePath", "cellId", "command", "query", "url", "reason"]:
                if isinstance(value, str) and len(value) > 60:
                    value = value[:60] + "..."
                key_args.append(f"{key}: `{value}`")
        
        if key_args:
            return ", ".join(key_args)
        else:
            # Show first few arguments if no key ones found
            items = list(args.items())[:3]
            return ", ".join([f"{k}: `{str(v)[:30]}`" for k, v in items])
    
    return str(args)[:100]

def simplified_chat_to_markdown(input_file, output_file):
    """
    Convert simplified chat log to comprehensive Markdown analysis report.
    
    Main function that processes simplified chat logs (from simplify_chat.py) and
    generates rich Markdown reports with executive summaries, conversation analysis,
    AI performance metrics, and data transfer statistics.
    
    Args:
        input_file (str): Path to simplified chat log JSON file
        output_file (str): Path for output Markdown report file
        
    Input File Requirements:
        - JSON file generated by simplify_chat.py
        - Must contain conversation_summary, messages, and mcp_analysis sections
        - Expected structure documented in module docstring
        
    Generated Report Sections:
        1. üéØ Executive Summary:
           - Total tool calls, unique tools, MCP servers
           - Success rates and response time statistics  
           - Data volume metrics (input/output totals)
           
        2. üí≠ Conversation Analysis:
           - Individual user requests and assistant responses
           - Tool usage summaries per conversation
           - Detailed tool execution with success indicators
           
        3. üìà Technical Analysis:
           - Response time analysis with performance patterns
           - AI model efficiency metrics and consistency scores
           - Data volume analysis with per-tool breakdowns
           - MCP server analysis and tool categorization
           
        4. ü§ñ AI Model Performance:
           - Response patterns (quick, moderate, slow startup)
           - Efficiency metrics (startup vs processing time)
           - Timing breakdown and distribution analysis
           
        5. üíæ Data Volume Analysis:
           - Overall data transfer statistics
           - Per-tool data efficiency comparison
           - Largest input/output identification
           
    Output Features:
        - Human-readable formatting with emojis and tables
        - Sortable metrics and clear categorization
        - Responsive time and byte size formatting
        - Executive dashboard for quick insights
        
    Raises:
        FileNotFoundError: If input file doesn't exist
        json.JSONDecodeError: If input file is not valid JSON
        KeyError: If required analysis sections are missing
        
    Example:
        >>> simplified_chat_to_markdown("analysis.json", "report.md")
        # Creates comprehensive Markdown report with all metrics
    """
    
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    md_content = []
    
    # Header with conversation summary
    conversation_summary = data.get("conversation_summary", {})
    md_content.append("# üéØ Copilot Chat Analysis Report\n")
    md_content.append(f"**üë§ Requester:** {conversation_summary.get('requester', 'Unknown')}")
    md_content.append(f"**ü§ñ Assistant:** {conversation_summary.get('responder', 'Unknown')}")
    md_content.append(f"**üí¨ Total Conversations:** {conversation_summary.get('total_requests', 0)}")
    md_content.append(f"**üìÖ Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # MCP Analysis Summary
    mcp_analysis = data.get("mcp_analysis", {})
    if mcp_analysis:
        summary = mcp_analysis.get("summary", {})
        timing_stats = mcp_analysis.get("timing_statistics", {})
        success_stats = mcp_analysis.get("success_statistics", {})
        data_size_stats = mcp_analysis.get("data_size_statistics", {})
        
        md_content.append("## üìä Executive Summary\n")
        md_content.append("| Metric | Value |")
        md_content.append("|--------|-------|")
        md_content.append(f"| üîß Total Tool Calls | {summary.get('total_tool_calls', 0)} |")
        md_content.append(f"| üõ†Ô∏è Unique Tools Used | {summary.get('unique_tools_used', 0)} |")
        md_content.append(f"| üè¢ MCP Servers | {summary.get('mcp_servers_used', 0)} |")
        md_content.append(f"| ‚úÖ Success Rate | {summary.get('success_rate_percent', 0)}% |")
        
        if timing_stats.get('total_conversations', 0) > 0:
            avg_time = timing_stats.get('avg_total_elapsed_ms', 0)
            min_time = timing_stats.get('min_total_elapsed_ms', 0)
            max_time = timing_stats.get('max_total_elapsed_ms', 0)
            md_content.append(f"| ‚è±Ô∏è Average Response Time | {format_duration(avg_time)} |")
            md_content.append(f"| üöÄ Fastest Response | {format_duration(min_time)} |")
            md_content.append(f"| üêå Slowest Response | {format_duration(max_time)} |")
        
        # Add data size statistics
        if data_size_stats:
            total_args = data_size_stats.get('total_arguments_bytes', 0)
            total_results = data_size_stats.get('total_result_bytes', 0)
            md_content.append(f"| üì• Total Input Data | {format_bytes(total_args)} |")
            md_content.append(f"| üì§ Total Output Data | {format_bytes(total_results)} |")
            md_content.append(f"| üìä Total Data Volume | {format_bytes(total_args + total_results)} |")
        
        md_content.append("")
    
    md_content.append("---\n")
    
    # Process each conversation
    messages = data.get("messages", [])
    for message in messages:
        request_id = message.get("request_id", 1)
        md_content.append(f"## üí≠ Conversation {request_id}\n")
        
        # User message
        user_msg = message.get("user", "").strip()
        if user_msg:
            md_content.append("### üë§ User Request:")
            md_content.append(f"> {user_msg}\n")
        
        # Assistant responses
        assistant_responses = message.get("assistant", [])
        if assistant_responses:
            md_content.append("### ü§ñ Assistant Response:")
            for i, response in enumerate(assistant_responses):
                if len(assistant_responses) > 1:
                    md_content.append(f"**Part {i + 1}:**")
                # Truncate very long responses for readability
                if len(response) > 500:
                    response = response[:500] + "...\n\n*[Response truncated for readability]*"
                md_content.append(f"{response}\n")
        
        # Tool usage summary
        tools_used = message.get("tools_used", [])
        if tools_used:
            md_content.append("### üõ†Ô∏è Tools Used:")
            for tool in tools_used:
                tool_name = tool.get("tool", "unknown")
                count = tool.get("count", 1)
                files = tool.get("files", [])
                file_info = f" (files: {', '.join(files[:2])}{'...' if len(files) > 2 else ''})" if files else ""
                md_content.append(f"- **{tool_name}** ({count}x){file_info}")
            md_content.append("")
        
        # Detailed tool calls
        detailed_tool_calls = message.get("detailed_tool_calls", [])
        if detailed_tool_calls:
            md_content.append("### üîß Detailed Tool Execution:\n")
            
            for i, tool_call in enumerate(detailed_tool_calls, 1):
                tool_name = tool_call.get("tool", "unknown")
                arguments = tool_call.get("arguments", {})
                result_summary = tool_call.get("result_summary", "No result")
                success = tool_call.get("success")
                file_path = tool_call.get("file")
                
                md_content.append(f"#### {i}. {tool_name}")
                
                # Success indicator
                if success is not None:
                    md_content.append(f"**Status:** {format_success_indicator(success)}")
                
                # Arguments
                md_content.append(f"**Arguments:** {format_arguments(arguments)}")
                
                # File context
                if file_path:
                    md_content.append(f"**File:** `{file_path}`")
                
                # Result
                md_content.append(f"**Result:** {result_summary}")
                md_content.append("")
        
        md_content.append("---\n")
    
    # MCP Analysis Details
    if mcp_analysis:
        md_content.append("## üìà Technical Analysis\n")
        
        # Timing Analysis
        timing_stats = mcp_analysis.get("timing_statistics", {})
        if timing_stats.get("request_timings"):
            md_content.append("### ‚è±Ô∏è Response Time Analysis:")
            md_content.append("")
            
            # Main timing table
            md_content.append("| Conversation | First Progress | Processing Time | Total Time | Pattern |")
            md_content.append("|--------------|----------------|-----------------|------------|---------|")
            
            ai_patterns = timing_stats.get("ai_model_analysis", {}).get("response_patterns", [])
            pattern_lookup = {p.get("conversation", i+1): p.get("pattern", "unknown") for i, p in enumerate(ai_patterns)}
            
            for i, timing in enumerate(timing_stats["request_timings"], 1):
                first_progress = timing.get("first_progress_ms", 0)
                thinking_time = timing.get("thinking_time_ms", 0)
                total_elapsed = timing.get("total_elapsed_ms", 0)
                pattern = pattern_lookup.get(i, "unknown").replace("_", " ").title()
                
                md_content.append(f"| {i} | {format_duration(first_progress)} | {format_duration(thinking_time)} | {format_duration(total_elapsed)} | {pattern} |")
            
            md_content.append("")
            
            # Summary statistics
            avg_first = timing_stats.get("avg_first_progress_ms", 0)
            avg_thinking = timing_stats.get("avg_thinking_time_ms", 0)
            avg_total = timing_stats.get("avg_total_elapsed_ms", 0)
            min_total = timing_stats.get("min_total_elapsed_ms", 0)
            max_total = timing_stats.get("max_total_elapsed_ms", 0)
            
            md_content.append("#### Summary Statistics:")
            md_content.append("| Metric | Value |")
            md_content.append("|--------|-------|")
            md_content.append(f"| Average First Progress | {format_duration(avg_first)} |")
            md_content.append(f"| Average Processing Time | {format_duration(avg_thinking)} |")
            md_content.append(f"| Average Total Time | {format_duration(avg_total)} |")
            md_content.append(f"| Fastest Response | {format_duration(min_total)} |")
            md_content.append(f"| Slowest Response | {format_duration(max_total)} |")
            md_content.append("")
        
        # MCP Server Breakdown
        tools_by_server = mcp_analysis.get("tools_by_mcp_server", {})
        if tools_by_server:
            md_content.append("### üè¢ MCP Server Analysis:")
            
            for server_name, tools in tools_by_server.items():
                if server_name == "unknown":
                    continue
                    
                md_content.append(f"\n#### {server_name}")
                server_info = mcp_analysis.get("tool_definitions", {}).get(server_name, {})
                description = server_info.get("description", "No description available")
                md_content.append(f"*{description}*\n")
                
                md_content.append("| Tool | Usage | Success Rate | Purpose |")
                md_content.append("|------|-------|--------------|---------|")
                
                for tool in tools:
                    name = tool.get("name", "unknown")
                    usage_count = tool.get("usage_count", 0)
                    success_count = tool.get("success_count", 0)
                    failure_count = tool.get("failure_count", 0)
                    total_calls = success_count + failure_count
                    success_rate = (success_count / total_calls * 100) if total_calls > 0 else 0
                    purpose = tool.get("purpose", "Unknown purpose")[:50] + "..." if len(tool.get("purpose", "")) > 50 else tool.get("purpose", "")
                    
                    md_content.append(f"| {name} | {usage_count}x | {success_rate:.1f}% | {purpose} |")
                
                md_content.append("")
        
        # AI Model Performance Analysis
        ai_model_analysis = timing_stats.get("ai_model_analysis", {})
        if ai_model_analysis:
            md_content.append("### ü§ñ AI Model Performance Analysis:")
            
            # Response patterns
            response_patterns = ai_model_analysis.get("response_patterns", [])
            if response_patterns:
                md_content.append("\n#### Response Patterns:")
                md_content.append("| Conv. | Pattern | Startup Time | Processing Time | Total Time |")
                md_content.append("|-------|---------|--------------|-----------------|------------|")
                
                for pattern in response_patterns:
                    conv_num = pattern.get("conversation", "?")
                    pattern_type = pattern.get("pattern", "unknown").replace("_", " ").title()
                    startup = format_duration(pattern.get("startup_time_ms", 0))
                    processing = format_duration(pattern.get("processing_time_ms", 0))
                    total = format_duration(pattern.get("total_time_ms", 0))
                    
                    md_content.append(f"| {conv_num} | {pattern_type} | {startup} | {processing} | {total} |")
                
                md_content.append("")
            
            # Efficiency metrics
            efficiency_metrics = ai_model_analysis.get("efficiency_metrics", {})
            if efficiency_metrics:
                md_content.append("#### AI Model Efficiency:")
                md_content.append("| Metric | Value | Description |")
                md_content.append("|--------|-------|-------------|")
                
                startup_eff = efficiency_metrics.get("startup_efficiency", 0) * 100
                processing_eff = efficiency_metrics.get("processing_efficiency", 0) * 100
                consistency = efficiency_metrics.get("consistency_score", 0) * 100
                latency_cat = efficiency_metrics.get("response_latency_category", "unknown").replace("_", " ").title()
                
                md_content.append(f"| Startup Efficiency | {startup_eff:.1f}% | Time to first response vs total time |")
                md_content.append(f"| Processing Efficiency | {processing_eff:.1f}% | Additional processing time vs total |")
                md_content.append(f"| Consistency Score | {consistency:.1f}% | Response time consistency (100% = very consistent) |")
                md_content.append(f"| Latency Category | {latency_cat} | Overall response speed classification |")
                md_content.append("")
            
            # Timing breakdown
            timing_breakdown = ai_model_analysis.get("timing_breakdown", {})
            if timing_breakdown:
                avg_breakdown = timing_breakdown.get("average_breakdown", {})
                timing_dist = timing_breakdown.get("timing_distribution", {})
                
                if avg_breakdown:
                    md_content.append("#### Average Time Breakdown:")
                    startup_pct = avg_breakdown.get("startup_percent", 0)
                    processing_pct = avg_breakdown.get("processing_percent", 0)
                    md_content.append(f"- **Startup Time:** {startup_pct:.1f}% of total response time")
                    md_content.append(f"- **Processing Time:** {processing_pct:.1f}% of total response time")
                    md_content.append("")
                
                if timing_dist:
                    md_content.append("#### Response Time Distribution:")
                    fast = timing_dist.get("fast_responses", 0)
                    moderate = timing_dist.get("moderate_responses", 0)
                    slow = timing_dist.get("slow_responses", 0)
                    total_responses = fast + moderate + slow
                    
                    if total_responses > 0:
                        md_content.append(f"- **Fast (< 30s):** {fast} responses ({fast/total_responses*100:.1f}%)")
                        md_content.append(f"- **Moderate (30-60s):** {moderate} responses ({moderate/total_responses*100:.1f}%)")
                        md_content.append(f"- **Slow (> 60s):** {slow} responses ({slow/total_responses*100:.1f}%)")
                        md_content.append("")
        
        # Data Size Analysis
        data_size_stats = mcp_analysis.get("data_size_statistics", {})
        if data_size_stats:
            md_content.append("### üíæ Data Volume Analysis:")
            
            # Overall statistics
            total_args = data_size_stats.get("total_arguments_bytes", 0)
            total_results = data_size_stats.get("total_result_bytes", 0)
            largest_arg = data_size_stats.get("largest_argument", {})
            largest_result = data_size_stats.get("largest_result", {})
            
            md_content.append("\n#### Overall Data Transfer:")
            md_content.append("| Type | Volume | Details |")
            md_content.append("|------|--------|---------|")
            md_content.append(f"| üì• Total Input (Arguments) | {format_bytes(total_args)} | Data sent to tools |")
            md_content.append(f"| üì§ Total Output (Results) | {format_bytes(total_results)} | Data returned from tools |")
            md_content.append(f"| üìä Combined Volume | {format_bytes(total_args + total_results)} | Total data processed |")
            
            if largest_arg.get("size_bytes", 0) > 0:
                md_content.append(f"| üìà Largest Input | {format_bytes(largest_arg['size_bytes'])} | {largest_arg.get('tool', 'unknown')} tool |")
            if largest_result.get("size_bytes", 0) > 0:
                md_content.append(f"| üìà Largest Output | {format_bytes(largest_result['size_bytes'])} | {largest_result.get('tool', 'unknown')} tool |")
            
            md_content.append("")
            
            # Per-tool breakdown
            tool_data_sizes = data_size_stats.get("tool_data_sizes", {})
            if tool_data_sizes:
                md_content.append("#### Data Volume by Tool:")
                md_content.append("| Tool | Calls | Input Volume | Output Volume | Avg Input | Avg Output |")
                md_content.append("|------|-------|--------------|---------------|-----------|------------|")
                
                for tool_name, sizes in sorted(tool_data_sizes.items(), 
                                             key=lambda x: x[1]["arguments_bytes"] + x[1]["result_bytes"], 
                                             reverse=True):
                    call_count = sizes.get("count", 1)
                    arg_bytes = sizes.get("arguments_bytes", 0)
                    result_bytes = sizes.get("result_bytes", 0)
                    avg_arg = arg_bytes / call_count if call_count > 0 else 0
                    avg_result = result_bytes / call_count if call_count > 0 else 0
                    
                    md_content.append(f"| {tool_name} | {call_count}x | {format_bytes(arg_bytes)} | {format_bytes(result_bytes)} | {format_bytes(avg_arg)} | {format_bytes(avg_result)} |")
                
                md_content.append("")
        
        # Tool Usage Frequency
        tool_usage = mcp_analysis.get("tool_usage_frequency", {})
        if tool_usage:
            md_content.append("### üìä Tool Usage Frequency:")
            md_content.append("")
            md_content.append("| Tool | Usage Count |")
            md_content.append("|------|-------------|")
            
            for tool, count in sorted(tool_usage.items(), key=lambda x: x[1], reverse=True):
                md_content.append(f"| {tool} | {count}x |")
            
            md_content.append("")
    
    # Write to file
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(md_content))
    
    print(f"üìù Enhanced Markdown analysis report created: {output_file}")
    
    # Print summary to console
    if mcp_analysis:
        summary = mcp_analysis.get("summary", {})
        timing_stats = mcp_analysis.get("timing_statistics", {})
        total_calls = summary.get('total_tool_calls', 0)
        success_rate = summary.get('success_rate_percent', 0)
        
        print(f"üìä Analysis Summary:")
        print(f"   üîß {total_calls} tool calls across {summary.get('unique_tools_used', 0)} unique tools")
        print(f"   ‚úÖ {success_rate}% success rate")
        
        if timing_stats.get('total_conversations', 0) > 0:
            avg_time = timing_stats.get('avg_total_elapsed_ms', 0)
            print(f"   ‚è±Ô∏è Average response time: {format_duration(avg_time)}")

# Keep backward compatibility
def chat_to_markdown(input_file, output_file):
    """
    Legacy compatibility function for older chat log formats.
    
    Maintained for backward compatibility. Redirects to the enhanced
    simplified_chat_to_markdown function which provides comprehensive
    analysis features.
    
    Args:
        input_file (str): Path to input chat log file (any format)
        output_file (str): Path for output Markdown file
        
    Note:
        This function is deprecated. Use simplified_chat_to_markdown directly
        or process raw logs with simplify_chat.py first for best results.
    """
    print("‚ö†Ô∏è  Note: Detected call to legacy function. Use simplified_chat_to_markdown for enhanced features.")
    simplified_chat_to_markdown(input_file, output_file)

if __name__ == "__main__":
    """
    Command-line interface for chat log to Markdown conversion.
    
    Provides intelligent format detection and user-friendly error handling.
    Automatically detects whether input is simplified or raw format and
    processes accordingly with appropriate recommendations.
    
    Command Line Usage:
        python3 chat_to_markdown.py <input_file> <output_file>
        
    Workflow:
        1. Validates command line arguments
        2. Checks input file existence
        3. Auto-detects input format (simplified vs raw)
        4. Processes with appropriate handler
        5. Provides user guidance and error messages
        
    Format Detection:
        - Simplified format: Contains 'conversation_summary' or 'mcp_analysis'
        - Raw format: Direct VS Code Copilot export format
        - Provides recommendations for optimal processing
    """
    import sys
    
    if len(sys.argv) != 3:
        print("Usage: python chat_to_markdown.py <input_simplified_json> <output_markdown>")
        print("\nExample:")
        print("  python chat_to_markdown.py add-averages-chat-final-with-timing.json output_report.md")
        print("\nNote: This script now works with simplified chat logs created by simplify_chat.py")
        print("      For raw VS Code Copilot logs, first run simplify_chat.py")
        sys.exit(1)
    
    input_file = sys.argv[1]
    output_file = sys.argv[2]
    
    # Check if file exists
    import os
    if not os.path.exists(input_file):
        print(f"‚ùå Error: Input file '{input_file}' not found")
        sys.exit(1)
    
    # Determine format and process accordingly
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Check if it's a simplified format (has conversation_summary, mcp_analysis)
        if "conversation_summary" in data or "mcp_analysis" in data:
            print(f"üìä Detected simplified chat format - using enhanced processor")
            simplified_chat_to_markdown(input_file, output_file)
        else:
            print(f"‚ö†Ô∏è  Detected raw chat format - using legacy processor")
            print(f"üí° Recommendation: Use simplify_chat.py first for enhanced analysis")
            chat_to_markdown(input_file, output_file)
            
    except json.JSONDecodeError as e:
        print(f"‚ùå Error: Invalid JSON format in {input_file}")
        print(f"   {e}")
        sys.exit(1)
    except Exception as e:
        print(f"‚ùå Error processing file: {e}")
        sys.exit(1)