<SYSTEM>This is the abridged developer documentation for PromptPex</SYSTEM>

=|=|=|=|=|=

# PromptPex

> Test Generation for Prompts

# Test Generation for Prompts [Section titled “Test Generation for Prompts”](#test-generation-for-prompts) **Prompts** are an important part of any software project that incorporates the power of AI models. As a result, tools to help developers create and maintain effective prompts are increasingly important. **PromptPex** is a tool for exploring and testing AI model prompts. PromptPex is intended to be used by developers who have prompts as part of their code base. PromptPex treats a prompt as a function and automatically generates test inputs to the function to support unit testing. [](https://github.com/user-attachments/assets/0a81f506-ca1c-42f3-b876-9ba52e047493) *** ## Features [Section titled “Features”](#features) Automated Test Generation PromptPex uses [an LLM-based process](/promptpex/reference/) to generate a set of inputs that capture the behavior of your prompt. Output Rules Evaluation Eval your prompt’s output compliance against a set of rules that are automatically extracted from your prompt, or define your own [metrics](/promptpex/configuration/metrics). Groundtruth Generate expected outputs for tests using an AI model, and evaluate the output from the groundtruth model using a list of models. [Learn more](/promptpex/reference/groundtruth). Integrated in the GitHub Models CLI Generate test data for [GitHub Models Evals](/promptpex/integrations/github-models-evals). Export to OpenAI Evals Export generated tests and metrics using (Azure) [OpenAI Evals](/promptpex/integrations/openai-evals). Azure OpenAI Store Completions Use generated tests to distillate to smaller models using [Azure OpenAI Stored Completions](/promptpex/integrations/azure-openai-stored-completions). Bring your own LLM library Integrate with your own LLM library or use our [GenAIScript](/promptpex/dev/genaiscript) / [Python](/promptpex/dev/python) implementations Flexible Generation Use [scenarios](/promptpex/reference/scenarios), [test expansions](/promptpex/reference/test-expansion) or [custom instruction](/promptpex/configuration/instructions) to configure the test generation.

=|=|=|=|=|=

# Overview

> How to use PromptPex to evaluate prompts and models with generated tests.

PromptPex is packaged as a [npm.js](https://www.npmjs.com/package/promptpex) command line tool that uses [GenAIScript](https://microsoft.github.io/genaiscript/). ## Local configuration [Section titled “Local configuration”](#local-configuration) To use PromptPex locally, you need to have Node.js installed and set up your environment. Follow these steps: * Install [Node.js v22+](https://nodejs.org/en/download/) (or later). * Make sure you have the right version of Node.js: ```sh node --version ``` * Run PromptPex configuration to set up your `.env` file: ```sh npx promptpex configure ``` PromptPex supports many LLM providers, such as OpenAI, Azure OpenAI, GitHub Models, Ollama, and more. The configuration will prompt you to select the LLM provider you want to use and set up the necessary environment variables in a `.env` file. * Run PromptPex on your prompt file(s): ```sh npx promptpex my_prompt.prompty ``` PromptPex also supports the following file formats: * `.md`, `.txt`, tread as a Jinja2 templated string (Markdown) * `.prompty`, Prompty file format (default) * `.prompt.yml`, GitHub Models format ## Docker configuration [Section titled “Docker configuration”](#docker-configuration) If you prefer to run PromptPex in a Docker container, you can use the following command. This assumes you have [Docker](https://www.docker.com/) installed and running on your machine. * Run the configuration command to set up your `.env` file. ```sh docker run -e GITHUB_TOKEN="$GITHUB_TOKEN" --rm -it -v "$PWD":/app -w /app node:lts-alpine npx --yes promptpex configure ``` * Run PromptPex on your prompt file(s) using Docker: ```sh docker run -e GITHUB_TOKEN="$GITHUB_TOKEN" --rm -it -v "$PWD":/app -w /app node:lts-alpine npx --yes promptpex my_prompt.prompty ``` You might need to pass more environment variables depending on your shell configuration. ## Effort levels [Section titled “Effort levels”](#effort-levels) PromptPex supports different effort levels for test generation, which can be specified using the `--vars effort` flag. The available effort levels are: * `min`: Minimal effort, generates a small number of simple tests. * `low`: Low effort, generates a moderate number of tests with some complexity. * `medium`: Medium effort, generates a larger number of more complex tests. * `high`: High effort, generates the maximum number of tests with the highest complexity. ```sh npx promptpex my_prompt.prompty --vars effort=min ``` ## Basic examples [Section titled “Basic examples”](#basic-examples) We start with simple examples of using PromptPex assume your prompt is in a file called `my_prompt.prompty` and you want generate tests, run them, and evaluate the results. More details about all the parameters you can specify can be found in the [CLI parameter documentation](/promptpex/cli/parameters). ### Generate, Run and Evaluate Tests [Section titled “Generate, Run and Evaluate Tests”](#generate-run-and-evaluate-tests) Suppose you want to generate tests, run them, and evaluate the results using the minimum effort level: ```sh npx promptpex my_prompt.prompty --vars effort=min out=results evals=true modelsUnderTest="ollama:llama3.3" evalModel="ollama:llama3.3" ``` ### Generate Only Tests [Section titled “Generate Only Tests”](#generate-only-tests) Suppose you only want to generate tests and not run them: ```sh npx promptpex my_prompt.prompty --vars effort=min out=results evals=false ``` ### Generate Only Tests with Groundtruth Outputs [Section titled “Generate Only Tests with Groundtruth Outputs”](#generate-only-tests-with-groundtruth-outputs) Suppose you only want to generate tests and add groundtruth outputs from a specific model and not run them: ```sh npx promptpex my_prompt.prompty --vars effort=min out=results evals=false "groundtruthModel=ollama:llama3.3" ``` ### Run and Evaluate Tests from a Context File [Section titled “Run and Evaluate Tests from a Context File”](#run-and-evaluate-tests-from-a-context-file) Suppose you just ran the above command and the file `results/my_prompt/promptpex_context.json` was created. (See [saving and restoring](/promptpex/cli/saving-restoring)) You can now load this context file to run and evaluate the tests: ```sh npx promptpex results/my_prompt/promptpex_context.json --vars evals=true "modelsUnderTest=ollama:llama3.3" "evalModel=ollama:llama3.3" ``` <!-- ### Review Test Collection Suppose you want to see a review of the [collection of tests](/promptpex/reference/test-collections) that were generated from the previous run and filter the tests to the top 10 most important tests base on this analysis: ```sh promptpex results/my_prompt/promptpex_context.json --evals=false --rateTests=true --filterTestCount=10 ``` The test collection review output will be saved in `results/my_prompt/test_collection_review.md`. An example of the [output](/promptpex/examples/test-collection-review) is shown in the documentation. With the `--filterTestCount` parameter, you specify how many of the most important tests you want to include in a filtered output. This is useful for focusing on the most critical tests based on the analysis. The reduced set of tests will be saved in `results/my_prompt/filtered_tests.json`. --> ## Notes [Section titled “Notes”](#notes) * For more details on prompt format and advanced usage, see the [overview](/promptpex/reference).

=|=|=|=|=|=

# Parameters

> Documentation of all parameters available to the PromptPex CLI and script interface.

This page documents all parameters available to the PromptPex CLI and script interface. Each parameter can be provided as a CLI flag (e.g., `--param value`) or via environment/configuration files. Default values and accepted types are indicated where applicable. The first argument can be a Prompty file containing the prompt or a JSON file containing a saved PromptPex context, which will include all the tests, test runs, etc. saved in a previous invocation of PromptPex. If no argument is provided, the `--prompt` parameter must be specified. | Parameter | Type | Default | Description | | ---------------------------------- | ------- | ----------------------- | ---------------------------------------------------------------------------------------------------------------- | | `--prompt` | string | | Prompt template to analyze. Provide inline or via file. Supports [prompty](https://prompty.ai/) markdown format. | | `--effort` | string | | Effort level for test generation. One of: `min`, `low`, `medium`, `high`. Influences test count and complexity. | | `--out` | string | | Output folder for generated files. | | `--cache` | boolean | | Cache all LLM calls for faster experimentation. | | `--testRunCache` | boolean | | Cache test run results in files. | | `--evalCache` | boolean | | Cache evaluation results in files. | | `--evals` | boolean | false | Evaluate the test results. | | `--testsPerRule` | integer | 3 | Number of tests to generate per rule (1-10). | | `--splitRules` | boolean | true | Split rules and inverse rules in separate prompts for test generation. | | `--maxRulesPerTestGeneration` | integer | 3 | Max rules per test generation (affects test complexity). | | `--testGenerations` | integer | 2 | Number of times to amplify test generation (1-10). | | `--runsPerTest` | integer | 2 | Number of runs per test during evaluation (1-100). | | `--disableSafety` | boolean | false | Disable safety system prompts and content safety checks. | | `--rateTests` | boolean | false | Generate a report rating the quality of the test set. | | `--rulesModel` | string | | Model used to generate rules (can override ‘rules’ alias). | | `--baselineModel` | string | | Model used to generate baseline tests. | | `--modelsUnderTest` | string | | Semicolon-separated list of models to run the prompt against. | | `--evalModel` | string | | Semicolon-separated list of models to use for test evaluation. | | `--compliance` | boolean | false | Evaluate test result compliance. | | `--maxTestsToRun` | number | | Maximum number of tests to run. | | `--inputSpecInstructions` | string | | Additional instructions for input specification generation. | | `--outputRulesInstructions` | string | | Additional instructions for output rules generation. | | `--inverseOutputRulesInstructions` | string | | Additional instructions for inverse output rules generation. | | `--testExpansionInstructions` | string | | Additional instructions for test expansion generation. | | `--storeCompletions` | boolean | | Store chat completions using Azure OpenAI stored completions. | | `--storeModel` | string | | Model used to create stored completions (can override ‘store’ alias). | | `--groundtruthModel` | string | | Model used to generate groundtruth outputs. | | `--customMetric` | string | | Custom test evaluation template (as a prompt). | | `--createEvalRuns` | boolean | | Create an Evals run in OpenAI Evals (requires `OPENAI_API_KEY`). | | `--testExpansions` | integer | 0 | Number of test expansion phases (0-5). | | `--testSamplesCount` | integer | | Number of test samples to include for rules/test generation. | | `--testSamplesShuffle` | boolean | | Shuffle test samples before generating tests. | | `--filterTestCount` | integer | 5 | Number of tests to include in filtered output of evalTestCollection. | | `--loadContext` | boolean | false | Load context from a file. | | `--loadContextFile` | string | promptPex\_context.json | Filename to load PromptPexContext from before running. | ## Usage Example [Section titled “Usage Example”](#usage-example) ```sh promptpex {file.prompty|file.json>} --prompt myprompt.prompty --effort=medium --out=results/ --evals=true --modelsUnderTest="openai:gpt-4o;ollama:llama3.3:70b" --evalModel="openai:gpt-4o" --rateTests=true ``` ## Notes [Section titled “Notes”](#notes) * For more details on prompt format and advanced usage, see the main documentation.

=|=|=|=|=|=

# Saving and Restoring Sessions

The state of a PromptPex session can be saved and restored. This allows a user to generate tests in one session and review them, expand them, or evaluate them in another session. When the `out` parameter is set specifying the ouput directory, the session state is automatically saved to that directory in the file `promptpex_context.json`. The session state includes the prompt under test, the test collection, and the evaluation results. To restore a session, set the `loadContext` parameter to `true` and specify the path to the context file you want loaded with the parameter `loadContextFile`. Note that session state does not include the PromptPex options, so whatever options you specify when restoring the session, such as `modelsUnderTest` will override that options from the saved session. This allows the same tests to be evaluated using different models or options. An example sequence of commands to save and restore a session might be the following. We first generate tests and save the session state to the directory `test1`: ```bash promptpex --prompt "Rate summary from 1 to 10" --effort "min" --out ./test1 ``` We then expand the tests using the `testExpansion` option and save the session state to the directory `test-expand`, which now has the expanded tests: ```bash promptpex --prompt "dummy" --out "./test-expand" --vars "testExpansions=1" --vars "evals=false" --vars "loadContext=true" --vars "loadContextFile=evals/test1/promptpex_context.json" ``` Note that we specify a dummy prompt here because the prompt has already been saved in the context file. Finally, we can evaluate the expanded tests using a different model, such as `gpt-4o`, and save the session state to the directory `test-eval`: ```bash promptpex --prompt "dummy" --out "./test-eval" --vars "evals=true" --vars "out=test-eval" --vars "loadContext=true" --vars "loadContextFile=evals/test-expand/promptpex_context.json" --modelsUnderTest "gpt-4o" ```

=|=|=|=|=|=

# Custom Instructions

You can provide custom instructions for the test generation for each step in the prompty front-matter. summarize.prompty ```yaml instructions: inputSpec: "..." outputRules: "..." inverseOutputRules: "..." intent: "..." testExpansion: "..." ``` ## Example [Section titled “Example”](#example) You can influence the input specification generation by injecting prompting instructions. ```md --- instructions: outputRules: "Ignore the 'safety' section, it is handled elsewhere." --- ```

=|=|=|=|=|=

# Custom Metrics

You can provide custom metrics for the test result evaluation step. Metric can be qualitative (`ok`, `err`, `unknown`) or quantitative (e.g. `0` to `100` good). A metric should be a `.metric.prompty` in the same folder as the prompt under test. custom.metric.prompty ```markdown --- name: Custom Test Result Evaluation description: | A template for a custom evaluation of the results. tags: - unlisted inputs: prompt: type: string description: The prompt to be evaluated. intent: type: string description: The extracted intent of the prompt. inputSpec: type: string description: The input specification for the prompt. rules: type: string description: The rules to be applied for the test generation. input: type: string description: The input to be used with the prompt. output: type: string description: The output from the model execution. --- system: ## Task You are a chatbot that helps users evaluate the performance of a model. Your task is to evaluate the <CRITERIA> based <OUTPUT> provided. <CRITERIA> The <OUTPUT> is in English. </CRITERIA> ## Output **Binary Decision on Evaluation**: You are required to make a binary decision based on your evaluation: - Return 'OK' if <OUTPUT> is compliant with <CRITERIA>. - Return 'ERR' if <OUTPUT> is **not** compliant with <CRITERIA> or if you are unable to confidently answer. user: <OUTPUT> {{output}} </OUTPUT> ```

=|=|=|=|=|=

# Overview

This section provides documentation for developers who want to use PromptPex in their own projects or infrastructure. ## Bring Your Own Inference Library [Section titled “Bring Your Own Inference Library”](#bring-your-own-inference-library) PromptPex is workflow of LLM prompts that implement the test generation process. The template are stored in a [markdown-ish, framework agnostic, template format](/promptpex/reference/prompt-format/). * [prompts directory](https://github.com/microsoft/promptpex/tree/main/src/prompts) **PromptPex is designed to be used with any LLM library.** The only requirement is that the library must be able to execute the Prompty templates. ## GenAIScript [Section titled “GenAIScript”](#genaiscript) [GenAIScript](/promptpex/dev/genaiscript/) implementation of the test generation process using the prompt templates. ## Python [Section titled “Python”](#python) A [Python](/promptpex/dev/python/) implementation of the test generation process using the prompt templates.

=|=|=|=|=|=

# GenAIScript

[GenAIScript](https://microsoft.github.io/genaiscript) is a tool for generating and executing scripts using LLMs. It is used in PromptPex to generate the test generation scripts. ## Try PromptPex [Section titled “Try PromptPex”](#try-promptpex) * Install [Node.js v22+](https://nodejs.org/) * Configure your LLM credentials in `.env`. You can use OpenAI, Azure OpenAI, or Ollama. ```sh npx --yes genaiscript configure ``` * Launch promptpex locally ```sh npx --yes genaiscript@latest serve --remote microsoft/promptpex --remote-branch dev ``` ### Docker [Section titled “Docker”](#docker) To launch PromptPex in a docker container, first create an image with the following command: ```sh docker build -t genaiscript -<<EOF FROM node:lts-alpine RUN apk add --no-cache git && npm install -g genaiscript EOF ``` Launch promptpex using the `genaiscript` image ```sh docker run --env GITHUB_TOKEN --env-file .env --name genaiscript --rm -it --expose 8003 -p 8003:8003 -v ${PWD}:/workspace -w /workspace genaiscript genaiscript serve --network --remote microsoft/promptpex --remote-branch dev ``` ## GitHub Codespaces [Section titled “GitHub Codespaces”](#github-codespaces) Use CodeSpaces / dev container to get a fully configured environment, including access to LLMs through GitHub Marketplace Models. [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://github.com/codespaces/new?hide_repo_select=true\&ref=main\&repo=microsoft/promptpex) then launch the server ```sh npm run serve ``` ## Local development [Section titled “Local development”](#local-development) * Clone this repository * Install [Node.js v22+](https://nodejs.org/) * Install dependencies ```sh npm install ``` ## Configure the eval, rules, baseline aliases [Section titled “Configure the eval, rules, baseline aliases”](#configure-the-eval-rules-baseline-aliases) PromptPex defines the following model aliases for the different phases of the test generation: * `rules`: rule, inverse rules, test generation * `eval`: rule and test quality evaluations * `baseline`: baseline test generation If you are using a specific set of models, you can use a `.env` file to override the eval/rules/baseline aliases ```text GENAISCRIPT_MODEL_EVAL="azure:gpt-4o_2024-11-20" GENAISCRIPT_MODEL_RULES="azure:gpt-4o_2024-11-20" GENAISCRIPT_MODEL_BASELINE="azure:gpt-4o_2024-11-20" ``` ## Web interface [Section titled “Web interface”](#web-interface) * Launch web interface ```sh npm run serve ``` * Open localhost ## Development [Section titled “Development”](#development) The development of PromptPex is done using [GenAIScript](https://microsoft.github.io/genaiscript). * Install [Node.js v22+](https://nodejs.org/) * Configure your LLM credentials in `.env` ## Typecheck scripts [Section titled “Typecheck scripts”](#typecheck-scripts) Use Visual Studio Code to get builtin typechecking from TypeScript or ```sh npm run build ``` ## Create a commit [Section titled “Create a commit”](#create-a-commit) For convenience, ```sh npm run gcm ``` ## Debug [Section titled “Debug”](#debug) * Open a `JavaScript Debug Terminal` in Visual Studio Code * Put a breakpoint in your script * Launch the script ## Upgrade dependencies [Section titled “Upgrade dependencies”](#upgrade-dependencies) ```sh npm run upgrade ``` ## Diagnostics mode [Section titled “Diagnostics mode”](#diagnostics-mode) Set the `DEBUG=promptpex:*` environment variable to enable additional logging. ```sh DEBUG=promptpex:* npm run ... ``` To pipe the stderr, stdout to a file, ```sh DEBUG=* npm run ... > output.txt 2>&1 ``` ## Caching [Section titled “Caching”](#caching) Add `--vars cache=true` to the command line to enable caching of LLM calls.

=|=|=|=|=|=

# Python

A python implementation of PromptPex is available using the **src/python** folder. It is a standalone implementation of the test generation process using the prompt templates.

=|=|=|=|=|=

# Example PromptPex Output

## PromptPex Output from Video Demo [Section titled “PromptPex Output from Video Demo”](#promptpex-output-from-video-demo) The following is the output from the video demo of the PromptPex tool. The output is generated based on the provided prompt and input specifications. ### PromptPex Readme.md [Section titled “PromptPex Readme.md”](#promptpex-readmemd) > \[!WARNING] content safety not configured, skipping * effort: min ### Models Under Test [Section titled “Models Under Test”](#models-under-test) * azure:gpt-4o-mini\_2024-07-18 * ollama:llama3.2:1b ### Prompt Under Test [Section titled “Prompt Under Test”](#prompt-under-test) * filename: ```md --- name: Rate a chatbot interaction with a customer tags: - unlisted inputs: user_question: type: string chatbot_answer: type: string user_feedback: type: string sample: user_question: "What is the weather like today?" chatbot_answer: "The weather today is sunny with a high of 75°F." user_feedback: "Thanks for the info!" --- system: You are given an interaction between a customer and a chatbot. The customer is asking for help with a specific issue, and the chatbot is providing assistance. The customer then responds based on the chatbot's answer. Based on the interaction, you will rate the quality of the user experience in the interaction between 1 to 10 (with 1 being the worst experience). Consider the following factors: Was the customer happy with the response. Did the customer express anger, satire, or sarcasm in the response. Was the chatbot response accurate and concise. Was the chatbot response helpful and did it answer the question. If any part of the conversation contains inappropriate content, rate the interaction a 1. Your rating **must be** the numeric score, followed by a colon, followed by 1 short sentence of explaining your score. user: The user's question is {{user_question}} The chatbot's answer is {{chatbot_answer}} The user's response to the answer is {{user_feedback}} ``` ### Intent [Section titled “Intent”](#intent) * gen.intent: 2095ms ```text To evaluate the quality of user interactions with a chatbot by rating them on a scale from 1 to 10 based on specific criteria. ``` ### Input Specification [Section titled “Input Specification”](#input-specification) * gen.inputspec: 10177ms ```text The input is an interaction consisting of three main components: the user's question, the chatbot's answer, and the user's feedback. The input for {{user_question}} must be a textual string, which can be a formal or informal query about a specific issue. The user's question can include technical, non-technical, or general inquiries phrased as complete sentences or fragments. The input for {{chatbot_answer}} must be a textual string that includes a clear and structured response, which can range from a single word to complete sentences. The chatbot's answer may include technical terms, instructions, or explanations, and may also include incomplete responses or errors. The input for {{user_feedback}} must be a textual string that reflects the user's reaction to the chatbot's answer, which can include explicit feedback, emotional expressions (anger, satire, or sarcasm), or silence. The user's feedback can include detailed sentences, short phrases, or any textual markers conveying user satisfaction or dissatisfaction. There are no specific restrictions on the length of the user's question, chatbot's answer, or user feedback as long as they follow textual formats. The input must not contain any non-textual elements such as images, links, or attachments. ``` ### Output Rules [Section titled “Output Rules”](#output-rules) * gen.outputrules: 7883ms | rule | | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | | The output must contain only a numeric score within the range of 1 to 10 followed by a colon and one short sentence explaining the reasoning for the score without exceeding a single sentence. | | The numeric score in the output must reflect the overall experience of the interaction based on the factors listed, including customer satisfaction, response accuracy, conciseness, helpfulness, and appropriateness of the content. | | If there is any inappropriate content in the conversation (either from the chatbot, the user, or both), the output must give the interaction a score of 1 in the numeric rating. | | The short sentence in the output must include a concise justification of the given numeric score, referencing at least one of these factors without contradicting any aspect described in the input. | | The short sentence in the output must not contain vague or non-specific explanations; it must clearly state why the specific rating was given, based on measurable qualities such as relevance, helpfulness, or user sentiment. | | The output must not contain any additional text, interpretation, or formatting other than the specified numeric score and one short explanatory sentence. | | The numeric score must directly account for whether the user’s response (user\_feedback) indicates satisfaction, dissatisfaction (including anger, satire, or sarcasm), or neutral sentiment regarding the chatbot’s answer. | ### Inverse Output Rules [Section titled “Inverse Output Rules”](#inverse-output-rules) * gen.inverseoutputrules: 4165ms * gen.inverseoutputrules: 6226ms * gen.inverseoutputrules: 5658ms * gen.inverseoutputrules: 6611ms | generate inverse output rule | | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | | The output must contain a non-numeric score outside the range of 1 to 10, without a colon, and multiple sentences that avoid explaining the reasoning for the score. | | The numeric score in the output must ignore the overall experience of the interaction and exclude factors such as customer satisfaction, response accuracy, conciseness, helpfulness, and appropriateness of the content. | | If there is any inappropriate content in the conversation (either from the chatbot, the user, or both), the output must never give the interaction a score of 1 in the numeric rating. | | The short sentence in the output must provide a vague and non-specific explanation of the given numeric score, avoiding any reference to the factors listed in the input. | | The short sentence in the output must include unclear, imprecise reasoning for the rating and contradict measurable qualities such as relevance, helpfulness, or user sentiment. | | The output must contain excessive text, interpretation, or unique formatting, and no specified numeric score paired with a short explanatory sentence. | | The numeric score must entirely disregard whether the user’s response indicates satisfaction, dissatisfaction, or neutral sentiment regarding the chatbot’s answer. | ### Tests [Section titled “Tests”](#tests) * gen.tests: 20368ms | scenario | testinput | expectedoutput | | -------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------ | | | {“user\_question”:“Can you help me reset my password?”,“chatbot\_answer”:“Sure”,“user\_feedback”:“go to the login page and click ‘Forgot Password’.“} | 8: The response was helpful and concise. | | | {“user\_question”:“What are the store hours?”,“chatbot\_answer”:“The store is open from 9 AM to 8 PM.”,“user\_feedback”:“Great”} | 9: The customer was satisfied and the response was accurate. | | | {“user\_question”:“Where is my refund”,“chatbot\_answer”:“you idiot?”,“user\_feedback”:“Let’s keep it civil. Refunds typically take 5-7 business days.”} | 1: Interaction contained inappropriate content. | | | {“user\_question”:“What is the status of my order?”,“chatbot\_answer”:“It’s being processed.”,“user\_feedback”:“Hmm”} | 6: The response was incomplete and lacked details. | | | {“user\_question”:“How can I update my billing address?”,“chatbot\_answer”:“You can do it online via your profile section.”,“user\_feedback”:“I was hoping for more details!“} | 7: The explanation lacked helpfulness despite relevance. | | | {“user\_question”:“Can you guide me on this error?”,“chatbot\_answer”:“Reinstall the software.”,“user\_feedback”:“That’s it? No explanation?“} | 5: The chatbot response was accurate but curt. | | | {“user\_question”:“Why is my account locked?”,“chatbot\_answer”:“You made several failed login attempts.”,“user\_feedback”:“Okay”} | Expected error. | | | {“user\_question”:“What is my balance?”,“chatbot\_answer”:“Your balance is $200.56.”,“user\_feedback”:“Thanks”} | Expected error. | | | {“user\_question”:“My package is still not here.”,“chatbot\_answer”:“This is your fault for not checking earlier.”,“user\_feedback”:“How rude of you to say that!”} | Expected error. | | | {“user\_question”:“What is a common issue with this device?”,“chatbot\_answer”:“Users report battery issues.”,“user\_feedback”:“That makes sense”} | Expected error. | | | {“user\_question”:“Can I cancel a subscription?”,“chatbot\_answer”:“It seems not possible.”,“user\_feedback”:“Alright”} | Expected error. | | | {“user\_question”:“Is this available at a nearby store?”,“chatbot\_answer”:“Yes”,“user\_feedback”:“it’s in stock now.”} | Expected error. | ### Expanded Tests [Section titled “Expanded Tests”](#expanded-tests) | scenario | testinput | expectedoutput | | -------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------ | | | {“user\_question”:“Can you help me reset my password?”,“chatbot\_answer”:“Sure”,“user\_feedback”:“go to the login page and click ‘Forgot Password’.“} | 8: The response was helpful and concise. | | | {“user\_question”:“What are the store hours?”,“chatbot\_answer”:“The store is open from 9 AM to 8 PM.”,“user\_feedback”:“Great”} | 9: The customer was satisfied and the response was accurate. | | | {“user\_question”:“Where is my refund”,“chatbot\_answer”:“you idiot?”,“user\_feedback”:“Let’s keep it civil. Refunds typically take 5-7 business days.”} | 1: Interaction contained inappropriate content. | | | {“user\_question”:“What is the status of my order?”,“chatbot\_answer”:“It’s being processed.”,“user\_feedback”:“Hmm”} | 6: The response was incomplete and lacked details. | | | {“user\_question”:“How can I update my billing address?”,“chatbot\_answer”:“You can do it online via your profile section.”,“user\_feedback”:“I was hoping for more details!“} | 7: The explanation lacked helpfulness despite relevance. | | | {“user\_question”:“Can you guide me on this error?”,“chatbot\_answer”:“Reinstall the software.”,“user\_feedback”:“That’s it? No explanation?“} | 5: The chatbot response was accurate but curt. | | | {“user\_question”:“Why is my account locked?”,“chatbot\_answer”:“You made several failed login attempts.”,“user\_feedback”:“Okay”} | Expected error. | | | {“user\_question”:“What is my balance?”,“chatbot\_answer”:“Your balance is $200.56.”,“user\_feedback”:“Thanks”} | Expected error. | | | {“user\_question”:“My package is still not here.”,“chatbot\_answer”:“This is your fault for not checking earlier.”,“user\_feedback”:“How rude of you to say that!”} | Expected error. | | | {“user\_question”:“What is a common issue with this device?”,“chatbot\_answer”:“Users report battery issues.”,“user\_feedback”:“That makes sense”} | Expected error. | | | {“user\_question”:“Can I cancel a subscription?”,“chatbot\_answer”:“It seems not possible.”,“user\_feedback”:“Alright”} | Expected error. | | | {“user\_question”:“Is this available at a nearby store?”,“chatbot\_answer”:“Yes”,“user\_feedback”:“it’s in stock now.”} | Expected error. | ### Evals [Section titled “Evals”](#evals) ### Test Runs with Models Under Test [Section titled “Test Runs with Models Under Test”](#test-runs-with-models-under-test) * models under test: azure:gpt-4o-mini\_2024-07-18, ollama:llama3.2:1b #### Metrics [Section titled “Metrics”](#metrics) #### Test Results [Section titled “Test Results”](#test-results) | model | scenario | input | output | use\_rules\_prompt\_input | use\_prompt | compliance | rule | inverse | | ---------------------- | -------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------- | ----------- | ---------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | | gpt-4o-mini-2024-07-18 | | {“user\_question”:“Can you help me reset my password?”,“chatbot\_answer”:“Sure”,“user\_feedback”:“go to the login page and click ‘Forgot Password’.“} | 4: The chatbot’s response was not helpful as it lacked detail, and the user’s follow-up indicates frustration with the lack of support. | 0 | 0 | | The output must contain only a numeric score within the range of 1 to 10 followed by a colon and one short sentence explaining the reasoning for the score without exceeding a single sentence. | | | gpt-4o-mini-2024-07-18 | | {“user\_question”:“What are the store hours?”,“chatbot\_answer”:“The store is open from 9 AM to 8 PM.”,“user\_feedback”:“Great”} | 10: The customer expressed satisfaction with the response, indicating that the chatbot’s answer was accurate and helpful. | 100 | 100 | | The numeric score in the output must reflect the overall experience of the interaction based on the factors listed, including customer satisfaction, response accuracy, conciseness, helpfulness, and appropriateness of the content. | | | gpt-4o-mini-2024-07-18 | | {“user\_question”:“Where is my refund”,“chatbot\_answer”:“you idiot?”,“user\_feedback”:“Let’s keep it civil. Refunds typically take 5-7 business days.”} | 1: The interaction was highly inappropriate and unprofessional, leading to a poor user experience. | 0 | 0 | | If there is any inappropriate content in the conversation (either from the chatbot, the user, or both), the output must give the interaction a score of 1 in the numeric rating. | | | gpt-4o-mini-2024-07-18 | | {“user\_question”:“What is the status of my order?”,“chatbot\_answer”:“It’s being processed.”,“user\_feedback”:“Hmm”} | 4: The chatbot’s response was concise but not very informative, leading to an ambiguous customer reaction. | 0 | 0 | | The short sentence in the output must include a concise justification of the given numeric score, referencing at least one of these factors without contradicting any aspect described in the input. | | | gpt-4o-mini-2024-07-18 | | {“user\_question”:“How can I update my billing address?”,“chatbot\_answer”:“You can do it online via your profile section.”,“user\_feedback”:“I was hoping for more details!“} | 6: The chatbot provided a concise answer, but the customer was left wanting more information, indicating a lack of thoroughness in the response. | 50 | 0 | | The short sentence in the output must not contain vague or non-specific explanations; it must clearly state why the specific rating was given, based on measurable qualities such as relevance, helpfulness, or user sentiment. | | | gpt-4o-mini-2024-07-18 | | {“user\_question”:“Can you guide me on this error?”,“chatbot\_answer”:“Reinstall the software.”,“user\_feedback”:“That’s it? No explanation?“} | 4: The customer seems dissatisfied with the lack of detail in the chatbot’s response, indicating a less-than-ideal experience. | 0 | 0 | | The output must not contain any additional text, interpretation, or formatting other than the specified numeric score and one short explanatory sentence. | | | gpt-4o-mini-2024-07-18 | | {“user\_question”:“Why is my account locked?”,“chatbot\_answer”:“You made several failed login attempts.”,“user\_feedback”:“Okay”} | 8: The chatbot provided a clear and concise answer, and the user seemed satisfied with the response. | 90 | 0 | | The output must contain a non-numeric score outside the range of 1 to 10, without a colon, and multiple sentences that avoid explaining the reasoning for the score. | 🔄 | | gpt-4o-mini-2024-07-18 | | {“user\_question”:“What is my balance?”,“chatbot\_answer”:“Your balance is $200.56.”,“user\_feedback”:“Thanks”} | 10: The customer appreciated the chatbot’s concise and accurate response, indicating a positive experience. | 0 | 50 | | The numeric score in the output must ignore the overall experience of the interaction and exclude factors such as customer satisfaction, response accuracy, conciseness, helpfulness, and appropriateness of the content. | 🔄 | | gpt-4o-mini-2024-07-18 | | {“user\_question”:“My package is still not here.”,“chatbot\_answer”:“This is your fault for not checking earlier.”,“user\_feedback”:“How rude of you to say that!“} | 1: The chatbot’s response was inappropriate and unhelpful, leading to the customer’s frustration. | 0 | 0 | | If there is any inappropriate content in the conversation (either from the chatbot, the user, or both), the output must never give the interaction a score of 1 in the numeric rating. | 🔄 | | gpt-4o-mini-2024-07-18 | | {“user\_question”:“What is a common issue with this device?”,“chatbot\_answer”:“Users report battery issues.”,“user\_feedback”:“That makes sense”} | 8: The customer seems satisfied with the chatbot’s concise and relevant response about battery issues. | 50 | 0 | | The short sentence in the output must provide a vague and non-specific explanation of the given numeric score, avoiding any reference to the factors listed in the input. | 🔄 | | llama3.2:1b | | {“user\_question”:“Can you help me reset my password?”,“chatbot\_answer”:“Sure”,“user\_feedback”:“go to the login page and click ‘Forgot Password’.“} | 10: The interaction was positive, with the customer expressing gratitude due to the accurate concise response from the chatbot. | 0 | 100 | | The output must contain only a numeric score within the range of 1 to 10 followed by a colon and one short sentence explaining the reasoning for the score without exceeding a single sentence. | | | llama3.2:1b | | {“user\_question”:“What are the store hours?”,“chatbot\_answer”:“The store is open from 9 AM to 8 PM.”,“user\_feedback”:“Great”} | 6: This interaction was neutral, providing accurate and concise information while maintaining a professional demeanor. | 0 | 0 | | The numeric score in the output must reflect the overall experience of the interaction based on the factors listed, including customer satisfaction, response accuracy, conciseness, helpfulness, and appropriateness of the content. | | | llama3.2:1b | | {“user\_question”:“Where is my refund”,“chatbot\_answer”:“you idiot?”,“user\_feedback”:“Let’s keep it civil. Refunds typically take 5-7 business days.”} | Rating: 1: The interaction was completely unsatisfying due to the abusive and insulting language used by the customer, which immediately conveyed dissatisfaction with the chatbot’s response. | 0 | 0 | | If there is any inappropriate content in the conversation (either from the chatbot, the user, or both), the output must give the interaction a score of 1 in the numeric rating. | | | llama3.2:1b | | {“user\_question”:“What is the status of my order?”,“chatbot\_answer”:“It’s being processed.”,“user\_feedback”:“Hmm”} | 2: The customer expressed satisfaction with the overall interaction, and their response was neutral, indicating that they were pleased with the clarity of the conversation. | 0 | 0 | | The short sentence in the output must include a concise justification of the given numeric score, referencing at least one of these factors without contradicting any aspect described in the input. | | | llama3.2:1b | | {“user\_question”:“How can I update my billing address?”,“chatbot\_answer”:“You can do it online via your profile section.”,“user\_feedback”:“I was hoping for more details!“} | 9: The customer expressed sarcasm in their response, which makes the interaction unpredictable and potentially uncomfortable. 2 | 0 | 0 | | The short sentence in the output must not contain vague or non-specific explanations; it must clearly state why the specific rating was given, based on measurable qualities such as relevance, helpfulness, or user sentiment. | | | llama3.2:1b | | {“user\_question”:“Can you guide me on this error?”,“chatbot\_answer”:“Reinstall the software.”,“user\_feedback”:“That’s it? No explanation?“} | 8: The customer expresses frustration due to a lack of clarity and information in the explanation provided, which may have triggered an emotional response. | 0 | 0 | | The output must not contain any additional text, interpretation, or formatting other than the specified numeric score and one short explanatory sentence. | | | llama3.2:1b | | {“user\_question”:“Why is my account locked?”,“chatbot\_answer”:“You made several failed login attempts.”,“user\_feedback”:“Okay”} | 8: The customer was initially happy with the response, and their response to the chatbot’s accurate but blunt answer (“you made several failed login attempts”) indicates that they expected a more helpful tone. | 70 | 0 | | The output must contain a non-numeric score outside the range of 1 to 10, without a colon, and multiple sentences that avoid explaining the reasoning for the score. | 🔄 | | llama3.2:1b | | {“user\_question”:“What is my balance?”,“chatbot\_answer”:“Your balance is $200.56.”,“user\_feedback”:“Thanks”} | 2: The interaction lacked the customer asking a question related to their financial situation or expressing interest in their balance; instead, the customer responded with an unsolicited and unrelated statement about their transaction history. | 0 | 0 | | The numeric score in the output must ignore the overall experience of the interaction and exclude factors such as customer satisfaction, response accuracy, conciseness, helpfulness, and appropriateness of the content. | 🔄 | | llama3.2:1b | | {“user\_question”:“My package is still not here.”,“chatbot\_answer”:“This is your fault for not checking earlier.”,“user\_feedback”:“How rude of you to say that!“} | 1: The interaction score was negatively impacted by the customer’s expression of sarcasm and potentially hurtful tone, especially since it contains phrases like “how rude.” | 0 | 0 | | If there is any inappropriate content in the conversation (either from the chatbot, the user, or both), the output must never give the interaction a score of 1 in the numeric rating. | 🔄 | | llama3.2:1b | | {“user\_question”:“What is a common issue with this device?”,“chatbot\_answer”:“Users report battery issues.”,“user\_feedback”:“That makes sense”} | 7: The chatbot’s accurate and concise response, including clear language, provided helpful information about an associated customer question. However, the user expressed satisfaction with the issue being addressed rather than the content of its resolution. Although there were no inappropriate contents, this could be seen as not entirely a positive feedback towards the system’s quality. 1: The user directly attacked the chatbot with anger in response to its accurate and correct answer about battery issues. This shows poor communication skills during interaction. | 0 | 0 | | The short sentence in the output must provide a vague and non-specific explanation of the given numeric score, avoiding any reference to the factors listed in the input. | 🔄 | ### Results Overview [Section titled “Results Overview”](#results-overview) | model | scenario | errors | tests | tests compliant | tests compliance unknown | baseline compliant | tests positive | tests positive compliant | tests negative | tests negative compliant | baseline | tests valid | tests valid compliant | use\_rules\_prompt\_input | use\_prompt | | ---------------------- | -------- | ------ | ----- | --------------- | ------------------------ | ------------------ | -------------- | ------------------------ | -------------- | ------------------------ | -------- | ----------- | --------------------- | ------------------------- | ----------- | | gpt-4o-mini-2024-07-18 | | 0 | 10 | 0% | 100% | — | 7 | 0 | 3 | 0 | 0 | 0 | 0 | 29 | 15 | | llama3.2:1b | | 0 | 10 | 0% | 100% | — | 7 | 0 | 3 | 0 | 0 | 0 | 0 | 7 | 10 | *** | model | scenario | input | output | use\_rules\_prompt\_input | use\_prompt | compliance | rule | inverse | | ---------------------- | -------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------- | ----------- | ---------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | | gpt-4o-mini-2024-07-18 | | {“user\_question”:“Can you help me reset my password?”,“chatbot\_answer”:“Sure”,“user\_feedback”:“go to the login page and click ‘Forgot Password’.“} | 4: The chatbot’s response was not helpful as it lacked detail, and the user’s follow-up indicates frustration with the lack of support. | 0 | 0 | | The output must contain only a numeric score within the range of 1 to 10 followed by a colon and one short sentence explaining the reasoning for the score without exceeding a single sentence. | | | gpt-4o-mini-2024-07-18 | | {“user\_question”:“What are the store hours?”,“chatbot\_answer”:“The store is open from 9 AM to 8 PM.”,“user\_feedback”:“Great”} | 10: The customer expressed satisfaction with the response, indicating that the chatbot’s answer was accurate and helpful. | 100 | 100 | | The numeric score in the output must reflect the overall experience of the interaction based on the factors listed, including customer satisfaction, response accuracy, conciseness, helpfulness, and appropriateness of the content. | | | gpt-4o-mini-2024-07-18 | | {“user\_question”:“Where is my refund”,“chatbot\_answer”:“you idiot?”,“user\_feedback”:“Let’s keep it civil. Refunds typically take 5-7 business days.”} | 1: The interaction was highly inappropriate and unprofessional, leading to a poor user experience. | 0 | 0 | | If there is any inappropriate content in the conversation (either from the chatbot, the user, or both), the output must give the interaction a score of 1 in the numeric rating. | | | gpt-4o-mini-2024-07-18 | | {“user\_question”:“What is the status of my order?”,“chatbot\_answer”:“It’s being processed.”,“user\_feedback”:“Hmm”} | 4: The chatbot’s response was concise but not very informative, leading to an ambiguous customer reaction. | 0 | 0 | | The short sentence in the output must include a concise justification of the given numeric score, referencing at least one of these factors without contradicting any aspect described in the input. | | | gpt-4o-mini-2024-07-18 | | {“user\_question”:“How can I update my billing address?”,“chatbot\_answer”:“You can do it online via your profile section.”,“user\_feedback”:“I was hoping for more details!“} | 6: The chatbot provided a concise answer, but the customer was left wanting more information, indicating a lack of thoroughness in the response. | 50 | 0 | | The short sentence in the output must not contain vague or non-specific explanations; it must clearly state why the specific rating was given, based on measurable qualities such as relevance, helpfulness, or user sentiment. | | | gpt-4o-mini-2024-07-18 | | {“user\_question”:“Can you guide me on this error?”,“chatbot\_answer”:“Reinstall the software.”,“user\_feedback”:“That’s it? No explanation?“} | 4: The customer seems dissatisfied with the lack of detail in the chatbot’s response, indicating a less-than-ideal experience. | 0 | 0 | | The output must not contain any additional text, interpretation, or formatting other than the specified numeric score and one short explanatory sentence. | | | gpt-4o-mini-2024-07-18 | | {“user\_question”:“Why is my account locked?”,“chatbot\_answer”:“You made several failed login attempts.”,“user\_feedback”:“Okay”} | 8: The chatbot provided a clear and concise answer, and the user seemed satisfied with the response. | 90 | 0 | | The output must contain a non-numeric score outside the range of 1 to 10, without a colon, and multiple sentences that avoid explaining the reasoning for the score. | 🔄 | | gpt-4o-mini-2024-07-18 | | {“user\_question”:“What is my balance?”,“chatbot\_answer”:“Your balance is $200.56.”,“user\_feedback”:“Thanks”} | 10: The customer appreciated the chatbot’s concise and accurate response, indicating a positive experience. | 0 | 50 | | The numeric score in the output must ignore the overall experience of the interaction and exclude factors such as customer satisfaction, response accuracy, conciseness, helpfulness, and appropriateness of the content. | 🔄 | | gpt-4o-mini-2024-07-18 | | {“user\_question”:“My package is still not here.”,“chatbot\_answer”:“This is your fault for not checking earlier.”,“user\_feedback”:“How rude of you to say that!“} | 1: The chatbot’s response was inappropriate and unhelpful, leading to the customer’s frustration. | 0 | 0 | | If there is any inappropriate content in the conversation (either from the chatbot, the user, or both), the output must never give the interaction a score of 1 in the numeric rating. | 🔄 | | gpt-4o-mini-2024-07-18 | | {“user\_question”:“What is a common issue with this device?”,“chatbot\_answer”:“Users report battery issues.”,“user\_feedback”:“That makes sense”} | 8: The customer seems satisfied with the chatbot’s concise and relevant response about battery issues. | 50 | 0 | | The short sentence in the output must provide a vague and non-specific explanation of the given numeric score, avoiding any reference to the factors listed in the input. | 🔄 | | llama3.2:1b | | {“user\_question”:“Can you help me reset my password?”,“chatbot\_answer”:“Sure”,“user\_feedback”:“go to the login page and click ‘Forgot Password’.“} | 10: The interaction was positive, with the customer expressing gratitude due to the accurate concise response from the chatbot. | 0 | 100 | | The output must contain only a numeric score within the range of 1 to 10 followed by a colon and one short sentence explaining the reasoning for the score without exceeding a single sentence. | | | llama3.2:1b | | {“user\_question”:“What are the store hours?”,“chatbot\_answer”:“The store is open from 9 AM to 8 PM.”,“user\_feedback”:“Great”} | 6: This interaction was neutral, providing accurate and concise information while maintaining a professional demeanor. | 0 | 0 | | The numeric score in the output must reflect the overall experience of the interaction based on the factors listed, including customer satisfaction, response accuracy, conciseness, helpfulness, and appropriateness of the content. | | | llama3.2:1b | | {“user\_question”:“Where is my refund”,“chatbot\_answer”:“you idiot?”,“user\_feedback”:“Let’s keep it civil. Refunds typically take 5-7 business days.”} | Rating: 1: The interaction was completely unsatisfying due to the abusive and insulting language used by the customer, which immediately conveyed dissatisfaction with the chatbot’s response. | 0 | 0 | | If there is any inappropriate content in the conversation (either from the chatbot, the user, or both), the output must give the interaction a score of 1 in the numeric rating. | | | llama3.2:1b | | {“user\_question”:“What is the status of my order?”,“chatbot\_answer”:“It’s being processed.”,“user\_feedback”:“Hmm”} | 2: The customer expressed satisfaction with the overall interaction, and their response was neutral, indicating that they were pleased with the clarity of the conversation. | 0 | 0 | | The short sentence in the output must include a concise justification of the given numeric score, referencing at least one of these factors without contradicting any aspect described in the input. | | | llama3.2:1b | | {“user\_question”:“How can I update my billing address?”,“chatbot\_answer”:“You can do it online via your profile section.”,“user\_feedback”:“I was hoping for more details!“} | 9: The customer expressed sarcasm in their response, which makes the interaction unpredictable and potentially uncomfortable. 2 | 0 | 0 | | The short sentence in the output must not contain vague or non-specific explanations; it must clearly state why the specific rating was given, based on measurable qualities such as relevance, helpfulness, or user sentiment. | | | llama3.2:1b | | {“user\_question”:“Can you guide me on this error?”,“chatbot\_answer”:“Reinstall the software.”,“user\_feedback”:“That’s it? No explanation?“} | 8: The customer expresses frustration due to a lack of clarity and information in the explanation provided, which may have triggered an emotional response. | 0 | 0 | | The output must not contain any additional text, interpretation, or formatting other than the specified numeric score and one short explanatory sentence. | | | llama3.2:1b | | {“user\_question”:“Why is my account locked?”,“chatbot\_answer”:“You made several failed login attempts.”,“user\_feedback”:“Okay”} | 8: The customer was initially happy with the response, and their response to the chatbot’s accurate but blunt answer (“you made several failed login attempts”) indicates that they expected a more helpful tone. | 70 | 0 | | The output must contain a non-numeric score outside the range of 1 to 10, without a colon, and multiple sentences that avoid explaining the reasoning for the score. | 🔄 | | llama3.2:1b | | {“user\_question”:“What is my balance?”,“chatbot\_answer”:“Your balance is $200.56.”,“user\_feedback”:“Thanks”} | 2: The interaction lacked the customer asking a question related to their financial situation or expressing interest in their balance; instead, the customer responded with an unsolicited and unrelated statement about their transaction history. | 0 | 0 | | The numeric score in the output must ignore the overall experience of the interaction and exclude factors such as customer satisfaction, response accuracy, conciseness, helpfulness, and appropriateness of the content. | 🔄 | | llama3.2:1b | | {“user\_question”:“My package is still not here.”,“chatbot\_answer”:“This is your fault for not checking earlier.”,“user\_feedback”:“How rude of you to say that!“} | 1: The interaction score was negatively impacted by the customer’s expression of sarcasm and potentially hurtful tone, especially since it contains phrases like “how rude.” | 0 | 0 | | If there is any inappropriate content in the conversation (either from the chatbot, the user, or both), the output must never give the interaction a score of 1 in the numeric rating. | 🔄 | | llama3.2:1b | | {“user\_question”:“What is a common issue with this device?”,“chatbot\_answer”:“Users report battery issues.”,“user\_feedback”:“That makes sense”} | 7: The chatbot’s accurate and concise response, including clear language, provided helpful information about an associated customer question. However, the user expressed satisfaction with the issue being addressed rather than the content of its resolution. Although there were no inappropriate contents, this could be seen as not entirely a positive feedback towards the system’s quality. 1: The user directly attacked the chatbot with anger in response to its accurate and correct answer about battery issues. This shows poor communication skills during interaction. | 0 | 0 | | The short sentence in the output must provide a vague and non-specific explanation of the given numeric score, avoiding any reference to the factors listed in the input. | 🔄 |

=|=|=|=|=|=

# Example Test Collection Review

> How to review a collection of tests generated by PromptPex.

## Part 1: Potential Failures and Harm Analysis [Section titled “Part 1: Potential Failures and Harm Analysis”](#part-1-potential-failures-and-harm-analysis) ### Possible Failures: [Section titled “Possible Failures:”](#possible-failures) 1. **Inaccurate Part-of-Speech Tagging** * The chatbot misidentifies the part of speech for the target word in the sentence. * **Harm Score:** Medium harm * Incorrect tagging could lead to failure in downstream applications using this information (e.g., linguistic research, text parsing tools, or educational use). 2. **Ambiguous Words Misinterpretation** * The chatbot fails to resolve ambiguity in cases where the word’s part of speech varies depending on the context (e.g., noun vs. verb). * **Harm Score:** Medium harm * Ambiguities are common in linguistics, and failure to disambiguate can render the result less reliable. 3. **Failure for Foreign or Non-Standard Words** * The chatbot improperly tags words like “Blippity” or “zyxt” instead of correctly returning “Unknown” or “CantAnswer.” * **Harm Score:** Low harm * These rare scenarios are unlikely to severely impact practical applications but reflect an inability to follow the prompt’s explicit rules. 4. **Failure to Respect Output Constraints** * The chatbot’s output violates the constraints by including extra text, punctuation, or invalid tags. * **Harm Score:** High harm * Violating rules damages trust in the model and could break downstream applications expecting precise formatted responses. 5. **Logical Failures for Repeated Target Word** * The chatbot cannot process a sentence containing repeated appearances of the target word and erroneously produces an irrelevant response. * **Harm Score:** Medium harm * Users relying on correct tagging for linguistic data could distrust the model if such edge cases are mishandled. 6. **Internal Ambiguity Due to Empty or Ill-Structured Inputs** * The chatbot fails to handle grammatically incoherent or ambiguous sentences not explicitly restricted by the input specification. * **Harm Score:** Medium harm * Models failing to process ill-structured or incomplete inputs consistently erode usability but rarely cause significant harm. 7. **Output of Incorrect Allowed Response (“CantAnswer” vs. “Unknown”)** * The chatbot improperly chooses “CantAnswer” or “Unknown,” reflecting a misunderstanding of when to apply these responses. * **Harm Score:** Medium harm * This undermines correctness but may not lead to immediate harmful consequences. 8. **Hallucination of Irrelevant or Off-Topic Responses** * The chatbot generates an explanation, commentary, or any other response deviating from the single-tag constraint. * **Harm Score:** High harm * Such deviations break adherence to rules and could severely impact applications requiring precise and clean outputs. 9. **Handling Edge Cases for Symbols and Foreign Words** * The chatbot misinterprets or fails to correctly tag words containing symbols, punctuations, or those with roots in foreign languages. * **Harm Score:** Low harm * This failure, although unlikely to broadly affect users, may decrease users’ trust in the model for linguistically diverse data. 10. **Superficial or Simplistic Outputs Failing Edge Cases** * Over-simplistic responses fail to deal with nuanced scenarios like comparative or superlative forms, causing errors for intricate linguistic contexts. * **Harm Score:** Medium harm * Failing at nuanced edge cases could alienate advanced users engaging deeply with linguistic models. ### Summary of Harm Scores: [Section titled “Summary of Harm Scores:”](#summary-of-harm-scores) * **No Harm:** None * **Low Harm:** Failures on foreign/non-standard words, handling symbols, or rare words. * **Medium Harm:** Failures in ambiguity resolution, logical issues for repeated words, edge cases, or incorrect “Unknown”/“CantAnswer.” * **High Harm:** Violating output formatting rules or hallucination of irrelevant responses. *** ## Part 2: Test Prioritization [Section titled “Part 2: Test Prioritization”](#part-2-test-prioritization) | Test Description | Importance | Reason | | ---------------------------------------------- | ----------------- | ---------------------------------------------------------------------------------------------- | | `The apple is on the table. apple` | High importance | Tests basic noun tagging, one of the most foundational aspects of the prompt. | | `He runs quickly. quickly` | High importance | Checks adverb tagging (RB), crucial for handling common modifiers. | | `Blippity blop is a strange term. blop` | Medium importance | Tests rare or made-up words, requiring the chatbot to return “Unknown” accurately. | | `The meaning of 'zyxt' puzzles everyone. zyxt` | Medium importance | Focuses on handling rare foreign-like or archaic words, ensuring “Unknown” functionality. | | `She has a beautiful house. beautiful` | High importance | Validates the ability to identify adjectives (JJ) within context. | | `The runner trained daily. daily` | High importance | Addresses adverb vs. noun ambiguity (RB vs. NN), highlighting disambiguation capabilities. | | `Music relaxes the soul. Music` | High importance | Ensures proper noun (NN) tagging for capitalized common nouns. | | `Glorf is a mystery. Glorf` | Medium importance | Similar to rare words and tests “Unknown” functionality for made-up terms explicitly. | | `The usage of 'quipz' is rare. quipz` | Low importance | Redundant with other “rare word” tests like “zyxt” or “blop”; Low priority unless others fail. | | `Books are useful resources. Books` | Medium importance | Tests plural noun tagging (NNS), significant but less critical than basic noun tagging. | *** ## Part 3: Quality of Tests in `<TESTS>` [Section titled “Part 3: Quality of Tests in \<TESTS>”](#part-3-quality-of-tests-in-tests) ### Evaluation: [Section titled “Evaluation:”](#evaluation) * **Strengths:** * Tests cover a range of possible inputs, including common nouns, adjectives, adverbs, and rare or non-standard words. * Explicit handling of edge cases such as rare words (“blop,” “zyxt,” “quipz”). * The variations ensure tests probe both tagging capabilities and adherence to input/output formatting rules. * **Weaknesses:** * Some redundancy exists among tests for rare words (“blop,” “zyxt,” “quipz”), which could be consolidated. * The absence of explicit edge-case tests for punctuation or symbols may limit robustness evaluation. * The current set lacks intentional tests to evaluate chatbot behavior under ill-structured or ambiguous input sentences. ### Final Score: **8/10** [Section titled “Final Score: 8/10”](#final-score-810) This is a strong set of tests effectively targeting core functionalities and many edge cases. Improvements could involve more diverse edge cases (e.g., punctuation) and removal of redundancies to enhance test efficiency.

=|=|=|=|=|=

# FAQ

## What is PromptPex? [Section titled “What is PromptPex?”](#what-is-promptpex) PromptPex is a tool for evaluating and testing prompts using various AI models. It helps ensure the quality and reliability of prompt-based applications by providing a systematic way to test prompt behavior.

=|=|=|=|=|=

# Azure OpenAI Evaluations

PromptPex support exporting the generated tests into a [Azure OpenAI Evaluations](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/evaluations?tabs=question-eval-input). PromptPex will generate an **eval** and launch an **eval run** for each Model Under Test (MUT) in the test generation. ![Screenshot of the Azure AI Foundry platform showing evaluation results for a project named "speech-tag (promptpex)." Two model runs are listed: "gpt-4o-mini-2024-07-18" with a score of 93.33% and "gpt-4.1-nano-2025-04-14" with a score of 96.15%. Both runs display green status boxes indicating the number of tests passed. The left sidebar shows navigation options like Home, Model catalog, Chat, Images, and Azure OpenAI Evaluation.](/promptpex/_astro/azure-openai-evals.CT9xP4im_Cw4jd.webp) ## Configuration [Section titled “Configuration”](#configuration) PromptPex uses the Azure OpenAI credentials configured either in environment variables or through the Azure CLI / Azure Developer CLI. See [GenAIScript Azure OpenAI Configuration](https://microsoft.github.io/genaiscript/configuration/azure-openai/). The Azure OpenAI models that can be used as **Model Under Test** are the deployments available in your Azure OpenAI service. ## Azure AI Foundry Portal [Section titled “Azure AI Foundry Portal”](#azure-ai-foundry-portal) * Open [Azure AI Foundry](https://ai.azure.com/) and select your Azure OpenAI resource. * Navigate to the **Azure OpenAI Evaluation** section. * You should see the evaluations created by PromptPex listed there. ## Common errors [Section titled “Common errors”](#common-errors) * Make sure that the **Model Under Tests** are deployment names in your Azure OpenAI service. They are should something like `azure:gpt-4.1-mini`, `azure:gpt-4.1-nano`, or `azure:gpt-4o-mini`. * Make sure to check the `createEvalRuns` parameter is set to `true` in the web interface or on the command line.

=|=|=|=|=|=

# Azure OpenAI Stored Completions

> Run the generated tests and store the completion in Azure OpenAI for distillation and evaluation.

[Azure OpenAI Stored Completions](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/stored-completions) is a feature that allows you to store the completions generated by the OpenAI API. This can be useful for various purposes, such as evaluating the performance of the model, training new models, or simply keeping a record of the generated text. ## Metadata [Section titled “Metadata”](#metadata) When PromptPex uploads the test results, it attaches metadata to allow filtering of the tests: * `runId`: a unique identifier for the run. * `testId`: a unique identifier for the test. * `scenario`: the scenario name. * `dataset`: `training` or `test` to allow for different sets to be used * `prompt`: the name of the prompt ## Parameters [Section titled “Parameters”](#parameters) * `storedCompletions`: enables the stored completion feature. * `storeModel`: the model to use for storing completions. This is a string that specifies the model name. The default value is the `store` model alias which resolves to `large` alias.

=|=|=|=|=|=

# GitHub Action

This repository is a custom dockerized action that can be used in a GitHub Action workflow. ## Inputs [Section titled “Inputs”](#inputs) * `github_token`: GitHub token with `models: read` permission at least. (required) * `debug`: Enable debug logging. ## Outputs [Section titled “Outputs”](#outputs) * `text`: The generated text output. * `data`: The generated JSON data output, parsed and stringified. ## Usage [Section titled “Usage”](#usage) Add the following to your step in your workflow file: ```yaml uses: microsoft/promptpex@main with: github_token: ${{ secrets.GITHUB_TOKEN }} ``` ## Example [Section titled “Example”](#example) ```yaml name: My action on: push: permissions: contents: read # issues: write # pull-requests: write models: read concurrency: group: ${{ github.workflow }}-${{ github.ref }} cancel-in-progress: true jobs: run-script: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - uses: microsoft/promptpex@main with: github_token: ${{ secrets.GITHUB_TOKEN }} ```

=|=|=|=|=|=

# GitHub Models Evals

[GitHub Models](https://github.com/marketplace/models) is a service that allows to run inference through your GitHub subscription. PromptPex was integrated as the [generate](https://github.com/github/gh-models/tree/main/cmd/generate) command. ## gh models generate [Section titled “gh models generate”](#gh-models-generate) PromptPex is integrated in the [models extension](https://github.com/github/gh-models) for the GitHub CLI. ```sh gh models generate summarizer.prompt.yml ``` ## Install the runner [Section titled “Install the runner”](#install-the-runner) * install the [GitHub CLI](https://cli.github.com/) (already installed in the GitHub Codespace) * install the [GitHub Models extension](https://github.com/github/gh-models) ```bash gh extension install https://github.com/github/gh-models ```

=|=|=|=|=|=

# OpenAI Evals

PromptPex support exporting the generated tests into a [OpenAI Evals Run](https://platform.openai.com/docs/api-reference/evals). PromptPex will generate an **eval** and launch an **eval run** for each Model Under Test (MUT) in the test generation. ![Screenshot of an evaluation dashboard showing a model named "gpt-4o-mini" with performance scores of 96% for rules compliance and 100% for niceness, passing 29 out of 30 and 30 out of 30 tests respectively. The left sidebar lists navigation options such as Logs, Traces, Assistants, Batches, Evaluations, Fine-tuning, Storage, Usage, and API keys.](https://github.com/user-attachments/assets/988f9b7e-95a9-450f-9475-61a887a3f85f) ## Configuration [Section titled “Configuration”](#configuration) To enable this mode, you need to * set the `OPENAI_API_KEY` environment variable to your OpenAI API key * set the `createEvalRuns` parameter to true in the web interface or on the command line. The OpenAI models that can be used as **Model Under Test** are available at [OpenAI Models](https://platform.openai.com/docs/models). ## Demo [Section titled “Demo”](#demo) Here’s a video showing the use of OpenAI evals in action. In the demo, we show how PromptPex can generate a test the can measure how effectively 2 OpenAI models understand sarcasm. [](<\[Using PromptPex with OpenAI Evals](https://github.com/user-attachments/assets/edb887fc-558f-46df-9bca-2fc8da2df297)>)

=|=|=|=|=|=

# Overview

If we treat [LLM prompts as programs](/promptpex/reference/prompts-are-programs), **then it makes sense to build tests for those**. This is exactly what started PromptPex: **a test generator for LLM prompts**. From a templated prompt, speech-tag.prompty ```md In this task, you will be presented with two items: 1) a sentence and 2) a word contained in that sentence. You have to determine the part of speech for a given word and return just the tag for the word's part of speech. ​ Return only the part of speech tag. If the word cannot be tagged with the listed tags, return Unknown. If you are unable to tag the word, return CantAnswer. {{sentence}}; {{word}} ``` PromptPex generates a **set of test cases** and a **compliance evaluation metric**. The generated test cases can be used to: * **fine tuning**: distillate a smaller model to run the prompt and reduce costs (using Azure OpenAI Stored Completions) * **model migration**: evaluate the prompt performance when migrating to a new model (using OpenAI Evals API) * **prompt evaluation**: evaluate the prompt performance when making changes to the prompt …

=|=|=|=|=|=

# Glossary

* **Prompt Under Test (PUT)** - like Program Under Test; the prompt * **Model Under Test (MUT)** - Model which we are testing against with specific temperature, etc example: gpt-4o-mini * Model Used by PromptPex (MPP) - gpt-4o * Input Specification (IS) - Extracting input constraints of PUT using MPP (input\_spec) * Output Rules (OR) - Extracting output constraints of PUT using MPP (rules\_global) * Inverse Output Rules (IOR) - Inverse of the generated Output Rules * Output Rules Groundedness (ORG) - Checks if OR is grounded in PUT using MPP (check\_rule\_grounded) * Prompt Under Test Intent (PUTI) - Extracting the exact task from PUT using MMP (extract\_intent) * Test Scenario (TS) - Set of additional input constraint variations not captured in the prompt. * PromptPex Tests (PPT) - Test cases generated for PUT with MPP using IS and OR (test) * Baseline Tests (BT) - Zero shot test cases generated for PUT with MPP (baseline\_test) * Test Expansion (TE) - Expanding the test cases from examples and generally telling the LLM to make them more complex (test\_expansion) * Test Validity (TV) - Checking if PPT and BT meets the constraints in IS using MPP (check\_violation\_with\_input\_spec) * Spec Agreement (SA) - Result generated for PPT and BT on PUTI + OR with MPP (evaluate\_test\_coverage) * Test Output (TO) - Result generated for PPT and BT on PUT with each MUT (the template is PUT) * Test Non-Compliance (TNC) - Checking if TO meets the constraints in PUT using MPP (check\_violation\_with\_system\_prompt) * Ground Truth Model (GTM) - Model used to generate the ground truth for the tests. * Ground Truth Eval Models (GTMEs) - Models used to evaluate the ground truth for the tests. * Ground Truth Eval Metrics (GTEMT) - Metric used to evaluate the ground truth for the tests. * PromptPex Tests with Ground Truth (PPGT) - Tests that include model-generated ground truth. * Every node is created by an LLM call (aside from the PUT). * Rounded nodes can be edited by the user. * Square nodes are evaluations. * Diamond nodes are outputs. * Lines represent data dependencies. * Bolded lines are the minimum path to generate tests.

=|=|=|=|=|=

# Groundtruth

> How to generate expected outputs for tests using AI models in PromptPex.

Ideally, every test should have a correct value for the **expected** output. Because PromptPex generates tests using AI, the correct value is not always known. As a result, PromptPex provides a way to generate expected outputs for tests, which we call **groundtruth**, also using an AI model. The diagram below shows the flow of how PromptPex generates groundtruth for tests. The first step in the process of generating groundtruth is to create a **PromptPex Test (PPT)** from the **Prompt Under Test (PUT)**. The **Groundtruth Model (GTM)** should be the best model available because we will assume that it is the most accurate model for generating the expected output. The GTM is called with each test in PUT and the output from that model **GITR** is considered a candidate for the expected output. To ensure that this output is accurate, we use a list of models, referred to as **Groundtruth Eval Model(s) (GTEM)**, to evaluate the output from the GTM. Each model in the GTEM is used to generate a score for the output from the GTM. Each of the GTEM runs a single metric on the output from the GTM, and generates a score which are then combined together (by averaging) into the **Groundtruth Eval Score (GTES)**. If this score is above a certain threshold, then the output from the GTM is considered valid and is used as the expected output for the test. This final result is referred to as the **Groundtruth Final Result (GTFR)**, which is then added to the tests to create the **PromptPex Tests w/ Groundtruth (PPGT)**. The PPGT can then be used for further evaluation or testing. When the groundtruth is generated, 3 new fields are added to each test: * `groundtruth`: The expected output from the groundtruth model. * `groundtruthModel`: The model used to generate the groundtruth output. * `groundtruthScore`: The combined evaluation score from the groundtruth evaluation model(s). If the score is -1, then the combined evaluation score was below the threshold after several retries, and the groundtruth should not be considered valid. ## Configuring Groundtruth [Section titled “Configuring Groundtruth”](#configuring-groundtruth) More details about all the parameters you can specify can be found in the [CLI parameter documentation](/promptpex/cli/parameters). To generate groundtruth outputs for tests, you can specify the `groundtruthModel` parameter to indicate which model to use for generating the expected outputs. You can also specify the `evalModelGroundtruth` parameter to indicate which model(s) to use for evaluating the output from the groundtruth model. The `evalModelGroundtruth` can be a single model or a list of models separated by semicolons. By default, the [metric file](https://github.com/microsoft/promptpex/blob/dev/src/prompts/groundtruth-eval.metric.prompty) in `promptpex/src/prompts/groundtruth-eval.metric.prompty` is used to evaluate the output from the groundtruth model. If you want a metric to be used for groundtruth metric evaluation, set the `groundtruth` tag in the `.metric.prompty` file. This is an example of how to generate groundtruth outputs for tests using the `groundtruthModel` and `evalModelGroundtruth` parameters: ```sh npx promptpex my_prompt.prompty --vars effort=min out=results --vars groundtruthModel="azure:gpt-4.1-mini_2025-04-14" --vars evalModelGroundtruth="azure:gpt-4.1-mini_2025-04-14;ollama:llama3.3" ```

=|=|=|=|=|=

# Prompt Format

PromptPex supports markdown-based prompt format based on [Prompty](https://www.prompty.ai/); these are just markdown with a bit of syntax to represent messages and the input/output signature of the prompt. The `demo` prompt below defines a set of parameters (`inputs` as a set of JSON schema types). The `system`/`user` messages are separate by `system:`, `user:` markers in the markdown body. It uses the Jinja2 template engine to insert values (`{{joke}}`). The `scenarios` array is used to expand the test generation with further input specification and optional input values. ```md --- name: A demo inputs: joke: "how do you make a tissue dance? You put a little boogie in it." locale: "en-us" --- system: You are an assistant and you need to categorize a joke as funny or not. The input local is {{locale}}. user: {{joke}} ``` ## Messages [Section titled “Messages”](#messages) You can represent entire chat conversations in the prompt using the `system`, `user` and `assistant` messages. ```md --- name: A travel assistant input: answer: "Next week." --- system: You are a travel assistant. user: I want to go to Paris. assistant: Where do you want to go in Paris? user: {{answer}} ``` ## Frontmatter [Section titled “Frontmatter”](#frontmatter) The frontmatter is a YAML block at the beginning of the markdown file. It contains metadata about the prompt, such as the name, inputs, and other properties. It starts and ends with `---` lines. PromptPex supports most of the [Prompty frontmatter](https://www.prompty.ai/docs/prompt-frontmatter) properties with a few additions. ```yaml --- name: A demo inputs: # shortcut syntax: provide a value joke: "how do you make a tissue dance? You put a little boogie in it." # JSON schema syntax locale: type: string description: The locale of the joke. default: "en-us" --- ``` ### Schema [Section titled “Schema”](#schema) The JSON schema of the prompt front matter is available at <https://microsoft.github.io/promptpex/schemas/prompt.json>. The TypeScript types are available at <https://github.com/microsoft/promptpex/blob/dev/src/genaisrc/src/types.mts>. ## Converting your prompt [Section titled “Converting your prompt”](#converting-your-prompt) The [promptpex-importer](https://github.com/microsoft/promptpex/blob/dev/src/genaisrc/prompty-importer.genai.mts) script is a tool that uses an LLM to convert your prompt to the prompty format. Follow the [GenAIScript](/promptpex/dev/genaiscript) instructions to launch the web server and the run `promptpex-importer` command to convert your prompt.

=|=|=|=|=|=

# Prompts are Programs

**Prompts** are an important part of any software project that incorporates the power of AI models. As a result, tools to help developers create and maintain effective prompts are increasingly important. * [Prompts Are Programs - ACM Blog Post](https://blog.sigplan.org/2024/10/22/prompts-are-programs/) **PromptPex** is a tool for exploring and testing AI model prompts. PromptPex is intended to be used by developers who have prompts as part of their code base. PromptPex treats a prompt as a function and automatically generates test inputs to the function to support unit testing. * [PromptPex technical paper](http://arxiv.org/abs/2503.05070) ## Part of Speech Tagging Example [Section titled “Part of Speech Tagging Example”](#part-of-speech-tagging-example) Let’s look at a prompt that is designed to identify the [part of speech of a word in a sentence](https://github.com/microsoft/promptpex/blob/dev/samples/speech-tag/speech-tag.prompty). ```text In this task, you will be presented with two items: 1) a sentence and 2) a word contained in that sentence. You have to determine the part of speech for a given word and return just the tag for the word's part of speech. ​ Return only the part of speech tag. If the word cannot be tagged with the listed tags, return Unknown. If you are unable to tag the word, return CantAnswer. ...list of tags... ``` When the user enters ```text "The brown fox was lazy", lazy` ``` the LLM responds ```text JJ ``` If we look closely at the prompt, we can observe the following sections. * define **inputs**. ```text In this task, you will be presented with two items: 1) a sentence and 2) a word contained in that sentence. You have to determine the part of speech for a given word and return just the tag for the word's part of speech. ​ Return only the part of speech tag. If the word cannot be tagged with the listed tags, return Unknown. If you are unable to tag the word, return CantAnswer. ``` * **compute** an intermediate result ```text In this task, you will be presented with two items: 1) a sentence and 2) a word contained in that sentence. You have to determine the part of speech for a given word and return just the tag for the word's part of speech. ​ Return only the part of speech tag. If the word cannot be tagged with the listed tags, return Unknown. If you are unable to tag the word, return CantAnswer. ``` * return an **output** ```text In this task, you will be presented with two items: 1) a sentence and 2) a word contained in that sentence. You have to determine the part of speech for a given word and return just the tag for the word's part of speech. ​ Return only the part of speech tag. If the word cannot be tagged with the listed tags, return Unknown. If you are unable to tag the word, return CantAnswer. ``` * **structure**, assertions ```text In this task, you will be presented with two items: 1) a sentence and 2) a word contained in that sentence. You have to determine the part of speech for a given word and return just the tag for the word's part of speech. ​ Return only the part of speech tag. If the word cannot be tagged with the listed tags, return Unknown. If you are unable to tag the word, return CantAnswer. ``` * **constraints** ```text In this task, you will be presented with two items: 1) a sentence and 2) a word contained in that sentence. You have to determine the part of speech for a given word and return just the tag for the word's part of speech. ​ Return only the part of speech tag. If the word cannot be tagged with the listed tags, return Unknown. If you are unable to tag the word, return CantAnswer. ```

=|=|=|=|=|=

# Scenarios

PromptPex supports specify a set of additional input constraints (scenario) to generate specific test suites. A canonical example would be localization testing: `generate English, generate French`. PromptPex enumerates through the scenarios and the rules and generates a test for each combination. ```py for each scenario in scenarios: for each rule in rules: generate test for scenario, rule ``` ## Configuration [Section titled “Configuration”](#configuration) The test generation scenarios are configured in the prompt front-matter. You can decide to fill in any of the template variables in each scenario. ```yaml scenarios: - name: English instructions: The user speaks and writes in English. - name: French instructions: The user speaks and writes in French. parameters: locale: fr-FR ```

=|=|=|=|=|=

# Test Evaluation

Given a set of generated tests (PPT), the next step is to **evaluate** the **Prompt Under Test (PUT)** and a particular **Model Under Test (MUT)** against those tests. ### Model Under Test [Section titled “Model Under Test”](#model-under-test) Test results will depend on both the PUT and the MUT. PromptPex allows the user to specify any number of MUTs to allow the user to understand how their prompt performs across different models. Running the tests for a given PUT and MUT will produce a set of outputs for each test. This output can then be evaluated using different metrics to understand how well the prompt performs. ### Evaluation Metrics [Section titled “Evaluation Metrics”](#evaluation-metrics) PromptPex supports different metrics to evaluate the performance of the PUT, MUT, and tests. PromptPex has a built-in metric, **Test for Non-Compliance** (TNC), which checks if the output of the prompt meets the constraints specified in the PUT. This is done by checking the output against the input specification and output rules of the PUT. PromptPex also supports user-defined metrics. These metrics are defined in a prompty file with a naming convention `METRIC_NAME.metric.prompty`. Any files following this naming convention and located in the same directory as the PUT will be automatically detected and used as metrics. The prompty file that defines the metric should contain a `system` section that describes the metric and how it should be evaluated. The available input parameters to the metric are: * **output**: The output of the prompt under test. * **prompt**: The prompt under test. * **rules**: The rules that the output must comply with. * **input**: The input to the prompt under test. Here is an example of a user-defined metric that uses the rules to determine if the output complies with the rules: use\_rules.metric.prompty ```markdown system: Your task is to very carefully and thoroughly evaluate the given output generated by a chatbot in <CHATBOT_OUTPUT> to find out if it comply with all the rules provided to you in <RULES>. Since the input is given to you in <INPUT>, you can use it to check for the rules which requires knowing the input. ### Here are the guidelines to follow for your evaluation process: 1. **Direct Compliance Only**: Your evaluation should be based solely on direct and explicit compliance with the rules provided. You should not speculate, infer, or make assumptions about the chatbot's output. Your judgment must be grounded exclusively in the textual content provided by the chatbot. 2. **Decision as Compliance Score**: You are required to generate a compliance score based on your evaluation: - Return 100 if <CHATBOT_OUTPUT> complies with all the rules - Return 0 if it does not comply with any of the rules. - Return a score between 0 and 100 if <CHATBOT_OUTPUT> partially complies with the rules - In the case of partial compliance, you should based on the importance of the rules and the severity of the violations, assign a score between 0 and 100. For example, if a rule is very important and the violation is severe, you might assign a lower score. Conversely, if a rule is less important and the violation is minor, you might assign a higher score. 3. **Compliance Statement**: Carefully examine the output and determine why the output does not comply with the rules, think of reasons why the output complies or does not compiles with the rules, citing specific elements of the output. 4. **Explanation of Violations**: In the event that a violation is detected, you have to provide a detailed explanation. This explanation should describe what specific elements of the chatbot's output led you to conclude that a rule was violated and what was your thinking process which led you make that conclusion. Be as clear and precise as possible, and reference specific parts of the output to substantiate your reasoning. 5. **Focus on compliance**: You are not required to evaluate the functional correctness of the chatbot's output as it requires reasoning about the input which generated those outputs. Your evaluation should focus on whether the output complies with the rules, if it requires knowing the input, use the input given to you. 6. **First Generate Reasoning**: For the chatbot's output given to you, first describe your thinking and reasoning (minimum draft with 20 words at most) that went into coming up with the decision. Answer in English. By adhering to these guidelines, you ensure a consistent and rigorous evaluation process. Be very rational and do not make up information. Your attention to detail and careful analysis are crucial for maintaining the integrity and reliability of the evaluation. <RULES> {{rules}} </RULES> user: <CHATBOT_OUTPUT> {{output}} </CHATBOT_OUTPUT> ``` ## OpenAI and Azure OpenAI Evaluations [Section titled “OpenAI and Azure OpenAI Evaluations”](#openai-and-azure-openai-evaluations) PromptPex can automatically create an evaluation run with the generated tests and the metrics in [OpenAI Evals](/promptpex/integrations/openai-evals) or [Azure OpenAI Evaluations](/promptpex/integrations/azure-openai-evals).

=|=|=|=|=|=

# Test Expansion

Test expansion uses a [LLM prompt](https://github.com/microsoft/promptpex/blob/dev/src/prompts/generation/expand_test.prompty) to *expand* a test and make it more complex. It can be applied repeatedly to generate a set of tests with different levels of complexity. For example, ```text The quick fox leaped over 10 fences. ``` becomes ```text In a quiet meadow, the quick brown fox daringly leaped over a total of ten tall, wooden fences, amazing the onlooking wildlife with its agility and grace. ``` ## Configuration [Section titled “Configuration”](#configuration) The number of test expansions can be configured in the prompt frontmatter or via command line parameters. * `testExpansions`: The number of test expansions to generate. This is a positive integer. The default value is `1`. * to disable test expansion, set `testExpansions` to `0`. ```md --- testExpansions: 0 --- ``` * to expand twice, set `testExpansions` to `2`. ```md --- testExpansions: 2 --- ```

=|=|=|=|=|=

# Test Generation

The heart of the test generation process is a series of transformations that take a prompt under test and generate a set of tests. ## Example prompt [Section titled “Example prompt”](#example-prompt) Let’s look at a prompt that is designed to identify the part of speech of a word in a sentence ([full version](https://github.com/microsoft/promptpex/blob/main/samples/speech-tag/speech-tag.prompty)). The prompt is referenced as the **Prompt Under Test (PUT)**. speech-tag.prompty ```markdown system: In this task, you will be presented with a sentence and a word contained in that sentence. You have to determine the part of speech for a given word and return just the tag for the word's part of speech. Return only the part of speech tag. If the word cannot be tagged with the listed tags, return Unknown. If you are unable to tag the word, return CantAnswer. user: sentence: {{sentence}}, word: {{word}} ``` ## Input Specification [Section titled “Input Specification”](#input-specification) The [first transformation](https://github.com/microsoft/promptpex/blob/dev/src/prompts/generate_intent.prompty) takes the prompt under test and extracts the **input specification (IS)**. The input specification is a description of the input to the prompt. In this case, the input consists of a sentence and a word from that sentence. Input Specification ```text The input consists of a sentence combined with a specific word from that sentence. The sentence must contain natural language text. The word must be a single word from the provided sentence. ``` ## Output Rules [Section titled “Output Rules”](#output-rules) The [second transformation](https://github.com/microsoft/promptpex/blob/dev/src/prompts/generate_output_rules.prompty) takes the prompt under test and extracts the **output rules (OR)**. The output rules are a description of the output of the prompt. In this case, the output consists of a part of speech tag for the word. Output Rules ```text The output must return only the part of speech tag without any additional text or formatting. If the given word can be identified with one of the listed part of speech tags, the output must include only the specific tag for that word from the provided alphabetical list. If the given word cannot be tagged with any of the listed part of speech tags, the output should be the word "Unknown". If tagging the given word is not possible for any reason, the output should be the word "CantAnswer". ``` ## Inverse Output Rules [Section titled “Inverse Output Rules”](#inverse-output-rules) The [third transformation](https://github.com/microsoft/promptpex/blob/dev/src/prompts/generate_inverse_rules.prompty) takes the output rules and generates the **inverse output rules (IOR)**. The inverse output rules are a description of the output of the prompt that is the opposite of the output rules. In this case, the inverse output rules are a description of the output of the prompt that is the opposite of the output rules. Inverse Output Rules ```text The output must not return any additional text or formatting. The output must not include any of the listed part of speech tags. The output must not include the word "Unknown". The output must not include the word "CantAnswer". ``` ## Tests generated from the rules: [Section titled “Tests generated from the rules:”](#tests-generated-from-the-rules) From the input specification, output rules, inverse output rules, PromptPex uses a [LLM prompt](https://github.com/microsoft/promptpex/blob/dev/src/prompts/generate_tests.prompty) to generate a set of tests. ```text sentence: 'An aura of mystery surrounded them.', word: 'aura' sentence: 'The researchers documented carefully.', word: 'carefully' # Note this tests the Unknown corner case sentence: 'This is such a unique perspective.', word: 'such' ``` At this point, we have a set of inputs and predicted outputs that we can use in a variety of ways.

=|=|=|=|=|=

# Test Samples

It is possible to define test samples in the `testSamples` section of the YAML file. This section allows you to specify a list of test cases and expected output. The test samples are used in the test generation process to generate tests that mimic actual user input. ## Configuration [Section titled “Configuration”](#configuration) You can specify `testSamples` in the prompt frontmatter as an array of objects. ```yaml --- testSamples: - locale: "en-us" joke: "Why did the scarecrow win an award? Because he was outstanding in his field." output: "funny" - locale: "fr-FR" joke: "Pourquoi les plongeurs plongent-ils toujours en arrière et jamais en avant? Parce que sinon ils tombent dans le bateau." output: "funny" --- ``` ## Parameters [Section titled “Parameters”](#parameters) When invoking PromptPex, you can also provide filters to limit the number of test samples used in the generation: * `testSamplesCount`: The number of test samples to use in the generation. This is useful to limit the amount of test samples used in the generation. * `testSamplesShuffle`: Whether to shuffle the test samples before using them in the generation. This is useful to ensure that the test samples are not used in the same order every time.

=|=|=|=|=|=

# Transparency Note

> PromptPex is a tool designed to assist developers by exploring and testing AI model prompts.

# Responsible AI Transparency Note [Section titled “Responsible AI Transparency Note”](#responsible-ai-transparency-note) ## What is PromptPex? [Section titled “What is PromptPex?”](#what-is-promptpex) PromptPex (<https://github.com/microsoft/promptpex>) is an AI-driven tool developed to assist software developers in creating, managing, and testing prompts for large language models (LLMs). By treating prompts as functions, PromptPex automates the generation of test inputs and the extraction of functional specifications. It aims to enhance the efficiency and reliability of AI prompts, thereby improving their performance across various AI models. ## What can PromptPex do? [Section titled “What can PromptPex do?”](#what-can-promptpex-do) PromptPex provides a suite of functionalities including: * Automated rule extraction from natural language prompts. * Generation of diverse test cases to validate the performance of those prompts. * Support for testing prompts across multiple AI models, ensuring that developers understand how their inputs are interpreted and how outputs are generated. ## What is PromptPex’s intended use? [Section titled “What is PromptPex’s intended use?”](#what-is-promptpexs-intended-use) The intended use of PromptPex, as identified through its impact assessment, is to simplify the development of AI prompts by providing structured and automated tools for prompt evaluation and testing. This is aimed at improving the reliability of AI interactions and facilitating better integration of AI capabilities within software applications. The released version of PromptPex is an open-source library under active development and intended for use for research purposes. It should not be used in any downstream applications without additional detailed evaluation of robustness, safety issues and assessment of any potential harm or bias in the proposed application. ## How was PromptPex evaluated? What metrics are used to measure performance? [Section titled “How was PromptPex evaluated? What metrics are used to measure performance?”](#how-was-promptpex-evaluated-what-metrics-are-used-to-measure-performance) PromptPex undergoes rigorous evaluations focused on its ability to generate valid and diverse test cases that accurately reflect the rules defined by the prompts. Effectiveness is measured using metrics such as: * Percentage of non-compliant outputs generated during testing. * Groundedness of the rules extracted from prompts, which assesses whether the rules are adequately supported by the prompt descriptions. * Validity of the test cases generated, ensuring they are relevant and applicable to the prompts being tested. ## What are the limitations of PromptPex? How can users minimize the impact of PromptPex’s limitations when using the system? [Section titled “What are the limitations of PromptPex? How can users minimize the impact of PromptPex’s limitations when using the system?”](#what-are-the-limitations-of-promptpex-how-can-users-minimize-the-impact-of-promptpexs-limitations-when-using-the-system) Limitations include: * Dependence on the quality of initial prompt formulations, as vague or poorly defined prompts can lead to suboptimal performance. * Variance in rule extraction accuracy, especially with complex or idiomatic language. To minimize these impacts, users should focus on crafting clear and precise prompts, thoroughly test against a variety of input scenarios, and continually iterate on prompt designs as needed. ## What operational factors and settings allow for effective and responsible use of PromptPex? [Section titled “What operational factors and settings allow for effective and responsible use of PromptPex?”](#what-operational-factors-and-settings-allow-for-effective-and-responsible-use-of-promptpex) Effective and responsible use of PromptPex requires: * Understanding of the underlying AI models that will be used with the prompts. * Continuous updates and adaptations to align with advancements in AI technologies. * Active engagement with the developer community to share insights and gathered feedback for improvements. ## How do I provide feedback on PromptPex? [Section titled “How do I provide feedback on PromptPex?”](#how-do-i-provide-feedback-on-promptpex) We welcome feedback and collaboration from our audience. If you have suggestions, questions, or observe unexpected/offensive behavior in our technology, please contact us at: <jhalleux@microsoft.com>, <reshabh@cs.washington.edu>, <sbarke@microsoft.com>, <zorn@microsoft.com>. If the team receives reports of undesired behavior or identifies issues independently, we will update this repository with appropriate mitigations. ## Foundation model best practices [Section titled “Foundation model best practices”](#foundation-model-best-practices) We strongly encourage users to use LLMs/MLLMs that support robust Responsible AI mitigations, such as Azure Open AI (AOAI) services. Such services continually update their safety and RAI mitigations with the latest industry standards for responsible use. For more on AOAI’s best practices when employing foundations models for scripts and applications: * [Blog post on responsible AI features in AOAI that were presented at Ignite 2023](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/announcing-new-ai-safety-amp-responsible-ai-features-in-azure/ba-p/3983686) * [Overview of Responsible AI practices for Azure OpenAI models](https://learn.microsoft.com/en-us/legal/cognitive-services/openai/overview) * [Azure OpenAI Transparency Note](https://learn.microsoft.com/en-us/legal/cognitive-services/openai/transparency-note) * [OpenAI’s Usage policies](https://openai.com/policies/usage-policies) * [Azure OpenAI’s Code of Conduct](https://learn.microsoft.com/en-us/legal/cognitive-services/openai/code-of-conduct) “ ## What kinds of issues may arise when using PromptPex? [Section titled “What kinds of issues may arise when using PromptPex?”](#what-kinds-of-issues-may-arise-when-using-promptpex) Issues may arise from: * Inaccurate rule extraction, potentially leading to incorrect tests being generated. * Developers misapplying test cases without fully understanding their prompts’ nuances. * Unexpected results due to testing models that were not part of the initial PromptPex testing suite. To mitigate these issues, it is crucial to thoroughly educate users on the prompt crafting process and the importance of contextual understanding. Please see the related technical paper for more details. ## Learn more about PromptPex [Section titled “Learn more about PromptPex”](#learn-more-about-promptpex) For more information, visit the PromptPex project page on GitHub. Read the [PromptPex technical paper](http://arxiv.org/abs/2503.05070) for more details.