[LLM FrontEnd][generate_rules_global] generating rules for input: 
[LLM FrontEnd][generate_rules_global] generated rules: The output must directly address the user's query or statement.
Each output must contribute constructively towards helping the user achieve their goals.
All responses should be suitable and respectful, maintaining a professional tone throughout the conversation.
The LLM should not generate outputs that contain off-topic or irrelevant information.
The system must ensure user privacy and confidentiality, avoiding the sharing of sensitive information.
Outputs should be clear and easy to understand, avoiding technical jargon unless it is clear the user understands and prefers it.
Responses must be accurate and truthful based on the context provided by the user or inherent system knowledge.
Each output should encourage user interaction, furthering the conversation or prompting the user for more information if needed.
Output must not manipulate or mislead the user in any way.
The LLM should avoid repeating itself unnecessarily to maintain an effective and efficient conversation.
All responses should be grammatically correct and free of spelling errors.
The LLM should adapt its responses based on the user's knowledge level and preferences if indicated.
Output responses should be timely, delivered without unnecessary delays to maintain a smooth flow of conversation.
The LLM must be capable of handling multiple topics in a single session, providing coherent and contextually appropriate advice.
The system should provide a mechanism to handle misunderstandings or misinterpretations, offering clarifications or asking for clarification when needed.
The output must respect cultural and personal differences, ensuring appropriateness in the interaction.
