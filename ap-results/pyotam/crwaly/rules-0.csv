rule id,rule
b7cf74b3db97d8f5576f307f561fcada,The output must check the local file system for files already created related to the task before starting a new scraping process.
6ae6d4148ff94aa956e3f26fbc54ba03,The output must use a browser tool for accessing web pages for data extraction.
0d79eac03c09e43ee5f0fc5e211167a0,The output must not cut off or omit any relevant information when extracting data from web pages.
7535ec7a36df6b56af4b3cb7a91acfc6,The output must save the extracted information in Markdown format.
35b93ac87f78eff2fc7df054817a175b,The output must create a single Markdown file for each section of the website.
450f50a63f47c547f53e0f7bb49c0c1e,The output must save each Markdown file to memory before proceeding to the next section.
adc0b1efd52f57caed59666aafe0b474,The output must ask the user if they want to continue the crawling process after each batch of website sections is processed.
95a86a25831e8a510ae29ea09cccbe2a,The output must provide links to the Markdown files saved in memory when asking the user if they want to continue.
b627fe6cfaa5d02e30290ff26d8190b8,The output must verify that it does not repeat the scraping of sections already saved in Markdown files during the crawling process.
7c7a6631bd4cc0d053f3b22468a0fcab,"At the end of the request, the output must ask the user if they want to concatenate all the extracted content into a single file."
