---
title: Example Test Collection Review
description: How to review a collection of tests generated by PromptPex.
sidebar:
    order: 31
---

## Part 1: Potential Failures and Harm Analysis

### Possible Failures:

1. **Inaccurate Part-of-Speech Tagging**  
   - The chatbot misidentifies the part of speech for the target word in the sentence.  
   - **Harm Score:** Medium harm  
     - Incorrect tagging could lead to failure in downstream applications using this information (e.g., linguistic research, text parsing tools, or educational use).

2. **Ambiguous Words Misinterpretation**  
   - The chatbot fails to resolve ambiguity in cases where the word's part of speech varies depending on the context (e.g., noun vs. verb).  
   - **Harm Score:** Medium harm  
     - Ambiguities are common in linguistics, and failure to disambiguate can render the result less reliable.

3. **Failure for Foreign or Non-Standard Words**  
   - The chatbot improperly tags words like "Blippity" or "zyxt" instead of correctly returning "Unknown" or "CantAnswer."  
   - **Harm Score:** Low harm  
     - These rare scenarios are unlikely to severely impact practical applications but reflect an inability to follow the prompt's explicit rules.

4. **Failure to Respect Output Constraints**  
   - The chatbot's output violates the constraints by including extra text, punctuation, or invalid tags.  
   - **Harm Score:** High harm  
     - Violating rules damages trust in the model and could break downstream applications expecting precise formatted responses.

5. **Logical Failures for Repeated Target Word**  
   - The chatbot cannot process a sentence containing repeated appearances of the target word and erroneously produces an irrelevant response.  
   - **Harm Score:** Medium harm  
     - Users relying on correct tagging for linguistic data could distrust the model if such edge cases are mishandled.

6. **Internal Ambiguity Due to Empty or Ill-Structured Inputs**  
   - The chatbot fails to handle grammatically incoherent or ambiguous sentences not explicitly restricted by the input specification.  
   - **Harm Score:** Medium harm  
     - Models failing to process ill-structured or incomplete inputs consistently erode usability but rarely cause significant harm. 

7. **Output of Incorrect Allowed Response ("CantAnswer" vs. "Unknown")**  
   - The chatbot improperly chooses "CantAnswer" or "Unknown," reflecting a misunderstanding of when to apply these responses.  
   - **Harm Score:** Medium harm  
     - This undermines correctness but may not lead to immediate harmful consequences.

8. **Hallucination of Irrelevant or Off-Topic Responses**  
   - The chatbot generates an explanation, commentary, or any other response deviating from the single-tag constraint.  
   - **Harm Score:** High harm  
     - Such deviations break adherence to rules and could severely impact applications requiring precise and clean outputs.

9. **Handling Edge Cases for Symbols and Foreign Words**  
   - The chatbot misinterprets or fails to correctly tag words containing symbols, punctuations, or those with roots in foreign languages.  
   - **Harm Score:** Low harm  
     - This failure, although unlikely to broadly affect users, may decrease users' trust in the model for linguistically diverse data.

10. **Superficial or Simplistic Outputs Failing Edge Cases**  
    - Over-simplistic responses fail to deal with nuanced scenarios like comparative or superlative forms, causing errors for intricate linguistic contexts.  
    - **Harm Score:** Medium harm  
      - Failing at nuanced edge cases could alienate advanced users engaging deeply with linguistic models.

### Summary of Harm Scores:

- **No Harm:** None
- **Low Harm:** Failures on foreign/non-standard words, handling symbols, or rare words.
- **Medium Harm:** Failures in ambiguity resolution, logical issues for repeated words, edge cases, or incorrect "Unknown"/"CantAnswer."
- **High Harm:** Violating output formatting rules or hallucination of irrelevant responses.

---

## Part 2: Test Prioritization

| Test Description                                      | Importance         | Reason                                                                                                    |
|------------------------------------------------------ |------------------- |----------------------------------------------------------------------------------------------------------|
| `The apple is on the table. apple`                   | High importance    | Tests basic noun tagging, one of the most foundational aspects of the prompt.                           |
| `He runs quickly. quickly`                           | High importance    | Checks adverb tagging (RB), crucial for handling common modifiers.                                       |
| `Blippity blop is a strange term. blop`              | Medium importance  | Tests rare or made-up words, requiring the chatbot to return "Unknown" accurately.                       |
| `The meaning of 'zyxt' puzzles everyone. zyxt`       | Medium importance  | Focuses on handling rare foreign-like or archaic words, ensuring "Unknown" functionality.                |
| `She has a beautiful house. beautiful`               | High importance    | Validates the ability to identify adjectives (JJ) within context.                                        |
| `The runner trained daily. daily`                    | High importance    | Addresses adverb vs. noun ambiguity (RB vs. NN), highlighting disambiguation capabilities.               |
| `Music relaxes the soul. Music`                      | High importance    | Ensures proper noun (NN) tagging for capitalized common nouns.                                           |
| `Glorf is a mystery. Glorf`                          | Medium importance  | Similar to rare words and tests "Unknown" functionality for made-up terms explicitly.                    |
| `The usage of 'quipz' is rare. quipz`                | Low importance     | Redundant with other "rare word" tests like "zyxt" or "blop"; Low priority unless others fail.            |
| `Books are useful resources. Books`                  | Medium importance  | Tests plural noun tagging (NNS), significant but less critical than basic noun tagging.                  |

---

## Part 3: Quality of Tests in `<TESTS>`

### Evaluation:

- **Strengths:** 
  - Tests cover a range of possible inputs, including common nouns, adjectives, adverbs, and rare or non-standard words.
  - Explicit handling of edge cases such as rare words ("blop," "zyxt," "quipz").
  - The variations ensure tests probe both tagging capabilities and adherence to input/output formatting rules.
- **Weaknesses:**
  - Some redundancy exists among tests for rare words ("blop," "zyxt," "quipz"), which could be consolidated.
  - The absence of explicit edge-case tests for punctuation or symbols may limit robustness evaluation.
  - The current set lacks intentional tests to evaluate chatbot behavior under ill-structured or ambiguous input sentences.

### Final Score: **8/10**  
This is a strong set of tests effectively targeting core functionalities and many edge cases. Improvements could involve more diverse edge cases (e.g., punctuation) and removal of redundancies to enhance test efficiency.