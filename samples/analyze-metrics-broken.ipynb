{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "benchmarkVersion = \"test-all-2025-09-18-28tests\"\n",
    "\n",
    "rootDir = \"/workspaces/promptpex/\"\n",
    "\n",
    "if not os.path.isdir(rootDir):\n",
    "    rootDir = \"..\"\n",
    "\n",
    "evalsDir = f'{rootDir}/evals/{benchmarkVersion}/'\n",
    "\n",
    "\n",
    "benchmarks = [\n",
    "    \"speech-tag\"\n",
    "]\n",
    "\n",
    "# full list\n",
    "benchmarks = [ \"speech-tag\", \"classify-input-text\", \"text-to-p\",  \"sentence-rewrite\", \"extract-names\", \"elements\", \"art-prompt\", \"shakespearean-writing-assistant\"]\n",
    "\n",
    "\n",
    "benchmarks = [\n",
    "    \"bayesian_games_29_7\",\n",
    "    \"bullet_journaling_145_1\",\n",
    "    \"canopy_management_298_8\",\n",
    "    \"fancy_title_generator\",\n",
    "    \"hearing_impairments_124_7\",\n",
    "    \"housing_market_dynamics_338_1\",\n",
    "    \"initial_public_offerings_ipos_70_9\",\n",
    "    \"news_broadcasting_693_9\",\n",
    "    # \"prompt_generator\",\n",
    "    \"real_time_analytics_609_2\",\n",
    "    \"recruiter\",\n",
    "    \"restaurant_owner\",\n",
    "    \"sewing_951_7\",\n",
    "    \"solr_search_engine\",\n",
    "    \"speaker_identification_595_2\",\n",
    "    \"startup_idea_generator\",\n",
    "    \"tea_taster\",\n",
    "    \"virtual_fitness_coach\",\n",
    "    \"yes_or_no_answer\",\n",
    "    \"speech-tag\", \"classify-input-text\", \"text-to-p\",  \"sentence-rewrite\", \"extract-names\", \"elements\", \"art-prompt\", \"shakespearean-writing-assistant\",\n",
    "]\n",
    "\n",
    "prettyBenchmarkNames = { \"speech-tag\": \"speech-tag\", \n",
    "                \"text-to-p\": \"text-to-p\",  \n",
    "                \"shakespearean-writing-assistant\": \"shakespeare\", \n",
    "                \"sentence-rewrite\": \"sentence\", \n",
    "                \"extract-names\": \"extract-names\", \n",
    "                \"elements\":\"elements\", \n",
    "                \"art-prompt\": \"art-prompt\", \n",
    "                \"classify-input-text\": \"classify\"}\n",
    "\n",
    "prettyMetrics = { \"tests compliant\": \"prompt ok/err\", \n",
    "                \"system_compliant\": \"prompt only\",  \n",
    "                \"rules_system_with_input_compliant\": \"prompt/rule/input\"\n",
    "}\n",
    "\n",
    "def parse_metric(val):\n",
    "    \"\"\"Convert metric to float, handle %, NaN, and '--'.\"\"\"\n",
    "    if isinstance(val, str):\n",
    "        val = val.strip()\n",
    "        if val.endswith('%'):\n",
    "            try:\n",
    "                return float(val.strip('%'))\n",
    "            except:\n",
    "                return 0.0\n",
    "        if val in ('NaN', '--', ''):\n",
    "            return 0.0\n",
    "    try:\n",
    "        v = float(val)\n",
    "        if np.isnan(v):\n",
    "            return 0.0\n",
    "        return v\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def get_metrics_start_col(df):\n",
    "    \"\"\"Get the start column index for metrics (after 'tests valid compliant').\"\"\"\n",
    "    df.columns = df.columns.str.strip()  # Strip whitespace from column names\n",
    "    \n",
    "    if 'tests valid compliant' in df.columns:\n",
    "        return df.columns.get_loc('tests valid') + 2\n",
    "    elif 'tests negative' in df.columns:\n",
    "        return df.columns.get_loc('tests negative compliant') + 2\n",
    "    elif 'tests positive' in df.columns:\n",
    "        return df.columns.get_loc('tests positive') + 2\n",
    "    else:\n",
    "        # Fallback: assume metrics start after 'model' column\n",
    "        return df.columns.get_loc('model') + 2 if 'model' in df.columns else 0\n",
    "\n",
    "    \n",
    "def analyze_benchmark_metrics(benchmark, evalsDir, prettyBenchmarkNames):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Build the path to the overview.csv for the given benchmark\n",
    "    csv_path = os.path.join(evalsDir, benchmark, benchmark, \"overview.csv\")\n",
    "\n",
    "    # Read the CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Get the start column for metrics\n",
    "    start_col = get_metrics_start_col(df)\n",
    "\n",
    "    # Include \"tests compliant\" as a metric (ensure it's included)\n",
    "    metrics = list(df.columns[start_col:])\n",
    "\n",
    "    # Extract model names and metrics columns\n",
    "    models = df['model']\n",
    "    metrics_table = df[['model'] + metrics].copy()\n",
    "\n",
    "    for metric in metrics:\n",
    "        metrics_table[metric] = metrics_table[metric].apply(parse_metric)\n",
    "\n",
    "    print(f\"Metrics by Model for benchmark '{benchmark}':\")\n",
    "    print(metrics_table.to_string(index=False))\n",
    "\n",
    "    # Plot grouped bar chart\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.8 / len(metrics)  # total width for all bars per group\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax.bar(x + i*width, metrics_table[metric], width, label=metric)\n",
    "\n",
    "    ax.set_xticks(x + width*(len(metrics)-1)/2)\n",
    "    ax.set_xticklabels(models, rotation=20)\n",
    "    ax.set_ylabel('Metric Value')\n",
    "    ax.set_title(f\"Model Metrics for {prettyBenchmarkNames.get(benchmark, benchmark)}\")\n",
    "    ax.legend(loc='best', fontsize='small', ncol=2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "for benchmark in benchmarks:\n",
    "    # Call the function to analyze and plot metrics for each benchmark\n",
    "    print(f\"Analyzing metrics for benchmark: {benchmark}\")\n",
    "    analyze_benchmark_metrics(benchmark, evalsDir, prettyBenchmarkNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collect_metrics(benchmarks, evalsDir):\n",
    "    # Data structure: {benchmark: {model: {metric: value}}}\n",
    "    all_data = {}\n",
    "    all_metrics = set()\n",
    "    all_models = set()\n",
    "\n",
    "    for benchmark in benchmarks:\n",
    "        csv_path = os.path.join(evalsDir, benchmark, benchmark, \"overview.csv\")\n",
    "        if not os.path.isfile(csv_path):\n",
    "            print(f\"Warning: {csv_path} not found, skipping.\")\n",
    "            continue\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Get the start column for metrics\n",
    "        start_col = get_metrics_start_col(df)\n",
    "\n",
    "        # Include \"tests compliant\" as a metric (ensure it's included)\n",
    "        metrics = list(df.columns[start_col:])\n",
    "\n",
    "        all_metrics.update(metrics)\n",
    "        all_data[benchmark] = {}\n",
    "        for _, row in df.iterrows():\n",
    "            model = row['model']\n",
    "            all_models.add(model)\n",
    "            all_data[benchmark].setdefault(model, {})\n",
    "            for metric in metrics:\n",
    "                all_data[benchmark][model][metric] = parse_metric(row[metric])\n",
    "    return all_data, sorted(all_models), sorted(all_metrics)\n",
    "\n",
    "def compute_model_metric_averages(all_data, all_models, all_metrics):\n",
    "    # {model: {metric: [values...]}}\n",
    "    model_metric_values = {model: {metric: [] for metric in all_metrics} for model in all_models}\n",
    "    for benchmark in all_data:\n",
    "        for model in all_models:\n",
    "            model_metrics = all_data[benchmark].get(model, {})\n",
    "            for metric in all_metrics:\n",
    "                val = model_metrics.get(metric, 0.0)\n",
    "                model_metric_values[model][metric].append(val)\n",
    "    # Compute averages\n",
    "    model_metric_avg = {model: {metric: np.mean(vals) for metric, vals in metrics.items()} for model, metrics in model_metric_values.items()}\n",
    "    return model_metric_avg\n",
    "\n",
    "def print_metric_table(model_metric_avg):\n",
    "    models = list(model_metric_avg.keys())\n",
    "    metrics = list(next(iter(model_metric_avg.values())).keys())\n",
    "    print(\"Average Metrics by Model:\")\n",
    "    header = [\"Model\"] + [prettyMetrics.get(m, m) for m in metrics]\n",
    "    \n",
    "    print(\"\\t\".join(header))\n",
    "    for model in models:\n",
    "        row = [model] + [f\"{model_metric_avg[model][metric]:.2f}\" for metric in metrics]\n",
    "        print(\"\\t\".join(row))\n",
    "\n",
    "def plot_grouped_bar_chart(model_metric_avg):\n",
    "    models = list(model_metric_avg.keys())\n",
    "    metrics = list(next(iter(model_metric_avg.values())).keys())\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.8 / len(metrics)\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    for i, metric in enumerate(metrics):\n",
    "        values = [model_metric_avg[model][metric] for model in models]\n",
    "        ax.bar(x + i*width, values, width, label=metric)\n",
    "    ax.set_xticks(x + width*(len(metrics)-1)/2)\n",
    "    ax.set_xticklabels(models, rotation=20)\n",
    "    ax.set_ylabel('Average Metric Value')\n",
    "    ax.set_title('Average Model Metrics Across Benchmarks')\n",
    "    ax.legend(loc='best', fontsize='small', ncol=2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "all_data, all_models, all_metrics = collect_metrics(benchmarks, evalsDir)\n",
    "model_metric_avg = compute_model_metric_averages(all_data, all_models, all_metrics)\n",
    "print_metric_table(model_metric_avg)\n",
    "plot_grouped_bar_chart(model_metric_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_benchmark_model_metrics_table(benchmarks, evalsDir, columns_of_interest):\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    width = 18\n",
    "    def fit(val):\n",
    "        s = str(val)\n",
    "        return s[:width].ljust(width)[:width] \n",
    "    \n",
    "    header = [\"Benchmark\", \"Model\"] + [\n",
    "        prettyMetrics.get(col, col) for col in columns_of_interest\n",
    "    ]\n",
    "\n",
    "    print(\"\".join([fit(h) for h in header]))\n",
    "    for benchmark in benchmarks:\n",
    "        csv_path = os.path.join(evalsDir, benchmark, benchmark, \"overview.csv\")\n",
    "        if not os.path.isfile(csv_path):\n",
    "            continue\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            model = row['model']\n",
    "            values = []\n",
    "            for col in columns_of_interest:\n",
    "                val0 = row.get(col, 0)\n",
    "                val = parse_metric(val0)\n",
    "                values.append(f\"{val:.2f}\")\n",
    "            print(\"\".join([fit(benchmark), fit(model)] + [fit(v) for v in values]))\n",
    "\n",
    "# read csv for first benchmark\n",
    "csv_path = os.path.join(evalsDir, benchmarks[0], benchmarks[0], \"overview.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Get the start column for metrics\n",
    "start_col = get_metrics_start_col(df)\n",
    "\n",
    "# columns_of_interest = [\"tests\", \"tests compliant\", \"errors\", \"tests compliance unknown\"] + list(df.columns[start_col:])\n",
    "columns_of_interest = list(df.columns[start_col:])\n",
    "\n",
    "print_benchmark_model_metrics_table(benchmarks, evalsDir, columns_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def collect_and_sum_benchmark_metrics(benchmarks, evalsDir, columns_of_interest):\n",
    "    # data[benchmark][model][column]\n",
    "    data = {}\n",
    "    sums = {bench: {col: 0.0 for col in columns_of_interest} for bench in benchmarks}\n",
    "    for benchmark in benchmarks:\n",
    "        csv_path = os.path.join(evalsDir, benchmark, benchmark, \"overview.csv\")\n",
    "        if not os.path.isfile(csv_path):\n",
    "            print(f\"Warning: {csv_path} not found, skipping.\")\n",
    "            continue\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df.columns = df.columns.str.strip()  # Strip whitespace from column names\n",
    "        data[benchmark] = {}\n",
    "        for _, row in df.iterrows():\n",
    "            model = row['model']\n",
    "            data[benchmark][model] = {}\n",
    "            for col in columns_of_interest:\n",
    "                val = parse_metric(row.get(col, 0))\n",
    "                data[benchmark][model][col] = val\n",
    "                sums[benchmark][col] += val\n",
    "    return data, sums\n",
    "\n",
    "def print_sums_table(sums, columns_of_interest):\n",
    "    print(\"Benchmark\\t\" + \"\\t\".join(columns_of_interest))\n",
    "    for bench, colvals in sums.items():\n",
    "        row = [bench] + [f\"{colvals[col]:.2f}\" for col in columns_of_interest]\n",
    "        print(\"\\t\".join(row))\n",
    "\n",
    "def plot_sums_bar(sums, columns_of_interest):\n",
    "    benchmarks = list(sums.keys())\n",
    "    for col in columns_of_interest:\n",
    "        values = [sums[bench][col] for bench in benchmarks]\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.bar(benchmarks, values)\n",
    "        plt.ylabel(col)\n",
    "        plt.title(f\"Sum of {col} by Benchmark\")\n",
    "        plt.xticks(rotation=20)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# columns_of_interest = [\"errors\", \"tests compliance unknown\"]\n",
    "# Get the start column for metrics\n",
    "start_col = get_metrics_start_col(df)\n",
    "\n",
    "# columns_of_interest = [\"tests\", \"tests compliant\", \"errors\", \"tests compliance unknown\"] + list(df.columns[start_col:])\n",
    "columns_of_interest = list(df.columns[start_col:])\n",
    "\n",
    "data, sums = collect_and_sum_benchmark_metrics(benchmarks, evalsDir, columns_of_interest)\n",
    "print_sums_table(sums, columns_of_interest)\n",
    "plot_sums_bar(sums, columns_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def average_tests_per_model(benchmarks, evalsDir):\n",
    "    averages = {}\n",
    "    for benchmark in benchmarks:\n",
    "        csv_path = os.path.join(evalsDir, benchmark, benchmark, \"overview.csv\")\n",
    "        if not os.path.isfile(csv_path):\n",
    "            print(f\"Warning: {csv_path} not found, skipping.\")\n",
    "            continue\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        # Parse the 'tests' column for all models\n",
    "        tests = df['tests'].apply(parse_metric)\n",
    "        if len(tests) > 0:\n",
    "            avg = np.mean(tests)\n",
    "        else:\n",
    "            avg = 0.0\n",
    "        averages[benchmark] = avg\n",
    "    return averages\n",
    "\n",
    "def print_avg_table(averages):\n",
    "    print(\"Benchmark\\tAverage Tests per Model\")\n",
    "    for bench, avg in averages.items():\n",
    "        print(f\"{bench}\\t{avg:.2f}\")\n",
    "\n",
    "def plot_avg_bar(averages):\n",
    "    benchmarks = list(averages.keys())\n",
    "    values = list(averages.values())\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(benchmarks, values)\n",
    "    plt.ylabel(\"Average Tests per Model\")\n",
    "    plt.title(\"Average Tests per Model by Benchmark\")\n",
    "    plt.xticks(rotation=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "averages = average_tests_per_model(benchmarks, evalsDir)\n",
    "print_avg_table(averages)\n",
    "plot_avg_bar(averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grouped_barplot_by_benchmark_and_model(benchmarks, evalsDir, column_of_interest):\n",
    "    \"\"\"\n",
    "    Create a grouped barplot showing a specific column as a function of benchmark and model.\n",
    "    Groups are benchmarks, bars within groups are models.\n",
    "    Includes an additional \"Average\" group showing averages across benchmarks for each model.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Data structure: {benchmark: {model: value}}\n",
    "    data = {}\n",
    "    all_models = set()\n",
    "    \n",
    "    for benchmark in benchmarks:\n",
    "        csv_path = os.path.join(evalsDir, benchmark, benchmark, \"overview.csv\")\n",
    "        if not os.path.isfile(csv_path):\n",
    "            print(f\"Warning: {csv_path} not found, skipping.\")\n",
    "            continue\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        \n",
    "        if \"model\" not in df.columns or column_of_interest not in df.columns:\n",
    "            print(f\"Warning: Required columns not found in {csv_path}, skipping.\")\n",
    "            continue\n",
    "            \n",
    "        data[benchmark] = {}\n",
    "        for _, row in df.iterrows():\n",
    "            model = row[\"model\"]\n",
    "            val = parse_metric(row[column_of_interest])\n",
    "            data[benchmark][model] = val\n",
    "            all_models.add(model)\n",
    "    \n",
    "    all_models = sorted(all_models)\n",
    "    benchmarks_with_data = [b for b in benchmarks if b in data]\n",
    "    \n",
    "    if not benchmarks_with_data or not all_models:\n",
    "        print(\"No data found for plotting.\")\n",
    "        return\n",
    "    \n",
    "    # Build data matrix: rows=benchmarks, columns=models\n",
    "    values = []\n",
    "    for benchmark in benchmarks_with_data:\n",
    "        row = []\n",
    "        for model in all_models:\n",
    "            row.append(data.get(benchmark, {}).get(model, 0.0))\n",
    "        values.append(row)\n",
    "    values = np.array(values)  # shape: (num_benchmarks, num_models)\n",
    "    \n",
    "    # Calculate averages across benchmarks for each model\n",
    "    model_averages = []\n",
    "    for i, model in enumerate(all_models):\n",
    "        # Get values for this model across all benchmarks\n",
    "        model_values = values[:, i]\n",
    "        # Only average non-zero values (or all values if you prefer)\n",
    "        avg = np.mean(model_values)\n",
    "        model_averages.append(avg)\n",
    "    \n",
    "    # Add the average row to the data\n",
    "    all_values = np.vstack([values, model_averages])\n",
    "    all_labels = benchmarks_with_data + [\"Average\"]\n",
    "    \n",
    "    # Create the grouped bar plot\n",
    "    x = np.arange(len(all_labels))  # positions for all groups including average\n",
    "    width = 0.8 / len(all_models)  # width of individual bars\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(18, 8))\n",
    "    \n",
    "    # Create bars for each model\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(all_models)))  # Use distinct colors\n",
    "    for i, model in enumerate(all_models):\n",
    "        offset = (i - (len(all_models) - 1) / 2) * width\n",
    "        bars = ax.bar(x + offset, all_values[:, i], width, label=model, alpha=0.8, color=colors[i])\n",
    "        \n",
    "        # Highlight the average bars with different styling\n",
    "        if len(bars) > len(benchmarks_with_data):\n",
    "            bars[-1].set_alpha(1.0)  # Make average bar more opaque\n",
    "            bars[-1].set_edgecolor('black')  # Add black border to average bar\n",
    "            bars[-1].set_linewidth(2)\n",
    "    \n",
    "    # Add a vertical separator line before the average group\n",
    "    if len(all_labels) > 1:\n",
    "        separator_x = len(benchmarks_with_data) - 0.5\n",
    "        ax.axvline(x=separator_x, color='gray', linestyle='--', alpha=0.7, linewidth=1)\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_xlabel('Benchmark')\n",
    "    ax.set_ylabel(column_of_interest)\n",
    "    ax.set_title(f'{column_of_interest} by Benchmark and Model (with Cross-Benchmark Averages)')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(all_labels, rotation=45, ha='right')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add text annotation for the average section\n",
    "    if len(all_labels) > 1:\n",
    "        ax.text(len(benchmarks_with_data), ax.get_ylim()[1] * 0.95, 'Cross-Benchmark\\nAverages', \n",
    "                ha='center', va='top', fontsize=10, style='italic', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage with a specific column\n",
    "# You can change this to any column that exists in your data\n",
    "column_to_plot = \"tests compliant\"\n",
    "\n",
    "# First check what columns are available in the first benchmark\n",
    "csv_path = os.path.join(evalsDir, benchmarks[0], benchmarks[0], \"overview.csv\")\n",
    "if os.path.isfile(csv_path):\n",
    "    df_sample = pd.read_csv(csv_path)\n",
    "    df_sample.columns = df_sample.columns.str.strip()\n",
    "    print(\"Available columns:\")\n",
    "    print(df_sample.columns.tolist())\n",
    "    print()\n",
    "    \n",
    "    # Try to find a good column to plot\n",
    "    if \"tests compliant\" in df_sample.columns:\n",
    "        column_to_plot = \"tests compliant\"\n",
    "    elif \"accuracy with eval\" in df_sample.columns:\n",
    "        column_to_plot = \"accuracy with eval\"\n",
    "    else:\n",
    "        # Use the first metric column after standard columns\n",
    "        start_col = get_metrics_start_col(df_sample)\n",
    "        \n",
    "        if start_col < len(df_sample.columns):\n",
    "            column_to_plot = df_sample.columns[start_col]\n",
    "\n",
    "print(f\"Plotting column: {column_to_plot}\")\n",
    "plot_grouped_barplot_by_benchmark_and_model(benchmarks, evalsDir, column_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_baseline_vs_main_metrics(benchmark, evalsDir, prettyBenchmarkNames=None):\n",
    "    \"\"\"\n",
    "    Compare metrics between overview.csv and overview-baseline.csv files for a given benchmark.\n",
    "    Creates side-by-side bar charts and difference analysis.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Build paths to both CSV files\n",
    "    main_csv_path = os.path.join(evalsDir, benchmark, benchmark, \"overview.csv\")\n",
    "    baseline_csv_path = os.path.join(evalsDir, benchmark, benchmark, \"overview-baseline.csv\")\n",
    "    \n",
    "    # Check if both files exist\n",
    "    if not os.path.isfile(main_csv_path):\n",
    "        print(f\"Warning: Main file {main_csv_path} not found, skipping.\")\n",
    "        return\n",
    "    if not os.path.isfile(baseline_csv_path):\n",
    "        print(f\"Warning: Baseline file {baseline_csv_path} not found, skipping.\")\n",
    "        return\n",
    "    \n",
    "    # Read both CSV files\n",
    "    main_df = pd.read_csv(main_csv_path)\n",
    "    baseline_df = pd.read_csv(baseline_csv_path)\n",
    "    \n",
    "    # Strip whitespace from column names\n",
    "    main_df.columns = main_df.columns.str.strip()\n",
    "    baseline_df.columns = baseline_df.columns.str.strip()\n",
    "    \n",
    "    # Find common models\n",
    "    main_models = set(main_df['model'])\n",
    "    baseline_models = set(baseline_df['model'])\n",
    "    common_models = sorted(main_models.intersection(baseline_models))\n",
    "    \n",
    "    if not common_models:\n",
    "        print(f\"No common models found between main and baseline for benchmark '{benchmark}'\")\n",
    "        return\n",
    "    \n",
    "    # Find common metric columns (excluding 'model' and other non-metric columns)\n",
    "    main_start_col = get_metrics_start_col(main_df)\n",
    "    baseline_start_col = get_metrics_start_col(baseline_df)\n",
    "    \n",
    "    main_metrics = set(main_df.columns[main_start_col:])\n",
    "    baseline_metrics = set(baseline_df.columns[baseline_start_col:])\n",
    "    common_metrics = sorted(main_metrics.intersection(baseline_metrics))\n",
    "    \n",
    "    if not common_metrics:\n",
    "        print(f\"No common metrics found between main and baseline for benchmark '{benchmark}'\")\n",
    "        print(f\"Main metrics: {list(main_metrics)}\")\n",
    "        print(f\"Baseline metrics: {list(baseline_metrics)}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Comparing benchmark '{benchmark}' with {len(common_models)} models and {len(common_metrics)} metrics\")\n",
    "    print(f\"Common models: {common_models}\")\n",
    "    print(f\"Common metrics: {common_metrics}\")\n",
    "    \n",
    "    # Prepare data for comparison\n",
    "    comparison_data = {}\n",
    "    for model in common_models:\n",
    "        main_row = main_df[main_df['model'] == model].iloc[0] if len(main_df[main_df['model'] == model]) > 0 else None\n",
    "        baseline_row = baseline_df[baseline_df['model'] == model].iloc[0] if len(baseline_df[baseline_df['model'] == model]) > 0 else None\n",
    "        \n",
    "        if main_row is not None and baseline_row is not None:\n",
    "            comparison_data[model] = {\n",
    "                'main': {metric: parse_metric(main_row[metric]) for metric in common_metrics},\n",
    "                'baseline': {metric: parse_metric(baseline_row[metric]) for metric in common_metrics}\n",
    "            }\n",
    "    \n",
    "    # Create side-by-side comparison plots\n",
    "    n_metrics = len(common_metrics)\n",
    "    n_models = len(comparison_data)\n",
    "    \n",
    "    if n_metrics == 0 or n_models == 0:\n",
    "        print(\"No data to plot\")\n",
    "        return\n",
    "    \n",
    "    # Create subplots for each metric\n",
    "    fig, axes = plt.subplots(n_metrics, 1, figsize=(12, 4 * n_metrics))\n",
    "    if n_metrics == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, metric in enumerate(common_metrics):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        models = list(comparison_data.keys())\n",
    "        main_values = [comparison_data[model]['main'][metric] for model in models]\n",
    "        baseline_values = [comparison_data[model]['baseline'][metric] for model in models]\n",
    "        \n",
    "        x = np.arange(len(models))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax.bar(x - width/2, baseline_values, width, label='Baseline', alpha=0.8, color='lightcoral')\n",
    "        bars2 = ax.bar(x + width/2, main_values, width, label='Main', alpha=0.8, color='skyblue')\n",
    "        \n",
    "        ax.set_xlabel('Model')\n",
    "        ax.set_ylabel(metric)\n",
    "        ax.set_title(f'{metric} - Baseline vs Main ({prettyBenchmarkNames.get(benchmark, benchmark) if prettyBenchmarkNames else benchmark})')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars1:\n",
    "            height = bar.get_height()\n",
    "            ax.annotate(f'{height:.1f}',\n",
    "                       xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                       xytext=(0, 3),  # 3 points vertical offset\n",
    "                       textcoords=\"offset points\",\n",
    "                       ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        for bar in bars2:\n",
    "            height = bar.get_height()\n",
    "            ax.annotate(f'{height:.1f}',\n",
    "                       xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                       xytext=(0, 3),  # 3 points vertical offset\n",
    "                       textcoords=\"offset points\",\n",
    "                       ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create difference analysis plot\n",
    "    fig, axes = plt.subplots(n_metrics, 1, figsize=(12, 4 * n_metrics))\n",
    "    if n_metrics == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, metric in enumerate(common_metrics):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        models = list(comparison_data.keys())\n",
    "        differences = [comparison_data[model]['main'][metric] - comparison_data[model]['baseline'][metric] for model in models]\n",
    "        \n",
    "        # Color bars based on positive/negative change\n",
    "        colors = ['green' if diff > 0 else 'red' if diff < 0 else 'gray' for diff in differences]\n",
    "        \n",
    "        bars = ax.bar(models, differences, color=colors, alpha=0.7)\n",
    "        \n",
    "        ax.set_xlabel('Model')\n",
    "        ax.set_ylabel(f'{metric} Difference (Main - Baseline)')\n",
    "        ax.set_title(f'{metric} - Improvement Analysis ({prettyBenchmarkNames.get(benchmark, benchmark) if prettyBenchmarkNames else benchmark})')\n",
    "        ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "        ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, diff in zip(bars, differences):\n",
    "            height = bar.get_height()\n",
    "            ax.annotate(f'{diff:+.1f}',\n",
    "                       xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                       xytext=(0, 3 if height >= 0 else -15),\n",
    "                       textcoords=\"offset points\",\n",
    "                       ha='center', va='bottom' if height >= 0 else 'top', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary table\n",
    "    print(f\"\\nDetailed Comparison for '{benchmark}':\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    header = [\"Model\"] + [f\"{m} (B)\" for m in common_metrics] + [f\"{m} (M)\" for m in common_metrics] + [f\"{m} (Î”)\" for m in common_metrics]\n",
    "    print(\"\\t\".join([h[:10] for h in header]))\n",
    "    \n",
    "    for model in comparison_data:\n",
    "        row = [model[:10]]\n",
    "        # Baseline values\n",
    "        row.extend([f\"{comparison_data[model]['baseline'][metric]:.1f}\" for metric in common_metrics])\n",
    "        # Main values\n",
    "        row.extend([f\"{comparison_data[model]['main'][metric]:.1f}\" for metric in common_metrics])\n",
    "        # Differences\n",
    "        row.extend([f\"{comparison_data[model]['main'][metric] - comparison_data[model]['baseline'][metric]:+.1f}\" for metric in common_metrics])\n",
    "        print(\"\\t\".join(row))\n",
    "\n",
    "def analyze_all_baseline_comparisons(benchmarks, evalsDir, prettyBenchmarkNames=None):\n",
    "    \"\"\"\n",
    "    Run baseline vs main comparison for all benchmarks that have both files.\n",
    "    \"\"\"\n",
    "    for benchmark in benchmarks:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Analyzing baseline comparison for: {benchmark}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        try:\n",
    "            compare_baseline_vs_main_metrics(benchmark, evalsDir, prettyBenchmarkNames)\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing {benchmark}: {str(e)}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the baseline comparison with art-prompt benchmark\n",
    "print(\"Testing baseline comparison analysis with art-prompt benchmark:\")\n",
    "compare_baseline_vs_main_metrics(\"art-prompt\", evalsDir, prettyBenchmarkNames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
