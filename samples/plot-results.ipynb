{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from webbrowser import BaseBrowser\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "benchmarkVersion = \"test-all-2025-09-29-paper\"\n",
    "\n",
    "rootDir = \"/home/zorn/promptpex\"\n",
    "\n",
    "if not os.path.isdir(rootDir):\n",
    "    rootDir = \"..\"\n",
    "\n",
    "evalsDir = f'{rootDir}/evals/{benchmarkVersion}/eval'\n",
    "\n",
    "benchmarks = [ \"speech-tag\"]\n",
    "\n",
    "# full set\n",
    "benchmarks = [ \"speech-tag\", \"classify-input-text\", \"text-to-p\",  \"sentence-rewrite\", \"extract-names\", \"elements\", \"art-prompt\", \"shakespearean-writing-assistant\"]\n",
    "# benchmarks = [ \"speech-tag\", \"text-to-p\",  \"sentence-rewrite\", \"extract-names\", \"elements\", \"art-prompt\", \"shakespearean-writing-assistant\"]\n",
    "\n",
    "# benchmarks = [ \"speech-tag\", \"classify-input-text\", \"text-to-p\",  \"sentence-rewrite\", \"elements\", \"art-prompt\", \"shakespearean-writing-assistant\"]\n",
    "\n",
    "\n",
    "#benchmarks = [ \"speech-tag\", \"text-to-p\", \"elements\", \"extract-names\", ]\n",
    "\n",
    "#benchmarks = [  \"speech-tag\", \"text-to-p\",  ]\n",
    "\n",
    "#benchmarks = [ \"speech-tag\",  ]\n",
    "\n",
    "#benchmarks = [ \"speech-tag\", \"text-to-p\",  \"sentence-rewrite\", \"extract-names\", \"elements\", \"art-prompt\",  \"shakespearean-writing-assistant\"]\n",
    "\n",
    "#benchmarks = [ \"speech-tag\", \"text-to-p\", \"sentence-rewrite\", \"extract-names\", \"elements\", \"art-prompt\", \"classify-input-text\"   ]\n",
    "# benchmarks = [ \"speech-tag\", \"text-to-p\", \"sentence-rewrite\", \"extract-names\", \"elements\",    ]\n",
    "#benchmarks = [ \"speech-tag\", \"text-to-p\", ]\n",
    "\n",
    "\n",
    "prettyNames = { \"speech-tag\": \"speech-tag\", \n",
    "                \"text-to-p\": \"text-to-p\",  \n",
    "                \"shakespearean-writing-assistant\": \"shakespeare\", \n",
    "                \"sentence-rewrite\": \"sentence\", \n",
    "                \"extract-names\": \"extract-names\", \n",
    "                \"elements\":\"elements\", \n",
    "                \"art-prompt\": \"art-prompt\", \n",
    "                \"classify-input-text\": \"classify\"}\n",
    "\n",
    "\n",
    "benchmarks = [ \"speech-tag\", \"classify-input-text\", \"text-to-p\",  \"sentence-rewrite\", \"extract-names\", \"elements\", \"art-prompt\", \n",
    "\"sewing_951_7\",\n",
    "\"hearing_impairments_124_7\",\n",
    "\"speaker_identification_595_2\",\n",
    "\"real_time_analytics_609_2\",\n",
    "\"housing_market_dynamics_338_1\",]\n",
    "\n",
    "benchmarks = [ \"speech-tag\",  ]\n",
    "\n",
    "benchmarks = [\n",
    "\"crewai_assistant_qqtuuwsby\",\n",
    "\"fragrance_finder_deluxe_e9avvjxcw\",\n",
    "\"information_kiosk_building_j6ry5iscb\",\n",
    "\"thread_weaver_krog0f5tg\",\n",
    "\"hurtig_ingeni_r_pgktzdcfk\",\n",
    "\n",
    "    \"speech-tag\",\n",
    "    \"text-to-p\",\n",
    "    \"elements\",\n",
    "    \"art-prompt\",\n",
    "    \"extract-names\",\n",
    "    \"classify-input-text\",\n",
    "    \"sentence-rewrite\",\n",
    "    \"shakespearean-writing-assistant\",\n",
    "\"architect_guide_for_programmers\",\n",
    "\"unconstrained_ai_model_dan\",\n",
    "\"idea_clarifier_gpt\",\n",
    "\"structured_iterative_reasoning_protocol_sirp\",\n",
    "#\"tech_challenged_customer\",\n",
    "\n",
    "#\"learning_coach\",\n",
    "#\"writing_editor\",\n",
    "#\"coding_partner\",\n",
    "#\"brainstormer\",\n",
    "#\"career_guide\",\n",
    "\n",
    "\"synonym_finder\",\n",
    "\"instructor_in_a_school\",\n",
    "\"dev_helper_upyxwdlcg\",\n",
    "\"url_to_business_plan_a3znu5fsn\",\n",
    "\"prompt_creator_8ope0amfj\"\n",
    "];\n",
    "\n",
    "\n",
    "### Add proper columns to the dataframe\n",
    "\n",
    "data = {}\n",
    "comp_val = {}\n",
    "base_comp_val = {}\n",
    "pos_comp_val = {}\n",
    "neg_comp_val = {}\n",
    "\n",
    "# Track benchmarks with negative tests for filtering\n",
    "benchmarks_with_neg_tests = []\n",
    "benchmarks_without_neg_tests = []\n",
    "\n",
    "for benchmark in benchmarks:\n",
    "    print(\"benchmark: \", benchmark)\n",
    "    data[benchmark] = pd.read_csv(f'{evalsDir}/{benchmark}/{benchmark}/overview.csv')\n",
    "    db = data[benchmark]\n",
    "\n",
    "    if \"tests negative\" in db.columns:\n",
    "        if db[\"tests negative\"][0] == 0:\n",
    "            print(\"No negative tests for: \", benchmark)\n",
    "            benchmarks_without_neg_tests.append(benchmark)\n",
    "        else:\n",
    "            benchmarks_with_neg_tests.append(benchmark)\n",
    "    else:\n",
    "        db[\"tests negative\"] = 0\n",
    "        benchmarks_without_neg_tests.append(benchmark)\n",
    "        \n",
    "    db[\"compliant %\"] = [float(p.strip('%')) / 100 for p in db[\"tests compliant\"]]\n",
    "    # db[\"compliant %\"] = 0\n",
    "    # Check if \"baseline compliant\" column exists before using it\n",
    "    if \"baseline compliant\" in db.columns:\n",
    "        db[\"baseline %\"] = [float(p.strip('%')) / 100 for p in db[\"baseline compliant\"]]\n",
    "    else:\n",
    "        db[\"baseline %\"] = 0\n",
    "    db[\"pos rule %\"] = db[\"tests positive compliant\"] / db[\"tests positive\"]\n",
    "    \n",
    "    # Handle division by zero for negative rules\n",
    "    if db[\"tests negative\"][0] > 0:\n",
    "        db[\"neg rule %\"] = db[\"tests negative compliant\"] / db[\"tests negative\"]\n",
    "    else:\n",
    "        db[\"neg rule %\"] = np.nan  # Use NaN for benchmarks with no negative tests\n",
    "        \n",
    "    db[\"valid test %\"] = db[\"tests valid\"] / db[\"tests\"]\n",
    "    # db[\"pos rule %\"] = 0\n",
    "    # db[\"neg rule %\"] = 0\n",
    "    # db[\"valid test %\"] = 0\n",
    "    \n",
    "    comp_val[benchmark] = db[\"compliant %\"]\n",
    "    base_comp_val[benchmark] = db[\"baseline %\"]\n",
    "    pos_comp_val[benchmark] = db[\"pos rule %\"]\n",
    "    neg_comp_val[benchmark] = db[\"neg rule %\"]\n",
    "\n",
    "    \n",
    "    print(db[\"compliant %\"])\n",
    "    print(db[\"baseline %\"])\n",
    "    print(db[\"pos rule %\"])\n",
    "    print(db[\"neg rule %\"])\n",
    "    print(db[\"valid test %\"])\n",
    "\n",
    "# Print warning about benchmarks with 0 negative tests\n",
    "if benchmarks_without_neg_tests:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"WARNING: The following benchmarks have 0 negative tests:\")\n",
    "    for benchmark in benchmarks_without_neg_tests:\n",
    "        print(f\"  - {benchmark}\")\n",
    "    print(\"These benchmarks will be excluded from negative rule compliance calculations.\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# zero out the sum\n",
    "n_models = len(data[benchmarks[0]][\"model\"])\n",
    "comp_val[\"sum\"] = [0 for i in range(n_models)]\n",
    "base_comp_val[\"sum\"] = [0 for i in range(n_models)]\n",
    "pos_comp_val[\"sum\"] = [0 for i in range(n_models)]\n",
    "neg_comp_val[\"sum\"] = [0 for i in range(n_models)]\n",
    "\n",
    "# sum up the values across the benchmarks\n",
    "for key in comp_val:\n",
    "    if (key != \"sum\"):\n",
    "        print(\"key:\", key)\n",
    "        comp_val[\"sum\"] = comp_val[\"sum\"] + comp_val[key]\n",
    "        base_comp_val[\"sum\"] = base_comp_val[\"sum\"] + base_comp_val[key]\n",
    "        pos_comp_val[\"sum\"] = pos_comp_val[\"sum\"] + pos_comp_val[key]\n",
    "        # Only sum negative values for benchmarks that have negative tests\n",
    "        if key in benchmarks_with_neg_tests:\n",
    "            neg_comp_val[\"sum\"] = neg_comp_val[\"sum\"] + neg_comp_val[key]\n",
    "        # print('neg_comp_val[\"sum\"]', neg_comp_val[\"sum\"])\n",
    "\n",
    "# calculate the mean\n",
    "#print('comp_val[\"sum\"]', comp_val[\"sum\"])\n",
    "comp_val[\"mean\"] = comp_val[\"sum\"]/len(benchmarks)\n",
    "#print('comp_val[\"mean\"]', comp_val[\"mean\"])\n",
    "base_comp_val[\"mean\"] = base_comp_val[\"sum\"]/len(benchmarks)\n",
    "pos_comp_val[\"mean\"] = pos_comp_val[\"sum\"]/len(benchmarks)\n",
    "#print('neg_comp_val[\"sum\"]', neg_comp_val[\"sum\"])\n",
    "# Calculate mean only for benchmarks with negative tests\n",
    "if len(benchmarks_with_neg_tests) > 0:\n",
    "    neg_comp_val[\"mean\"] = neg_comp_val[\"sum\"]/len(benchmarks_with_neg_tests)\n",
    "else:\n",
    "    neg_comp_val[\"mean\"] = [np.nan for i in range(n_models)]\n",
    "#print('neg_comp_val[\"mean\"]', neg_comp_val[\"mean\"])\n",
    "    \n",
    "    \n",
    "###############################################################################\n",
    "### Generate graphs from the data\n",
    "\n",
    "\n",
    "### Generate a csv with the non compliance % per benchmark for promptpex\n",
    "###\n",
    "    \n",
    "with open(f'{evalsDir}/pp-cpct.csv', 'w') as cfile:\n",
    "    print(\"Benchmark,\", end=\"\", file=cfile)\n",
    "    # just need the model names from the first benchmark\n",
    "    db = data[benchmarks[0]] \n",
    "    print(', '.join(map(str, db[\"model\"])), file=cfile)\n",
    "        \n",
    "    for benchmark in benchmarks:\n",
    "        name = prettyNames.get(benchmark, benchmark)\n",
    "        print(name, \",\", end=\"\", file=cfile)\n",
    "        db = data[benchmark]\n",
    "        print(', '.join(map(str, (1-db[\"compliant %\"]))), file=cfile)\n",
    "    \n",
    "    print(\"average\", \",\", end=\"\", file=cfile)\n",
    "    print(', '.join(map(str, (1-comp_val[\"mean\"]))), file=cfile)\n",
    "\n",
    "### Generate a csv with the test validity per benchmark for promptpex\n",
    "###\n",
    "  \n",
    "with open(f'{evalsDir}/pp-test-validity.csv', 'w') as cfile:\n",
    "    print(\"Benchmark, tests, valid tests\", file=cfile)\n",
    "        \n",
    "    for benchmark in benchmarks:\n",
    "        name = prettyNames.get(benchmark, benchmark)\n",
    "        db = data[benchmark]\n",
    "        print(f'{name},{db[\"tests\"][0]}, {db[\"tests valid\"][0]}', file=cfile)\n",
    "\n",
    "### Generate a csv comparing positive and negative compliance per model and benchmark for promptpex\n",
    "### Only include benchmarks that have negative tests\n",
    "###\n",
    "\n",
    "with open(f'{evalsDir}/pos-neg-cpct.csv', 'w') as cfile:\n",
    "    \n",
    "    models = data[benchmarks[0]][\"model\"]\n",
    "    pos_sum = [0 for i in range(len(models))]\n",
    "    neg_sum = [0 for i in range(len(models))]    \n",
    "    \n",
    "    print(\"Model, Rule % Non-Compliance, Inv Rule % Non-Compliance\", file=cfile)\n",
    "\n",
    "    # Only process benchmarks that have negative tests\n",
    "    for b in benchmarks_with_neg_tests:\n",
    "        db = data[b]\n",
    "        # print(f'benchmark:{benchmark}, {db[\"neg rule %\"]}')\n",
    "        pos_sum += db[\"pos rule %\"]\n",
    "        neg_sum += db[\"neg rule %\"]\n",
    "    \n",
    "    # Only output results if we have benchmarks with negative tests        \n",
    "    if len(benchmarks_with_neg_tests) > 0:\n",
    "        for m, psum, nsum in zip(models, pos_sum, neg_sum):\n",
    "            print(m, \",\", (1-psum/len(benchmarks_with_neg_tests)), \",\", (1-nsum/len(benchmarks_with_neg_tests)), file=cfile)\n",
    "        \n",
    "    #print(\"average\", \",\", end=\"\", file=cfile)\n",
    "    #print(pos_comp_val[\"mean\"], \",\", neg_comp_val[\"mean\"], file=cfile)\n",
    "\n",
    "### Generate a csv with the average compliance of promptpex versus baseline\n",
    "# each row is a model\n",
    "# each column is a method (pp vs baseline)\n",
    "\n",
    "with open(f'{evalsDir}/pp-compare.csv', 'w') as cfile:\n",
    "    \n",
    "    models = data[benchmarks[0]][\"model\"]\n",
    "    pp_sum = [0 for i in range(len(models))]\n",
    "    bl_sum = [0 for i in range(len(models))]    \n",
    "    \n",
    "    print(\"Model, PromptPex % Non-Compliance, Baseline % Non-Compliance\", file=cfile)\n",
    "\n",
    "    for b in benchmarks:\n",
    "        db = data[b]\n",
    "        #print(db[\"pos rule %\"])\n",
    "        pp_sum += db[\"compliant %\"]\n",
    "        bl_sum += db[\"baseline %\"]\n",
    "            \n",
    "    for m, psum, bsum in zip(models, pp_sum, bl_sum):\n",
    "        print(m, \",\", (1-psum/len(benchmarks)), \",\", (1-bsum/len(benchmarks)), file=cfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract out number of rules and grounded rules per benchmark\n",
    "\n",
    "import os\n",
    "\n",
    "grules = {}\n",
    "\n",
    "if True:\n",
    "    with open(f'{evalsDir}/pp-grounded-rules.csv', 'w') as cfile:\n",
    "        print(f\"benchmark, rules, grounded rules\", file=cfile)\n",
    "        for benchmark in benchmarks:\n",
    "            # Read the rules.txt file and count lines\n",
    "            rules_file_path = f'{evalsDir}/{benchmark}/{benchmark}/rules.txt'\n",
    "            if os.path.exists(rules_file_path):\n",
    "                with open(rules_file_path, 'r') as file:\n",
    "                    lines = file.readlines()\n",
    "                    # Count non-empty lines\n",
    "                    rule_count = len([line for line in lines if line.strip()])\n",
    "            else:\n",
    "                rule_count = 0\n",
    "                print(f\"Warning: {rules_file_path} not found\")\n",
    "            \n",
    "            name = prettyNames.get(benchmark, benchmark)\n",
    "            # Using rule_count for both rules and grounded rules since we're just counting lines\n",
    "            print(f\"{name}, {rule_count}, {rule_count}\", file=cfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "plt.rcParams.update({'font.size': 12}) \n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(f'{evalsDir}/pp-cpct.csv')\n",
    "\n",
    "# Remove the 'average' row if present\n",
    "df = df[df['Benchmark'] != 'average']\n",
    "\n",
    "# Set the 'Benchmark' column as the index\n",
    "df.set_index('Benchmark', inplace=True)\n",
    "\n",
    "# Plotting with larger figure size to accommodate labels\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "# Number of benchmarks\n",
    "n_benchmarks = len(df.index)\n",
    "# Number of models\n",
    "n_models = len(df.columns)\n",
    "\n",
    "# Bar width\n",
    "bar_width = 0.2\n",
    "\n",
    "# Positions of the bars on the x-axis\n",
    "indices = np.arange(n_benchmarks)\n",
    "\n",
    "# Plot each model's bars\n",
    "for i, model in enumerate(df.columns):\n",
    "    ax.bar(indices + i * bar_width, df[model] * 100, bar_width, label=model)\n",
    "\n",
    "# Set the x-axis labels with better formatting\n",
    "ax.set_xticks(indices + bar_width * (n_models - 1) / 2)\n",
    "ax.set_xticklabels(df.index, rotation=45, ha='right', fontsize=10)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Benchmark')\n",
    "ax.set_ylabel('Percentage Non-Compliance')\n",
    "# ax.set_title('Clustered Bar Chart of Benchmark % Non-Compliance by Model')\n",
    "ax.legend()\n",
    "\n",
    "# Adjust layout to prevent label cutoff\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.2)  # Add extra space at bottom for rotated labels\n",
    "print(\"Saving plot to:\", f'{evalsDir}/pp-cpct.pdf')\n",
    "plt.savefig(f'{evalsDir}/pp-cpct.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(f'{evalsDir}/pos-neg-cpct.csv')\n",
    "\n",
    "# Filter out models/rows where inverse rule non-compliance is NaN or where there were no negative tests\n",
    "# This happens when a benchmark has 0 negative tests\n",
    "df_filtered = df.dropna(subset=[' Inv Rule % Non-Compliance'])\n",
    "df_filtered = df_filtered[df_filtered[' Inv Rule % Non-Compliance'] != 1.0]  # Remove entries where inverse rule compliance is 0% (1.0 non-compliance)\n",
    "\n",
    "plt.rcParams.update({'font.size': 22}) \n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Bar width\n",
    "bar_width = 0.35\n",
    "\n",
    "# Positions of the bars on the x-axis\n",
    "indices = range(len(df_filtered))\n",
    "\n",
    "# Plot each compliance type's bars with different hatch patterns for accessibility\n",
    "ax.bar(indices, df_filtered[' Rule % Non-Compliance'] * 100, bar_width, \n",
    "       label='Rule', hatch='///', alpha=0.8)\n",
    "ax.bar([i + bar_width for i in indices], df_filtered[' Inv Rule % Non-Compliance'] * 100, bar_width, \n",
    "       label='Inverse Rule', hatch='...', alpha=0.8)\n",
    "\n",
    "# Set the x-axis labels\n",
    "ax.set_xticks([i + bar_width / 2 for i in indices])\n",
    "ax.set_xticklabels(df_filtered['Model'], rotation=0, ha='center')\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Percentage Non-Compliance')\n",
    "# ax.set_title('Rule % Non-Compliance and Inverse Rule % Non-Compliance by Model')\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{evalsDir}/pos-neg-cpct.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv(f'{evalsDir}/pp-test-validity.csv')\n",
    "\n",
    "# Plot the bar chart with hatch patterns for accessibility\n",
    "fig, ax = plt.subplots(figsize=(15, 8))  # Increased figure size\n",
    "\n",
    "# Get the columns to plot (excluding 'Benchmark')\n",
    "plot_columns = [col for col in df.columns if col != 'Benchmark']\n",
    "\n",
    "# Define different hatch patterns for each column\n",
    "hatch_patterns = ['///', '...', 'xxx', '---', '+++', '|||', 'ooo', '***']\n",
    "\n",
    "# Create a grouped bar chart\n",
    "n_benchmarks = len(df)\n",
    "n_columns = len(plot_columns)\n",
    "bar_width = 0.8 / n_columns\n",
    "x_positions = range(n_benchmarks)\n",
    "\n",
    "# Plot each column with a different hatch pattern\n",
    "for i, col in enumerate(plot_columns):\n",
    "    hatch_pattern = hatch_patterns[i % len(hatch_patterns)]\n",
    "    x_offset = [x + (i - (n_columns-1)/2) * bar_width for x in x_positions]\n",
    "    ax.bar(x_offset, df[col], width=bar_width, \n",
    "           label=col, hatch=hatch_pattern, alpha=0.8)\n",
    "\n",
    "# Set x-axis labels with better formatting\n",
    "ax.set_xticks(x_positions)\n",
    "ax.set_xticklabels(df['Benchmark'], rotation=45, ha='right', fontsize=10)\n",
    "\n",
    "ax.set_ylabel('Number of Tests')\n",
    "# ax.set_title('PromptPex Tests vs Valid Tests by Benchmark')\n",
    "ax.legend()\n",
    "\n",
    "# Adjust layout to prevent label cutoff\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.2)  # Add extra space at bottom for rotated labels\n",
    "\n",
    "# Show plot\n",
    "plt.savefig(f'{evalsDir}/pp-test-validity.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the CSV file\n",
    "if True:\n",
    "    df = pd.read_csv(f'{evalsDir}/pp-grounded-rules.csv')\n",
    "\n",
    "    # Plot the bar chart with hatch patterns for accessibility\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))  # Increased figure size\n",
    "    \n",
    "    # Get the columns to plot (excluding 'benchmark')\n",
    "    plot_columns = [col for col in df.columns if col != 'benchmark']\n",
    "    \n",
    "    # Define different hatch patterns for each column\n",
    "    hatch_patterns = ['///', '...', 'xxx', '---', '+++', '|||']\n",
    "    \n",
    "    # Create a grouped bar chart\n",
    "    n_benchmarks = len(df)\n",
    "    n_columns = len(plot_columns)\n",
    "    bar_width = 0.8 / n_columns\n",
    "    x_positions = range(n_benchmarks)\n",
    "    \n",
    "    # Plot each column with a different hatch pattern\n",
    "    for i, col in enumerate(plot_columns):\n",
    "        hatch_pattern = hatch_patterns[i % len(hatch_patterns)]\n",
    "        x_offset = [x + (i - (n_columns-1)/2) * bar_width for x in x_positions]\n",
    "        ax.bar(x_offset, df[col], width=bar_width, \n",
    "               label=col, hatch=hatch_pattern, alpha=0.8)\n",
    "    \n",
    "    # Set x-axis labels with better formatting\n",
    "    ax.set_xticks(x_positions)\n",
    "    ax.set_xticklabels(df['benchmark'], rotation=45, ha='right', fontsize=10)\n",
    "    \n",
    "    ax.set_xlabel('Benchmark')\n",
    "    ax.set_ylabel('Number of Rules')\n",
    "    #ax.set_title('PromptPex Rules vs Grounded Rules by Benchmark')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Adjust layout to prevent label cutoff\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.2)  # Add extra space at bottom for rotated labels\n",
    "\n",
    "    # Show plot\n",
    "    plt.savefig(f'{evalsDir}/pp-grounded-rules.pdf', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
