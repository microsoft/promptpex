{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import importlib\n",
        "\n",
        "# Add current directory to path to ensure we can import the analyze_metrics module\n",
        "sys.path.insert(0, os.getcwd())\n",
        "\n",
        "# Import the module and reload it to get the latest version\n",
        "import analyze_metrics\n",
        "importlib.reload(analyze_metrics)\n",
        "print(\"Module imported and reloaded successfully!\")\n",
        "\n",
        "# Define aliases for convenience - Original functions\n",
        "parse_metric = analyze_metrics.parse_metric\n",
        "get_metrics_start_col = analyze_metrics.get_metrics_start_col\n",
        "analyze_benchmark_metrics = analyze_metrics.analyze_benchmark_metrics\n",
        "collect_metrics = analyze_metrics.collect_metrics\n",
        "compute_model_metric_averages = analyze_metrics.compute_model_metric_averages\n",
        "print_metric_table = analyze_metrics.print_metric_table\n",
        "plot_grouped_bar_chart = analyze_metrics.plot_grouped_bar_chart\n",
        "collect_and_sum_benchmark_metrics = analyze_metrics.collect_and_sum_benchmark_metrics\n",
        "print_sums_table = analyze_metrics.print_sums_table\n",
        "plot_sums_bar = analyze_metrics.plot_sums_bar\n",
        "average_tests_per_model = analyze_metrics.average_tests_per_model\n",
        "print_avg_table = analyze_metrics.print_avg_table\n",
        "plot_avg_bar = analyze_metrics.plot_avg_bar\n",
        "get_default_benchmarks = analyze_metrics.get_default_benchmarks\n",
        "plot_grouped_barplot_by_benchmark_and_model = analyze_metrics.plot_grouped_barplot_by_benchmark_and_model\n",
        "plot_baseline_vs_main_metrics_by_benchmark = analyze_metrics.plot_baseline_vs_main_metrics_by_benchmark\n",
        "\n",
        "# New functions from analyze_metrics.py\n",
        "plot_baseline_vs_main_metrics_by_benchmark_single_model = analyze_metrics.plot_baseline_vs_main_metrics_by_benchmark_single_model\n",
        "parse_percentage = analyze_metrics.parse_percentage\n",
        "generate_plot_results_style_analysis = analyze_metrics.generate_plot_results_style_analysis\n",
        "_plot_non_compliance_by_benchmark = analyze_metrics._plot_non_compliance_by_benchmark\n",
        "_plot_rule_vs_inverse_rule = analyze_metrics._plot_rule_vs_inverse_rule\n",
        "_plot_promptpex_vs_baseline = analyze_metrics._plot_promptpex_vs_baseline\n",
        "_plot_test_validity = analyze_metrics._plot_test_validity\n",
        "_plot_rules_count = analyze_metrics._plot_rules_count\n",
        "\n",
        "print(\"All function aliases created successfully!\")\n",
        "\n",
        "benchmarkVersion = \"test-all-2025-09-29-paper/eval\"\n",
        "\n",
        "rootDir = \"/home/zorn/promptpex\"\n",
        "\n",
        "if not os.path.isdir(rootDir):\n",
        "    rootDir = \"..\"\n",
        "\n",
        "evalsDir = f\"{rootDir}/evals/{benchmarkVersion}/\"\n",
        "\n",
        "# Automatically detect available benchmarks instead of using hardcoded list\n",
        "print(\"Detecting available benchmarks...\")\n",
        "benchmarks = get_default_benchmarks(evalsDir)\n",
        "print(f\"Found {len(benchmarks)} available benchmarks:\")\n",
        "for i, benchmark in enumerate(benchmarks):\n",
        "    print(f\"  {i+1:2d}. {benchmark}\")\n",
        "\n",
        "# Use subset for faster testing (uncomment to use all benchmarks)\n",
        "# benchmarks = benchmarks[:5]  # Use first 5 benchmarks for testing\n",
        "print(f\"Using {len(benchmarks)} benchmarks for analysis\")\n",
        "\n",
        "# Control chart generation for individual benchmark analysis\n",
        "verbose = False  # Set to False to skip individual benchmark charts\n",
        "print(f\"Verbose mode: {verbose} - {'Charts will be generated' if verbose else 'Charts will be skipped'} for individual benchmarks\")\n",
        "\n",
        "prettyBenchmarkNames = { \n",
        "    \"speech-tag\": \"speech-tag\", \n",
        "    \"text-to-p\": \"text-to-p\",  \n",
        "    \"shakespearean-writing-assistant\": \"shakespeare\", \n",
        "    \"sentence-rewrite\": \"sentence\", \n",
        "    \"extract-names\": \"extract-names\", \n",
        "    \"elements\": \"elements\", \n",
        "    \"art-prompt\": \"art-prompt\", \n",
        "    \"classify-input-text\": \"classify\"\n",
        "}\n",
        "\n",
        "prettyMetrics = {\n",
        "    \"tests compliant\": \"prompt ok/err\", \n",
        "    \"system_compliant\": \"prompt only\",  \n",
        "    \"rules_system_with_input_compliant\": \"prompt/rule/input\"\n",
        "}\n",
        "\n",
        "# Display some random tests from the first available benchmark\n",
        "if benchmarks:\n",
        "    benchmark = benchmarks[0]\n",
        "    csv_path = f\"{evalsDir}/{benchmark}/{benchmark}/overview.csv\"\n",
        "    \n",
        "    if os.path.isfile(csv_path):\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print(f\"Metrics columns for {benchmark}:\", df.columns.tolist())\n",
        "        \n",
        "        df_sample = df.sample(n=min(10, len(df)), random_state=42)\n",
        "    else:\n",
        "        print(f\"No overview.csv found for {benchmark}\")\n",
        "else:\n",
        "    print(\"No benchmarks found!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Individual benchmark analysis - now using imported function with plot saving\n",
        "if verbose:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "    print(\"INDIVIDUAL BENCHMARK ANALYSIS (WITH CHARTS)\")\n",
        "    print(\"=\"*60)\n",
        "    for benchmark in benchmarks:\n",
        "        print(f\"Analyzing benchmark: {benchmark}\")\n",
        "        analyze_benchmark_metrics(benchmark, evalsDir, prettyBenchmarkNames, evalsDir)\n",
        "else:\n",
        "    print(\"\" + \"=\"*60)\n",
        "    print(\"INDIVIDUAL BENCHMARK ANALYSIS (SKIPPED - VERBOSE=FALSE)\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Skipping individual chart generation for {len(benchmarks)} benchmarks\")\n",
        "    print(\"To enable charts, set verbose = True in the first cell\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect all metrics data using imported function\n",
        "all_data, all_models, all_metrics = collect_metrics(benchmarks, evalsDir)\n",
        "\n",
        "# Compute averages using imported function\n",
        "model_metric_avg = compute_model_metric_averages(all_data, all_models, all_metrics)\n",
        "\n",
        "# Display table using imported function  \n",
        "print_metric_table(model_metric_avg, prettyMetrics)\n",
        "\n",
        "# Plot averages with plot saving enabled\n",
        "plot_grouped_bar_chart(model_metric_avg, evalsDir, evalsDir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the same columns from the first benchmark's data\n",
        "if benchmarks:\n",
        "    csv_path = os.path.join(evalsDir, benchmarks[0], benchmarks[0], \"overview.csv\")\n",
        "    if os.path.isfile(csv_path):\n",
        "        df = pd.read_csv(csv_path)\n",
        "        df.columns = df.columns.str.strip()\n",
        "        start_col = get_metrics_start_col(df)\n",
        "        columns_of_interest = list(df.columns[start_col:])\n",
        "        \n",
        "        # Sum metrics analysis with plot saving\n",
        "        print(\"Computing benchmark sums...\")\n",
        "        data, sums = collect_and_sum_benchmark_metrics(benchmarks, evalsDir, columns_of_interest)\n",
        "        print_sums_table(sums, columns_of_interest)\n",
        "        plot_sums_bar(sums, columns_of_interest, evalsDir, evalsDir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Average tests per model analysis with plot saving\n",
        "print(\"Computing average tests per model...\")\n",
        "averages = average_tests_per_model(benchmarks, evalsDir)\n",
        "print_avg_table(averages)\n",
        "plot_avg_bar(averages, evalsDir, evalsDir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grouped analysis with plot saving - using imported function\n",
        "column_to_plot = None\n",
        "\n",
        "# Check what columns are available\n",
        "if benchmarks:\n",
        "    csv_path = os.path.join(evalsDir, benchmarks[0], benchmarks[0], \"overview.csv\")\n",
        "    if os.path.isfile(csv_path):\n",
        "        df = pd.read_csv(csv_path)\n",
        "        df.columns = df.columns.str.strip()\n",
        "        available_columns = df.columns.tolist()\n",
        "        print(f\"Available columns: {available_columns}\")\n",
        "        \n",
        "        # Try to find a suitable column to plot\n",
        "        if \"tests compliant\" in available_columns:\n",
        "            column_to_plot = \"tests compliant\"\n",
        "        elif \"accuracy with azure:o4-mini_2025-04-16\" in available_columns:\n",
        "            column_to_plot = \"accuracy with azure:o4-mini_2025-04-16\"\n",
        "        elif len(available_columns) > 2:  # model + at least one metric\n",
        "            column_to_plot = available_columns[2]  # Skip 'model' and possibly other non-metric columns\n",
        "            \n",
        "if not column_to_plot:\n",
        "    column_to_plot = \"tests compliant\"\n",
        "    print(\"Using default column: tests compliant\")\n",
        "\n",
        "print(f\"Plotting column: {column_to_plot}\")\n",
        "plot_grouped_barplot_by_benchmark_and_model(benchmarks, evalsDir, column_to_plot, evalsDir, show_error_bars=False)\n",
        "\n",
        "# Try accuracy if available\n",
        "if \"accuracy with azure:o4-mini_2025-04-16\" in available_columns:\n",
        "    column_to_plot = \"accuracy with azure:o4-mini_2025-04-16\"\n",
        "    print(f\"Plotting column: {column_to_plot}\")\n",
        "    plot_grouped_barplot_by_benchmark_and_model(benchmarks, evalsDir, column_to_plot, evalsDir, show_error_bars=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Baseline comparison analysis with plot saving - using imported function\n",
        "print(\"=\" * 60)\n",
        "print(\"BASELINE COMPARISON ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check if baseline data exists\n",
        "has_baseline = False\n",
        "for benchmark in benchmarks[:3]:  # Check first few benchmarks\n",
        "    baseline_csv_path = os.path.join(evalsDir, benchmark, benchmark, \"overview-baseline.csv\")\n",
        "    if os.path.isfile(baseline_csv_path):\n",
        "        has_baseline = True\n",
        "        print(f\"Found baseline data for: {benchmark}\")\n",
        "        break\n",
        "\n",
        "if has_baseline:\n",
        "    print(\"Running baseline vs main analysis with plot saving...\")\n",
        "    plot_baseline_vs_main_metrics_by_benchmark(benchmarks, evalsDir, \"tests compliant\", evalsDir)\n",
        "    \n",
        "    # Try accuracy comparison if available\n",
        "    csv_path = os.path.join(evalsDir, benchmarks[0], benchmarks[0], \"overview.csv\")\n",
        "    if os.path.isfile(csv_path):\n",
        "        df = pd.read_csv(csv_path)\n",
        "        df.columns = df.columns.str.strip()\n",
        "        accuracy_cols = [col for col in df.columns if \"accuracy\" in col.lower()]\n",
        "        if accuracy_cols:\n",
        "            print(f\"Running accuracy baseline comparison for: {accuracy_cols[0]}\")\n",
        "            plot_baseline_vs_main_metrics_by_benchmark(benchmarks, evalsDir, accuracy_cols[0], evalsDir)\n",
        "        else:\n",
        "            print(\"No accuracy columns found for baseline comparison.\")\n",
        "    \n",
        "    print(\"Baseline comparison plots saved to:\", evalsDir)\n",
        "else:\n",
        "    print(\"No baseline data found. Checking available files:\")\n",
        "    for benchmark in benchmarks[:5]:  # Check first few benchmarks\n",
        "        baseline_path = os.path.join(evalsDir, benchmark, benchmark, \"overview-baseline.csv\")\n",
        "        main_path = os.path.join(evalsDir, benchmark, benchmark, \"overview.csv\")\n",
        "        print(f\"  {benchmark}:\")\n",
        "        print(f\"    Main: {'✓' if os.path.isfile(main_path) else '✗'}\")\n",
        "        print(f\"    Baseline: {'✓' if os.path.isfile(baseline_path) else '✗'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Single Model Analysis - gpt-oss specific analysis\n",
        "print(\"=\" * 60)\n",
        "print(\"SINGLE MODEL ANALYSIS - GPT-OSS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if has_baseline:\n",
        "    print(\"Running single model analysis for gpt-oss...\")\n",
        "    plot_baseline_vs_main_metrics_by_benchmark_single_model(benchmarks, evalsDir, \"gpt-oss\", \"tests compliant\", evalsDir)\n",
        "    \n",
        "    # Try accuracy comparison for single model if available\n",
        "    if 'accuracy_cols' in locals() and accuracy_cols:\n",
        "        plot_baseline_vs_main_metrics_by_benchmark_single_model(benchmarks, evalsDir, \"gpt-oss\", accuracy_cols[0], evalsDir)\n",
        "    \n",
        "    print(\"Single model analysis plots saved to:\", evalsDir)\n",
        "else:\n",
        "    print(\"No baseline data found. Skipping single model analysis.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot-Results Style Analysis - Complete compliance analysis suite\n",
        "print(\"=\" * 60)\n",
        "print(\"PLOT-RESULTS STYLE ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# This generates all the plots and CSV files that were originally in plot-results.ipynb\n",
        "# Including: non-compliance charts, rule vs inverse rule, promptpex vs baseline, \n",
        "# test validity, and rules count analysis\n",
        "\n",
        "generate_plot_results_style_analysis(benchmarks, evalsDir, evalsDir)\n",
        "\n",
        "print(\"Plot-results style analysis complete!\")\n",
        "print(\"Generated files:\")\n",
        "print(\"  - pp-cpct.csv (non-compliance percentages)\")\n",
        "print(\"  - pp-test-validity.csv (test validity statistics)\")  \n",
        "print(\"  - pos-neg-cpct.csv (rule vs inverse rule compliance)\")\n",
        "print(\"  - pp-compare.csv (promptpex vs baseline comparison)\")\n",
        "print(\"  - pp-grounded-rules.csv (rules count per benchmark)\")\n",
        "print(\"  - Various PDF charts for each analysis type\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Individual Plot-Results Chart Generation (Optional)\n",
        "# This cell demonstrates how to generate individual charts from the plot-results analysis\n",
        "# These charts are also generated by the generate_plot_results_style_analysis function above\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"INDIVIDUAL PLOT-RESULTS CHART GENERATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# You can uncomment any of these to generate individual charts\n",
        "# Note: The CSV files must exist first (generated by the previous cell)\n",
        "\n",
        "try:\n",
        "    print(\"Checking if CSV files exist for individual chart generation...\")\n",
        "    \n",
        "    # Check for existence of CSV files\n",
        "    csv_files = [\n",
        "        f\"{evalsDir}/pp-cpct.csv\",\n",
        "        f\"{evalsDir}/pos-neg-cpct.csv\", \n",
        "        f\"{evalsDir}/pp-compare.csv\",\n",
        "        f\"{evalsDir}/pp-test-validity.csv\",\n",
        "        f\"{evalsDir}/pp-grounded-rules.csv\"\n",
        "    ]\n",
        "    \n",
        "    existing_files = [f for f in csv_files if os.path.isfile(f)]\n",
        "    print(f\"Found {len(existing_files)}/{len(csv_files)} CSV files\")\n",
        "    \n",
        "    if len(existing_files) >= 3:  # Generate charts if we have most CSV files\n",
        "        print(\"\\nGenerating individual charts...\")\n",
        "        \n",
        "        # Generate non-compliance by benchmark chart\n",
        "        if os.path.isfile(f\"{evalsDir}/pp-cpct.csv\"):\n",
        "            print(\"1. Non-compliance by benchmark chart...\")\n",
        "            _plot_non_compliance_by_benchmark(evalsDir)\n",
        "        \n",
        "        # Generate rule vs inverse rule chart  \n",
        "        if os.path.isfile(f\"{evalsDir}/pos-neg-cpct.csv\"):\n",
        "            print(\"2. Rule vs inverse rule chart...\")\n",
        "            _plot_rule_vs_inverse_rule(evalsDir)\n",
        "        \n",
        "        # Generate promptpex vs baseline chart\n",
        "        if os.path.isfile(f\"{evalsDir}/pp-compare.csv\"):\n",
        "            print(\"3. PromptPex vs baseline chart...\")\n",
        "            _plot_promptpex_vs_baseline(evalsDir)\n",
        "        \n",
        "        # Generate test validity chart\n",
        "        if os.path.isfile(f\"{evalsDir}/pp-test-validity.csv\"):\n",
        "            print(\"4. Test validity chart...\")\n",
        "            _plot_test_validity(evalsDir)\n",
        "        \n",
        "        # Generate rules count chart\n",
        "        if os.path.isfile(f\"{evalsDir}/pp-grounded-rules.csv\"):\n",
        "            print(\"5. Rules count chart...\")\n",
        "            _plot_rules_count(evalsDir)\n",
        "        \n",
        "        print(\"Individual chart generation complete!\")\n",
        "    else:\n",
        "        print(\"Not enough CSV files found. Run the plot-results style analysis first.\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"Error in individual chart generation: {e}\")\n",
        "    print(\"Make sure to run the plot-results style analysis cell first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analysis Complete\n",
        "\n",
        "## Summary of Generated Plots and Files\n",
        "\n",
        "This notebook now generates all the same plots and analysis as the `analyze_metrics.py` script, including:\n",
        "\n",
        "### 1. **Individual Benchmark Analysis** (if verbose=True)\n",
        "- Individual metric charts per benchmark\n",
        "\n",
        "### 2. **Aggregated Analysis**\n",
        "- Model metric averages across benchmarks\n",
        "- Benchmark sums analysis\n",
        "- Average tests per model\n",
        "- Grouped analysis by benchmark and model\n",
        "\n",
        "### 3. **Baseline Comparison Analysis**\n",
        "- Overall baseline vs promptpex comparison (with inverse rule analysis)\n",
        "- Accuracy metrics comparison\n",
        "- Cross-benchmark statistics with error bars\n",
        "\n",
        "### 4. **Single Model Analysis**\n",
        "- GPT-OSS specific baseline vs promptpex analysis\n",
        "- Model-specific compliance and accuracy comparisons\n",
        "\n",
        "### 5. **Plot-Results Style Analysis**\n",
        "- **CSV Files Generated:**\n",
        "  - `pp-cpct.csv` - Non-compliance percentages by benchmark and model\n",
        "  - `pp-test-validity.csv` - Test validity statistics\n",
        "  - `pos-neg-cpct.csv` - Rule vs inverse rule compliance comparison\n",
        "  - `pp-compare.csv` - PromptPex vs baseline comparison\n",
        "  - `pp-grounded-rules.csv` - Rules count per benchmark\n",
        "\n",
        "- **Charts Generated:**\n",
        "  - Non-compliance clustered bar chart\n",
        "  - Rule vs inverse rule comparison\n",
        "  - PromptPex vs baseline comparison\n",
        "  - Test validity visualization\n",
        "  - Rules count per benchmark\n",
        "\n",
        "### 6. **Individual Chart Functions**\n",
        "- Demonstrates how to use the private plotting functions\n",
        "- Allows selective chart generation\n",
        "\n",
        "All charts use consistent styling and are saved as PDF files in the evaluation directory.\n",
        "\n",
        "**Files saved to:** `{evalsDir}`"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
