{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "benchmarkVersion = \"test-all-2025-09-29/run\"\n",
    "\n",
    "rootDir = \"/workspaces/promptpex/\"\n",
    "\n",
    "if not os.path.isdir(rootDir):\n",
    "    rootDir = \"..\"\n",
    "\n",
    "evalsDir = f'{rootDir}/evals/{benchmarkVersion}/'\n",
    "\n",
    "\n",
    "benchmarks = [\n",
    "    \"speech-tag\"\n",
    "]\n",
    "\n",
    "# full list\n",
    "benchmarks = [ \"speech-tag\", \"classify-input-text\", \"text-to-p\",  \"sentence-rewrite\", \"extract-names\", \"elements\", \"art-prompt\", \"shakespearean-writing-assistant\"]\n",
    "\n",
    "\n",
    "benchmarks = [\n",
    "    \"bayesian_games_29_7\",\n",
    "    \"bullet_journaling_145_1\",\n",
    "    \"canopy_management_298_8\",\n",
    "    \"fancy_title_generator\",\n",
    "    \"hearing_impairments_124_7\",\n",
    "    \"housing_market_dynamics_338_1\",\n",
    "    \"initial_public_offerings_ipos_70_9\",\n",
    "    \"news_broadcasting_693_9\",\n",
    "    \"prompt_generator\",\n",
    "    \"real_time_analytics_609_2\",\n",
    "    \"recruiter\",\n",
    "    \"restaurant_owner\",\n",
    "    \"sewing_951_7\",\n",
    "    \"solr_search_engine\",\n",
    "    \"speaker_identification_595_2\",\n",
    "    \"startup_idea_generator\",\n",
    "    \"tea_taster\",\n",
    "    \"virtual_fitness_coach\",\n",
    "    \"yes_or_no_answer\",\n",
    "    \"speech-tag\", \"classify-input-text\", \"text-to-p\",  \"sentence-rewrite\", \"extract-names\", \"elements\", \"art-prompt\", \"shakespearean-writing-assistant\",\n",
    "\"modular_arithmetic_26_4\",\n",
    "\"sleep_hygiene_174_3\",\n",
    "\"supply_chain_risk_management_347_8\",\n",
    "\"supporting_homework_and_study_habits_547_7\",\n",
    "\"decentralized_finance_defi_937_2\",\n",
    "\"fashion_brand_marketing_strategies_889_3\",\n",
    "\"kombucha_brewing_479_10\",\n",
    "\"smart_security_systems_554_2\",\n",
    "\"autism_spectrum_disorder_124_2\",\n",
    "\"sleep_hygiene_practices_163_4\",\n",
    "\"virtual_fitness_coach\",\n",
    "\"dentist\",\n",
    "\"personal_stylist\",\n",
    "\"developer_relations_consultant\",\n",
    "\"biblical_translator\",\n",
    "\"ai_assisted_doctor\",\n",
    "\"web_design_consultant\",\n",
    "\"public_speaking_coach\",\n",
    "\"buddha\",\n",
    "\"screenwriter\",\n",
    "]\n",
    "\n",
    "benchmarks = [\n",
    "\"startup_idea_generator\",\n",
    "\"tea_taster\",\n",
    "\"recruiter\",\n",
    "\"yes_or_no_answer\",\n",
    "\"virtual_fitness_coach\",\n",
    "\"fancy_title_generator\",\n",
    "\"restaurant_owner\",\n",
    "\"prompt_generator\",\n",
    "\"solr_search_engine\",\n",
    "\"sewing_951_7\",\n",
    "\"hearing_impairments_124_7\",\n",
    "\"speaker_identification_595_2\",\n",
    "] \n",
    "\n",
    "benchmarks = [ \"speech-tag\", \"classify-input-text\", \"text-to-p\",  \"sentence-rewrite\", \"extract-names\", \"elements\", \"art-prompt\", \n",
    "\"sewing_951_7\",\n",
    "\"hearing_impairments_124_7\",\n",
    "\"speaker_identification_595_2\",\n",
    "\"real_time_analytics_609_2\",\n",
    "\"housing_market_dynamics_338_1\",]\n",
    "\n",
    "benchmarks = [ \"speech-tag\",  ]\n",
    "\n",
    "benchmarks = [ \"speech-tag\", \"classify-input-text\",  \"sentence-rewrite\", \"extract-names\", \"elements\", \"art-prompt\", \"shakespearean-writing-assistant\" ]\n",
    "\n",
    "\n",
    "benchmarks = [ \"speech-tag\",  ]\n",
    "\n",
    "              \n",
    "benchmarks = [\n",
    "\"crewai_assistant_qqtuuwsby\",\n",
    "\"fragrance_finder_deluxe_e9avvjxcw\",\n",
    "\"information_kiosk_building_j6ry5iscb\",\n",
    "\"thread_weaver_krog0f5tg\",\n",
    "\"hurtig_ingeni_r_pgktzdcfk\",\n",
    "\n",
    "    \"speech-tag\",\n",
    "    \"text-to-p\",\n",
    "    \"elements\",\n",
    "    \"art-prompt\",\n",
    "    \"extract-names\",\n",
    "    \"classify-input-text\",\n",
    "    \"sentence-rewrite\",\n",
    "    \"shakespearean-writing-assistant\",\n",
    "\n",
    "\"architect_guide_for_programmers\",\n",
    "\"unconstrained_ai_model_dan\",\n",
    "\"idea_clarifier_gpt\",\n",
    "\"structured_iterative_reasoning_protocol_sirp\",\n",
    "\"tech_challenged_customer\",\n",
    "\n",
    "\"learning_coach\",\n",
    "\"writing_editor\",\n",
    "\"coding_partner\",\n",
    "\"brainstormer\",\n",
    "\"career_guide\",\n",
    "\n",
    "\"synonym_finder\",\n",
    "\"instructor_in_a_school\",\n",
    "\"dev_helper_upyxwdlcg\",\n",
    "\"url_to_business_plan_a3znu5fsn\",\n",
    "\"prompt_creator_8ope0amfj\"\n",
    "]\n",
    "\n",
    "\n",
    "benchmark = benchmarks[0]\n",
    "\n",
    "prettyBenchmarkNames = { \"speech-tag\": \"speech-tag\", \n",
    "                \"text-to-p\": \"text-to-p\",  \n",
    "                \"shakespearean-writing-assistant\": \"shakespeare\", \n",
    "                \"sentence-rewrite\": \"sentence\", \n",
    "                \"extract-names\": \"extract-names\", \n",
    "                \"elements\":\"elements\", \n",
    "                \"art-prompt\": \"art-prompt\", \n",
    "                \"classify-input-text\": \"classify\"}\n",
    "\n",
    "prettyMetrics = { \"tests compliant\": \"prompt ok/err\", \n",
    "                \"system_compliant\": \"prompt only\",  \n",
    "                \"rules_system_with_input_compliant\": \"prompt/rule/input\"\n",
    "}\n",
    "\n",
    "def parse_metric(val):\n",
    "    \"\"\"Convert metric to float, handle %, NaN, and '--'.\"\"\"\n",
    "    if isinstance(val, str):\n",
    "        val = val.strip()\n",
    "        if val.endswith('%'):\n",
    "            try:\n",
    "                return float(val.strip('%'))\n",
    "            except:\n",
    "                return 0.0\n",
    "        if val in ('NaN', '--', ''):\n",
    "            return 0.0\n",
    "    try:\n",
    "        v = float(val)\n",
    "        if np.isnan(v):\n",
    "            return 0.0\n",
    "        return v\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def get_metrics_start_col(df):\n",
    "    \"\"\"Get the start column index for metrics (after 'tests valid compliant').\"\"\"\n",
    "    df.columns = df.columns.str.strip()  # Strip whitespace from column names\n",
    "    print(\"DataFrame columns:\", df.columns.tolist())\n",
    "    \n",
    "    start_col = len(df.columns)  # Default to end of columns (no metrics)\n",
    "    \n",
    "    if 'tests valid compliant' in df.columns:\n",
    "        start_col = df.columns.get_loc('tests valid compliant') + 1\n",
    "    elif 'tests negative compliant' in df.columns:\n",
    "        start_col = df.columns.get_loc('tests negative compliant') + 1\n",
    "    elif 'tests positive compliant' in df.columns:\n",
    "        start_col = df.columns.get_loc('tests positive compliant') + 1\n",
    "    elif 'model' in df.columns:\n",
    "        # Fallback: assume metrics start after 'model' column\n",
    "        start_col = df.columns.get_loc('model') + 1\n",
    "    \n",
    "    # Ensure start_col doesn't exceed the number of columns\n",
    "    return min(start_col, len(df.columns))\n",
    "\n",
    "    \n",
    "def analyze_benchmark_metrics(benchmark, evalsDir, prettyBenchmarkNames):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Build the path to the overview.csv for the given benchmark\n",
    "    csv_path = os.path.join(evalsDir, benchmark, benchmark, \"overview.csv\")\n",
    "    \n",
    "    if not os.path.isfile(csv_path):\n",
    "        print(f\"Warning: {csv_path} not found, skipping benchmark '{benchmark}'.\")\n",
    "        return\n",
    "\n",
    "    # Read the CSV\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {csv_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Get the start column for metrics\n",
    "    start_col = get_metrics_start_col(df)\n",
    "\n",
    "    # Check if there are any metrics columns\n",
    "    if start_col >= len(df.columns):\n",
    "        print(f\"No metrics columns found for benchmark '{benchmark}'. Available columns: {df.columns.tolist()}\")\n",
    "        return\n",
    "\n",
    "    # Include \"tests compliant\" as a metric (ensure it's included)\n",
    "    metrics = list(df.columns[start_col:])\n",
    "    \n",
    "    if not metrics:\n",
    "        print(f\"No metrics found for benchmark '{benchmark}'.\")\n",
    "        return\n",
    "\n",
    "    # Extract model names and metrics columns\n",
    "    if 'model' not in df.columns:\n",
    "        print(f\"No 'model' column found in {csv_path}\")\n",
    "        return\n",
    "        \n",
    "    models = df['model']\n",
    "    \n",
    "    if len(models) == 0:\n",
    "        print(f\"No models found for benchmark '{benchmark}'.\")\n",
    "        return\n",
    "    \n",
    "    metrics_table = df[['model'] + metrics].copy()\n",
    "\n",
    "    for metric in metrics:\n",
    "        metrics_table[metric] = metrics_table[metric].apply(parse_metric)\n",
    "\n",
    "    print(f\"Metrics by Model for benchmark '{benchmark}':\")\n",
    "    print(metrics_table.to_string(index=False))\n",
    "\n",
    "    # Plot grouped bar chart\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.8 / len(metrics)  # total width for all bars per group\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax.bar(x + i*width, metrics_table[metric], width, label=metric)\n",
    "\n",
    "    ax.set_xticks(x + width*(len(metrics)-1)/2)\n",
    "    ax.set_xticklabels(models, rotation=20)\n",
    "    ax.set_ylabel('Metric Value')\n",
    "    ax.set_title(f\"Model Metrics for {prettyBenchmarkNames.get(benchmark, benchmark)}\")\n",
    "    ax.legend(loc='best', fontsize='small', ncol=2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "analyze_benchmark_metrics(benchmark, evalsDir, prettyBenchmarkNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_metrics(benchmarks, evalsDir):\n",
    "    # Data structure: {benchmark: {model: {metric: value}}}\n",
    "    all_data = {}\n",
    "    all_metrics = set()\n",
    "    all_models = set()\n",
    "\n",
    "    for benchmark in benchmarks:\n",
    "        csv_path = os.path.join(evalsDir, benchmark, benchmark, \"overview.csv\")\n",
    "        if not os.path.isfile(csv_path):\n",
    "            print(f\"Warning: {csv_path} not found, skipping.\")\n",
    "            continue\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Get the start column for metrics\n",
    "        start_col = get_metrics_start_col(df)\n",
    "\n",
    "        # Check if there are any metrics columns\n",
    "        if start_col >= len(df.columns):\n",
    "            print(f\"No metrics columns found for benchmark '{benchmark}', skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Include \"tests compliant\" as a metric (ensure it's included)\n",
    "        metrics = list(df.columns[start_col:])\n",
    "        \n",
    "        if not metrics:\n",
    "            print(f\"No metrics found for benchmark '{benchmark}', skipping.\")\n",
    "            continue\n",
    "\n",
    "        all_metrics.update(metrics)\n",
    "        all_data[benchmark] = {}\n",
    "        for _, row in df.iterrows():\n",
    "            model = row['model']\n",
    "            all_models.add(model)\n",
    "            all_data[benchmark].setdefault(model, {})\n",
    "            for metric in metrics:\n",
    "                all_data[benchmark][model][metric] = parse_metric(row[metric])\n",
    "    return all_data, sorted(all_models), sorted(all_metrics)\n",
    "\n",
    "def compute_model_metric_averages(all_data, all_models, all_metrics):\n",
    "    if not all_metrics:\n",
    "        print(\"No metrics available for computation.\")\n",
    "        return {}\n",
    "    \n",
    "    # {model: {metric: [values...]}}\n",
    "    model_metric_values = {model: {metric: [] for metric in all_metrics} for model in all_models}\n",
    "    for benchmark in all_data:\n",
    "        for model in all_models:\n",
    "            model_metrics = all_data[benchmark].get(model, {})\n",
    "            for metric in all_metrics:\n",
    "                val = model_metrics.get(metric, 0.0)\n",
    "                model_metric_values[model][metric].append(val)\n",
    "    # Compute averages\n",
    "    model_metric_avg = {model: {metric: np.mean(vals) for metric, vals in metrics.items()} for model, metrics in model_metric_values.items()}\n",
    "    return model_metric_avg\n",
    "\n",
    "def print_metric_table(model_metric_avg):\n",
    "    if not model_metric_avg:\n",
    "        print(\"No metric data available to display.\")\n",
    "        return\n",
    "    \n",
    "    models = list(model_metric_avg.keys())\n",
    "    if not models:\n",
    "        print(\"No models found.\")\n",
    "        return\n",
    "    \n",
    "    # Check if any model has metrics\n",
    "    first_model_metrics = model_metric_avg[models[0]]\n",
    "    if not first_model_metrics:\n",
    "        print(\"No metrics found for any model.\")\n",
    "        return\n",
    "    \n",
    "    metrics = list(first_model_metrics.keys())\n",
    "    print(\"Average Metrics by Model:\")\n",
    "    header = [\"Model\"] + [prettyMetrics.get(m, m) for m in metrics]\n",
    "    \n",
    "    print(\"\\t\".join(header))\n",
    "    for model in models:\n",
    "        row = [model] + [f\"{model_metric_avg[model][metric]:.2f}\" for metric in metrics]\n",
    "        print(\"\\t\".join(row))\n",
    "\n",
    "def plot_grouped_bar_chart(model_metric_avg):\n",
    "    if not model_metric_avg:\n",
    "        print(\"No metric data available for plotting.\")\n",
    "        return\n",
    "    \n",
    "    models = list(model_metric_avg.keys())\n",
    "    if not models:\n",
    "        print(\"No models found for plotting.\")\n",
    "        return\n",
    "    \n",
    "    # Check if any model has metrics\n",
    "    first_model_metrics = model_metric_avg[models[0]]\n",
    "    if not first_model_metrics:\n",
    "        print(\"No metrics found for plotting.\")\n",
    "        return\n",
    "    \n",
    "    metrics = list(first_model_metrics.keys())\n",
    "    if len(metrics) == 0:\n",
    "        print(\"No metrics available for plotting.\")\n",
    "        return\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.8 / len(metrics)\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    for i, metric in enumerate(metrics):\n",
    "        values = [model_metric_avg[model][metric] for model in models]\n",
    "        ax.bar(x + i*width, values, width, label=metric)\n",
    "    ax.set_xticks(x + width*(len(metrics)-1)/2)\n",
    "    ax.set_xticklabels(models, rotation=20)\n",
    "    ax.set_ylabel('Average Metric Value')\n",
    "    ax.set_title('Average Model Metrics Across Benchmarks')\n",
    "    ax.legend(loc='best', fontsize='small', ncol=2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "all_data, all_models, all_metrics = collect_metrics(benchmarks, evalsDir)\n",
    "\n",
    "if not all_metrics:\n",
    "    print(\"No metrics found across all benchmarks. Cannot proceed with analysis.\")\n",
    "elif not all_models:\n",
    "    print(\"No models found across all benchmarks. Cannot proceed with analysis.\")\n",
    "else:\n",
    "    model_metric_avg = compute_model_metric_averages(all_data, all_models, all_metrics)\n",
    "    print_metric_table(model_metric_avg)\n",
    "    plot_grouped_bar_chart(model_metric_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_benchmark_model_metrics_table(benchmarks, evalsDir, columns_of_interest):\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    width = 18\n",
    "    def fit(val):\n",
    "        s = str(val)\n",
    "        return s[:width].ljust(width)[:width] \n",
    "    \n",
    "    header = [\"Benchmark\", \"Model\"] + [\n",
    "        prettyMetrics.get(col, col) for col in columns_of_interest\n",
    "    ]\n",
    "\n",
    "    print(\"\".join([fit(h) for h in header]))\n",
    "    for benchmark in benchmarks:\n",
    "        csv_path = os.path.join(evalsDir, benchmark, benchmark, \"overview.csv\")\n",
    "        if not os.path.isfile(csv_path):\n",
    "            continue\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            model = row['model']\n",
    "            values = []\n",
    "            for col in columns_of_interest:\n",
    "                val0 = row.get(col, 0)\n",
    "                val = parse_metric(val0)\n",
    "                values.append(f\"{val:.2f}\")\n",
    "            print(\"\".join([fit(benchmark), fit(model)] + [fit(v) for v in values]))\n",
    "\n",
    "# read csv for first benchmark\n",
    "csv_path = os.path.join(evalsDir, benchmarks[0], benchmarks[0], \"overview.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Get the start column for metrics\n",
    "start_col = get_metrics_start_col(df)\n",
    "\n",
    "# columns_of_interest = [\"tests\", \"tests compliant\", \"errors\", \"tests compliance unknown\"] + list(df.columns[start_col:])\n",
    "columns_of_interest = list(df.columns[start_col:])\n",
    "\n",
    "print_benchmark_model_metrics_table(benchmarks, evalsDir, columns_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def collect_and_sum_benchmark_metrics(benchmarks, evalsDir, columns_of_interest):\n",
    "    # data[benchmark][model][column]\n",
    "    data = {}\n",
    "    sums = {bench: {col: 0.0 for col in columns_of_interest} for bench in benchmarks}\n",
    "    for benchmark in benchmarks:\n",
    "        csv_path = os.path.join(evalsDir, benchmark, benchmark, \"overview.csv\")\n",
    "        if not os.path.isfile(csv_path):\n",
    "            print(f\"Warning: {csv_path} not found, skipping.\")\n",
    "            continue\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df.columns = df.columns.str.strip()  # Strip whitespace from column names\n",
    "        data[benchmark] = {}\n",
    "        for _, row in df.iterrows():\n",
    "            model = row['model']\n",
    "            data[benchmark][model] = {}\n",
    "            for col in columns_of_interest:\n",
    "                val = parse_metric(row.get(col, 0))\n",
    "                data[benchmark][model][col] = val\n",
    "                sums[benchmark][col] += val\n",
    "    return data, sums\n",
    "\n",
    "def print_sums_table(sums, columns_of_interest):\n",
    "    print(\"Benchmark\\t\" + \"\\t\".join(columns_of_interest))\n",
    "    for bench, colvals in sums.items():\n",
    "        row = [bench] + [f\"{colvals[col]:.2f}\" for col in columns_of_interest]\n",
    "        print(\"\\t\".join(row))\n",
    "\n",
    "def plot_sums_bar(sums, columns_of_interest):\n",
    "    benchmarks = list(sums.keys())\n",
    "    for col in columns_of_interest:\n",
    "        values = [sums[bench][col] for bench in benchmarks]\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.bar(benchmarks, values)\n",
    "        plt.ylabel(col)\n",
    "        plt.title(f\"Sum of {col} by Benchmark\")\n",
    "        plt.xticks(rotation=20)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# columns_of_interest = [\"errors\", \"tests compliance unknown\"]\n",
    "# Get the start column for metrics\n",
    "start_col = get_metrics_start_col(df)\n",
    "\n",
    "# columns_of_interest = [\"tests\", \"tests compliant\", \"errors\", \"tests compliance unknown\"] + list(df.columns[start_col:])\n",
    "columns_of_interest = list(df.columns[start_col:])\n",
    "\n",
    "data, sums = collect_and_sum_benchmark_metrics(benchmarks, evalsDir, columns_of_interest)\n",
    "print_sums_table(sums, columns_of_interest)\n",
    "plot_sums_bar(sums, columns_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def average_tests_per_model(benchmarks, evalsDir):\n",
    "    averages = {}\n",
    "    for benchmark in benchmarks:\n",
    "        csv_path = os.path.join(evalsDir, benchmark, benchmark, \"overview.csv\")\n",
    "        if not os.path.isfile(csv_path):\n",
    "            print(f\"Warning: {csv_path} not found, skipping.\")\n",
    "            continue\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        # Parse the 'tests' column for all models\n",
    "        tests = df['tests'].apply(parse_metric)\n",
    "        if len(tests) > 0:\n",
    "            avg = np.mean(tests)\n",
    "        else:\n",
    "            avg = 0.0\n",
    "        averages[benchmark] = avg\n",
    "    return averages\n",
    "\n",
    "def print_avg_table(averages):\n",
    "    print(\"Benchmark\\tAverage Tests per Model\")\n",
    "    for bench, avg in averages.items():\n",
    "        print(f\"{bench}\\t{avg:.2f}\")\n",
    "\n",
    "def plot_avg_bar(averages):\n",
    "    benchmarks = list(averages.keys())\n",
    "    values = list(averages.values())\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(benchmarks, values)\n",
    "    plt.ylabel(\"Average Tests per Model\")\n",
    "    plt.title(\"Average Tests per Model by Benchmark\")\n",
    "    plt.xticks(rotation=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "averages = average_tests_per_model(benchmarks, evalsDir)\n",
    "print_avg_table(averages)\n",
    "plot_avg_bar(averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grouped_barplot_by_benchmark_and_model(benchmarks, evalsDir, column_of_interest):\n",
    "    \"\"\"\n",
    "    Create a grouped barplot showing a specific column as a function of benchmark and model.\n",
    "    Groups are benchmarks, bars within groups are models.\n",
    "    Includes an additional \"Average\" group showing averages across benchmarks for each model.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Data structure: {benchmark: {model: value}}\n",
    "    data = {}\n",
    "    all_models = set()\n",
    "    \n",
    "    for benchmark in benchmarks:\n",
    "        csv_path = os.path.join(evalsDir, benchmark, benchmark, \"overview.csv\")\n",
    "        if not os.path.isfile(csv_path):\n",
    "            print(f\"Warning: {csv_path} not found, skipping.\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            df.columns = df.columns.str.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {csv_path}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if \"model\" not in df.columns:\n",
    "            print(f\"Warning: 'model' column not found in {csv_path}, skipping.\")\n",
    "            continue\n",
    "            \n",
    "        if column_of_interest not in df.columns:\n",
    "            print(f\"Warning: '{column_of_interest}' column not found in {csv_path}, skipping.\")\n",
    "            continue\n",
    "            \n",
    "        data[benchmark] = {}\n",
    "        for _, row in df.iterrows():\n",
    "            model = row[\"model\"]\n",
    "            val = parse_metric(row[column_of_interest])\n",
    "            data[benchmark][model] = val\n",
    "            all_models.add(model)\n",
    "    \n",
    "    all_models = sorted(all_models)\n",
    "    benchmarks_with_data = [b for b in benchmarks if b in data and data[b]]\n",
    "    \n",
    "    if not benchmarks_with_data:\n",
    "        print(f\"No valid data found for any benchmarks for column '{column_of_interest}'.\")\n",
    "        return\n",
    "        \n",
    "    if not all_models:\n",
    "        print(f\"No models found across all benchmarks for column '{column_of_interest}'.\")\n",
    "        return\n",
    "\n",
    "    # Build data matrix: rows=benchmarks, columns=models\n",
    "    values = []\n",
    "    for benchmark in benchmarks_with_data:\n",
    "        row = []\n",
    "        for model in all_models:\n",
    "            row.append(data.get(benchmark, {}).get(model, 0.0))\n",
    "        values.append(row)\n",
    "    values = np.array(values)  # shape: (num_benchmarks, num_models)\n",
    "    \n",
    "    # Check if we have any non-zero values\n",
    "    if np.all(values == 0):\n",
    "        print(f\"All values are zero for column '{column_of_interest}'. No meaningful plot can be created.\")\n",
    "        return\n",
    "\n",
    "    # Calculate averages and standard deviations across benchmarks for each model\n",
    "    model_averages = []\n",
    "    model_std_devs = []\n",
    "    for i, model in enumerate(all_models):\n",
    "        # Get values for this model across all benchmarks\n",
    "        model_values = values[:, i]\n",
    "\n",
    "\n",
    "        # Include ALL values (including zeros) for statistics\n",
    "        avg = np.mean(model_values)\n",
    "        std = np.std(model_values, ddof=1) if len(model_values) > 1 else 0.0\n",
    "        model_averages.append(avg)\n",
    "        model_std_devs.append(std)\n",
    "    \n",
    "    # Add the average row to the data\n",
    "    all_values = np.vstack([values, model_averages])\n",
    "    all_labels = benchmarks_with_data + [\"Average\"]\n",
    "    \n",
    "    # Create the grouped bar plot\n",
    "    x = np.arange(len(all_labels))  # positions for all groups including average\n",
    "    width = 0.8 / len(all_models)  # width of individual bars\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(18, 8))\n",
    "    \n",
    "    # Create bars for each model\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(all_models)))  # Use distinct colors\n",
    "    for i, model in enumerate(all_models):\n",
    "        offset = (i - (len(all_models) - 1) / 2) * width\n",
    "        bars = ax.bar(x + offset, all_values[:, i], width, label=model, alpha=0.8, color=colors[i])\n",
    "        \n",
    "        # Add error bars only to the average (last) bar\n",
    "        if len(bars) > len(benchmarks_with_data):\n",
    "            # Highlight the average bars with different styling\n",
    "            bars[-1].set_alpha(1.0)  # Make average bar more opaque\n",
    "            bars[-1].set_edgecolor('black')  # Add black border to average bar\n",
    "            bars[-1].set_linewidth(2)\n",
    "            \n",
    "            # Add error bar to the average bar\n",
    "            avg_x_pos = x[-1] + offset\n",
    "            avg_height = model_averages[i]\n",
    "            error_bar = model_std_devs[i]\n",
    "            \n",
    "            if error_bar > 0:  # Only add error bar if we have variation\n",
    "                ax.errorbar(avg_x_pos, avg_height, yerr=error_bar, \n",
    "                           fmt='none', color='black', capsize=5, capthick=2, alpha=0.8)\n",
    "    \n",
    "    # Add a vertical separator line before the average group\n",
    "    if len(all_labels) > 1:\n",
    "        separator_x = len(benchmarks_with_data) - 0.5\n",
    "        ax.axvline(x=separator_x, color='gray', linestyle='--', alpha=0.7, linewidth=1)\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_xlabel('Benchmark')\n",
    "    ax.set_ylabel(column_of_interest)\n",
    "    ax.set_title(f'{column_of_interest} by Benchmark and Model (with Cross-Benchmark Averages ± SD)')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(all_labels, rotation=45, ha='right')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add text annotation for the average section\n",
    "    if len(all_labels) > 1:\n",
    "        ax.text(len(benchmarks_with_data), ax.get_ylim()[1] * 0.95, 'Cross-Benchmark\\nAverages ± SD', \n",
    "                ha='center', va='top', fontsize=10, style='italic', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics including standard deviations\n",
    "    print(f\"\\nModel Statistics for {column_of_interest} (including zeros):\")\n",
    "    print(\"=\" * 60)\n",
    "    for i, model in enumerate(all_models):\n",
    "        print(f\"{model}: {model_averages[i]:.2f} ± {model_std_devs[i]:.2f}\")\n",
    "\n",
    "# Example usage with a specific column\n",
    "# You can change this to any column that exists in your data\n",
    "column_to_plot = \"tests compliant\"\n",
    "\n",
    "# First check what columns are available in the first benchmark\n",
    "if benchmarks:  # Check if benchmarks list is not empty\n",
    "    csv_path = os.path.join(evalsDir, benchmarks[0], benchmarks[0], \"overview.csv\")\n",
    "    if os.path.isfile(csv_path):\n",
    "        try:\n",
    "            df_sample = pd.read_csv(csv_path)\n",
    "            df_sample.columns = df_sample.columns.str.strip()\n",
    "            print(\"Available columns:\")\n",
    "            print(df_sample.columns.tolist())\n",
    "            print()\n",
    "            \n",
    "            # Try to find a good column to plot\n",
    "            if \"tests compliant\" in df_sample.columns:\n",
    "                column_to_plot = \"tests compliant\"\n",
    "            elif \"accuracy with eval\" in df_sample.columns:\n",
    "                column_to_plot = \"accuracy with eval\"\n",
    "            else:\n",
    "                # Use the first metric column after standard columns\n",
    "                start_col = get_metrics_start_col(df_sample)\n",
    "                \n",
    "                if start_col < len(df_sample.columns):\n",
    "                    column_to_plot = df_sample.columns[start_col]\n",
    "                else:\n",
    "                    print(\"No metrics columns found, using 'tests compliant' as default\")\n",
    "                    column_to_plot = \"tests compliant\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading sample CSV: {e}\")\n",
    "            print(\"Using default column: tests compliant\")\n",
    "    else:\n",
    "        print(f\"Sample CSV not found: {csv_path}\")\n",
    "        print(\"Using default column: tests compliant\")\n",
    "else:\n",
    "    print(\"No benchmarks specified!\")\n",
    "    print(\"Using default column: tests compliant\")\n",
    "\n",
    "print(f\"Plotting column: {column_to_plot}\")\n",
    "plot_grouped_barplot_by_benchmark_and_model(benchmarks, evalsDir, column_to_plot)\n",
    "\n",
    "column_to_plot = \"accuracy with azure:o4-mini_2025-04-16\"\n",
    "print(f\"Plotting column: {column_to_plot}\")\n",
    "plot_grouped_barplot_by_benchmark_and_model(benchmarks, evalsDir, column_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grouped_barplot_by_benchmark_and_model_baseline(benchmarks, evalsDir, column_of_interest):\n",
    "    \"\"\"\n",
    "    Create a grouped barplot showing a specific column as a function of benchmark and model for BASELINE tests.\n",
    "    Groups are benchmarks, bars within groups are models.\n",
    "    Includes an additional \"Average\" group showing averages across benchmarks for each model.\n",
    "    This function reads from overview-baseline.csv files instead of overview.csv files.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Data structure: {benchmark: {model: value}}\n",
    "    data = {}\n",
    "    all_models = set()\n",
    "    \n",
    "    for benchmark in benchmarks:\n",
    "        csv_path = os.path.join(evalsDir, benchmark, benchmark, \"overview-baseline.csv\")\n",
    "        if not os.path.isfile(csv_path):\n",
    "            print(f\"Warning: {csv_path} not found, skipping.\")\n",
    "            continue\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        \n",
    "        if \"model\" not in df.columns or column_of_interest not in df.columns:\n",
    "            print(f\"Warning: Required columns not found in {csv_path}, skipping.\")\n",
    "            continue\n",
    "            \n",
    "        data[benchmark] = {}\n",
    "        for _, row in df.iterrows():\n",
    "            model = row[\"model\"]\n",
    "            val = parse_metric(row[column_of_interest])\n",
    "            data[benchmark][model] = val\n",
    "            all_models.add(model)\n",
    "    \n",
    "    all_models = sorted(all_models)\n",
    "    benchmarks_with_data = [b for b in benchmarks if b in data]\n",
    "    \n",
    "    if not benchmarks_with_data or not all_models:\n",
    "        print(\"No baseline data found for plotting.\")\n",
    "        return\n",
    "    \n",
    "    # Build data matrix: rows=benchmarks, columns=models\n",
    "    values = []\n",
    "    for benchmark in benchmarks_with_data:\n",
    "        row = []\n",
    "        for model in all_models:\n",
    "            row.append(data.get(benchmark, {}).get(model, 0.0))\n",
    "        values.append(row)\n",
    "    values = np.array(values)  # shape: (num_benchmarks, num_models)\n",
    "    \n",
    "    # Calculate averages and standard deviations across benchmarks for each model\n",
    "    model_averages = []\n",
    "    model_std_devs = []\n",
    "    for i, model in enumerate(all_models):\n",
    "        # Get values for this model across all benchmarks\n",
    "        model_values = values[:, i]\n",
    "        # Only consider non-zero values for statistics\n",
    "        non_zero_values = model_values[model_values > 0]\n",
    "        if len(non_zero_values) > 0:\n",
    "            avg = np.mean(non_zero_values)\n",
    "            std = np.std(non_zero_values, ddof=1) if len(non_zero_values) > 1 else 0.0\n",
    "        else:\n",
    "            avg = 0.0\n",
    "            std = 0.0\n",
    "        model_averages.append(avg)\n",
    "        model_std_devs.append(std)\n",
    "    \n",
    "    # Add the average row to the data\n",
    "    all_values = np.vstack([values, model_averages])\n",
    "    all_labels = benchmarks_with_data + [\"Average\"]\n",
    "    \n",
    "    # Create the grouped bar plot\n",
    "    x = np.arange(len(all_labels))  # positions for all groups including average\n",
    "    width = 0.8 / len(all_models)  # width of individual bars\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(18, 8))\n",
    "    \n",
    "    # Create bars for each model with baseline-specific colors (cooler tones)\n",
    "    colors = plt.cm.Blues(np.linspace(0.3, 0.9, len(all_models)))  # Use blue tones for baseline\n",
    "    for i, model in enumerate(all_models):\n",
    "        offset = (i - (len(all_models) - 1) / 2) * width\n",
    "        bars = ax.bar(x + offset, all_values[:, i], width, label=model, alpha=0.8, color=colors[i])\n",
    "        \n",
    "        # Add error bars only to the average (last) bar\n",
    "        if len(bars) > len(benchmarks_with_data):\n",
    "            # Highlight the average bars with different styling\n",
    "            bars[-1].set_alpha(1.0)  # Make average bar more opaque\n",
    "            bars[-1].set_edgecolor('black')  # Add black border to average bar\n",
    "            bars[-1].set_linewidth(2)\n",
    "            \n",
    "            # Add error bar to the average bar\n",
    "            avg_x_pos = x[-1] + offset\n",
    "            avg_height = model_averages[i]\n",
    "            error_bar = model_std_devs[i]\n",
    "            \n",
    "            if error_bar > 0:  # Only add error bar if we have variation\n",
    "                ax.errorbar(avg_x_pos, avg_height, yerr=error_bar, \n",
    "                           fmt='none', color='black', capsize=5, capthick=2, alpha=0.8)\n",
    "    \n",
    "    # Add a vertical separator line before the average group\n",
    "    if len(all_labels) > 1:\n",
    "        separator_x = len(benchmarks_with_data) - 0.5\n",
    "        ax.axvline(x=separator_x, color='gray', linestyle='--', alpha=0.7, linewidth=1)\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_xlabel('Benchmark')\n",
    "    ax.set_ylabel(column_of_interest)\n",
    "    ax.set_title(f'{column_of_interest} by Benchmark and Model - BASELINE Results (with Cross-Benchmark Averages ± SD)')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(all_labels, rotation=45, ha='right')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add text annotation for the average section\n",
    "    if len(all_labels) > 1:\n",
    "        ax.text(len(benchmarks_with_data), ax.get_ylim()[1] * 0.95, 'Cross-Benchmark\\nBaseline Averages ± SD', \n",
    "                ha='center', va='top', fontsize=10, style='italic', alpha=0.7)\n",
    "    \n",
    "    # Add watermark indicating this is baseline data\n",
    "    ax.text(0.02, 0.98, 'BASELINE DATA', transform=ax.transAxes, fontsize=14, \n",
    "            color='navy', alpha=0.7, weight='bold', va='top', ha='left',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='lightblue', alpha=0.3))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics including standard deviations\n",
    "    print(f\"\\nBaseline Model Statistics for {column_of_interest}:\")\n",
    "    print(\"=\" * 60)\n",
    "    for i, model in enumerate(all_models):\n",
    "        print(f\"{model}: {model_averages[i]:.2f} ± {model_std_devs[i]:.2f}\")\n",
    "\n",
    "# Run the baseline analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"BASELINE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check what columns are available in the first baseline file\n",
    "csv_path = os.path.join(evalsDir, benchmarks[0], benchmarks[0], \"overview-baseline.csv\")\n",
    "if os.path.isfile(csv_path):\n",
    "    df_sample = pd.read_csv(csv_path)\n",
    "    df_sample.columns = df_sample.columns.str.strip()\n",
    "    print(\"Available columns in baseline data:\")\n",
    "    print(df_sample.columns.tolist())\n",
    "    print()\n",
    "    \n",
    "    # Plot baseline results for tests compliant\n",
    "    if \"tests compliant\" in df_sample.columns:\n",
    "        column_to_plot = \"tests compliant\"\n",
    "        print(f\"Plotting baseline column: {column_to_plot}\")\n",
    "        plot_grouped_barplot_by_benchmark_and_model_baseline(benchmarks, evalsDir, column_to_plot)\n",
    "    \n",
    "    # Plot baseline results for accuracy with eval\n",
    "    if \"accuracy with eval\" in df_sample.columns:\n",
    "        column_to_plot = \"accuracy with azure:o4-mini_2025-04-16\"\n",
    "        print(f\"Plotting baseline column: {column_to_plot}\")\n",
    "        plot_grouped_barplot_by_benchmark_and_model_baseline(benchmarks, evalsDir, column_to_plot)\n",
    "else:\n",
    "    print(f\"Warning: No baseline file found at {csv_path}\")\n",
    "    print(\"Available baseline files:\")\n",
    "    for benchmark in benchmarks:\n",
    "        baseline_path = os.path.join(evalsDir, benchmark, benchmark, \"overview-baseline.csv\")\n",
    "        if os.path.isfile(baseline_path):\n",
    "            print(f\"  Found: {baseline_path}\")\n",
    "        else:\n",
    "            print(f\"  Missing: {baseline_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_baseline_vs_main_metrics(benchmark, evalsDir, prettyBenchmarkNames=None):\n",
    "    \"\"\"\n",
    "    Compare metrics between overview.csv and overview-baseline.csv files for a given benchmark.\n",
    "    Creates side-by-side bar charts and difference analysis.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Build paths to both CSV files\n",
    "    main_csv_path = os.path.join(evalsDir, benchmark, benchmark, \"overview.csv\")\n",
    "    baseline_csv_path = os.path.join(evalsDir, benchmark, benchmark, \"overview-baseline.csv\")\n",
    "    \n",
    "    # Check if both files exist\n",
    "    if not os.path.isfile(main_csv_path):\n",
    "        print(f\"Warning: Main file {main_csv_path} not found, skipping.\")\n",
    "        return\n",
    "    if not os.path.isfile(baseline_csv_path):\n",
    "        print(f\"Warning: Baseline file {baseline_csv_path} not found, skipping.\")\n",
    "        return\n",
    "    \n",
    "    # Read both CSV files\n",
    "    main_df = pd.read_csv(main_csv_path)\n",
    "    baseline_df = pd.read_csv(baseline_csv_path)\n",
    "    \n",
    "    # Strip whitespace from column names\n",
    "    main_df.columns = main_df.columns.str.strip()\n",
    "    baseline_df.columns = baseline_df.columns.str.strip()\n",
    "    \n",
    "    # Find common models\n",
    "    main_models = set(main_df['model'])\n",
    "    baseline_models = set(baseline_df['model'])\n",
    "    common_models = sorted(main_models.intersection(baseline_models))\n",
    "    \n",
    "    if not common_models:\n",
    "        print(f\"No common models found between main and baseline for benchmark '{benchmark}'\")\n",
    "        return\n",
    "    \n",
    "    # Find common metric columns (excluding 'model' and other non-metric columns)\n",
    "    main_start_col = get_metrics_start_col(main_df)\n",
    "    baseline_start_col = get_metrics_start_col(baseline_df)\n",
    "    \n",
    "    main_metrics = set(main_df.columns[main_start_col:])\n",
    "    baseline_metrics = set(baseline_df.columns[baseline_start_col:])\n",
    "    common_metrics = sorted(main_metrics.intersection(baseline_metrics))\n",
    "    \n",
    "    if not common_metrics:\n",
    "        print(f\"No common metrics found between main and baseline for benchmark '{benchmark}'\")\n",
    "        print(f\"Main metrics: {list(main_metrics)}\")\n",
    "        print(f\"Baseline metrics: {list(baseline_metrics)}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Comparing benchmark '{benchmark}' with {len(common_models)} models and {len(common_metrics)} metrics\")\n",
    "    print(f\"Common models: {common_models}\")\n",
    "    print(f\"Common metrics: {common_metrics}\")\n",
    "    \n",
    "    # Prepare data for comparison\n",
    "    comparison_data = {}\n",
    "    for model in common_models:\n",
    "        main_row = main_df[main_df['model'] == model].iloc[0] if len(main_df[main_df['model'] == model]) > 0 else None\n",
    "        baseline_row = baseline_df[baseline_df['model'] == model].iloc[0] if len(baseline_df[baseline_df['model'] == model]) > 0 else None\n",
    "        \n",
    "        if main_row is not None and baseline_row is not None:\n",
    "            comparison_data[model] = {\n",
    "                'main': {metric: parse_metric(main_row[metric]) for metric in common_metrics},\n",
    "                'baseline': {metric: parse_metric(baseline_row[metric]) for metric in common_metrics}\n",
    "            }\n",
    "    \n",
    "    # Create side-by-side comparison plots\n",
    "    n_metrics = len(common_metrics)\n",
    "    n_models = len(comparison_data)\n",
    "    \n",
    "    if n_metrics == 0 or n_models == 0:\n",
    "        print(\"No data to plot\")\n",
    "        return\n",
    "    \n",
    "    # Create subplots for each metric\n",
    "    fig, axes = plt.subplots(n_metrics, 1, figsize=(12, 4 * n_metrics))\n",
    "    if n_metrics == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, metric in enumerate(common_metrics):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        models = list(comparison_data.keys())\n",
    "        main_values = [comparison_data[model]['main'][metric] for model in models]\n",
    "        baseline_values = [comparison_data[model]['baseline'][metric] for model in models]\n",
    "        \n",
    "        x = np.arange(len(models))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax.bar(x - width/2, baseline_values, width, label='Baseline', alpha=0.8, color='lightcoral')\n",
    "        bars2 = ax.bar(x + width/2, main_values, width, label='Main', alpha=0.8, color='skyblue')\n",
    "        \n",
    "        ax.set_xlabel('Model')\n",
    "        ax.set_ylabel(metric)\n",
    "        ax.set_title(f'{metric} - Baseline vs Main ({prettyBenchmarkNames.get(benchmark, benchmark) if prettyBenchmarkNames else benchmark})')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars1:\n",
    "            height = bar.get_height()\n",
    "            ax.annotate(f'{height:.1f}',\n",
    "                       xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                       xytext=(0, 3),  # 3 points vertical offset\n",
    "                       textcoords=\"offset points\",\n",
    "                       ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        for bar in bars2:\n",
    "            height = bar.get_height()\n",
    "            ax.annotate(f'{height:.1f}',\n",
    "                       xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                       xytext=(0, 3),  # 3 points vertical offset\n",
    "                       textcoords=\"offset points\",\n",
    "                       ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create difference analysis plot\n",
    "    fig, axes = plt.subplots(n_metrics, 1, figsize=(12, 4 * n_metrics))\n",
    "    if n_metrics == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, metric in enumerate(common_metrics):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        models = list(comparison_data.keys())\n",
    "        differences = [comparison_data[model]['main'][metric] - comparison_data[model]['baseline'][metric] for model in models]\n",
    "        \n",
    "        # Color bars based on positive/negative change\n",
    "        colors = ['green' if diff > 0 else 'red' if diff < 0 else 'gray' for diff in differences]\n",
    "        \n",
    "        bars = ax.bar(models, differences, color=colors, alpha=0.7)\n",
    "        \n",
    "        ax.set_xlabel('Model')\n",
    "        ax.set_ylabel(f'{metric} Difference (Main - Baseline)')\n",
    "        ax.set_title(f'{metric} - Improvement Analysis ({prettyBenchmarkNames.get(benchmark, benchmark) if prettyBenchmarkNames else benchmark})')\n",
    "        ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "        ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, diff in zip(bars, differences):\n",
    "            height = bar.get_height()\n",
    "            ax.annotate(f'{diff:+.1f}',\n",
    "                       xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                       xytext=(0, 3 if height >= 0 else -15),\n",
    "                       textcoords=\"offset points\",\n",
    "                       ha='center', va='bottom' if height >= 0 else 'top', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary table\n",
    "    print(f\"\\nDetailed Comparison for '{benchmark}':\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    header = [\"Model\"] + [f\"{m} (B)\" for m in common_metrics] + [f\"{m} (M)\" for m in common_metrics] + [f\"{m} (Δ)\" for m in common_metrics]\n",
    "    print(\"\\t\".join([h[:10] for h in header]))\n",
    "    \n",
    "    for model in comparison_data:\n",
    "        row = [model[:10]]\n",
    "        # Baseline values\n",
    "        row.extend([f\"{comparison_data[model]['baseline'][metric]:.1f}\" for metric in common_metrics])\n",
    "        # Main values\n",
    "        row.extend([f\"{comparison_data[model]['main'][metric]:.1f}\" for metric in common_metrics])\n",
    "        # Differences\n",
    "        row.extend([f\"{comparison_data[model]['main'][metric] - comparison_data[model]['baseline'][metric]:+.1f}\" for metric in common_metrics])\n",
    "        print(\"\\t\".join(row))\n",
    "\n",
    "def analyze_all_baseline_comparisons(benchmarks, evalsDir, prettyBenchmarkNames=None):\n",
    "    \"\"\"\n",
    "    Run baseline vs main comparison for all benchmarks that have both files.\n",
    "    \"\"\"\n",
    "    for benchmark in benchmarks:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Analyzing baseline comparison for: {benchmark}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        try:\n",
    "            compare_baseline_vs_main_metrics(benchmark, evalsDir, prettyBenchmarkNames)\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing {benchmark}: {str(e)}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the baseline comparison with art-prompt benchmark\n",
    "#print(\"Testing baseline comparison analysis with art-prompt benchmark:\")\n",
    "#compare_baseline_vs_main_metrics(\"art-prompt\", evalsDir, prettyBenchmarkNames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze_all_baseline_comparisons(benchmarks, evalsDir, prettyBenchmarkNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_baseline_vs_main_compliance_by_benchmark(benchmarks, evalsDir, metric_column=\"tests compliant\"):\n",
    "    \"\"\"\n",
    "    Create a grouped bar plot showing % tests non-compliant for baseline vs main across benchmarks.\n",
    "    \n",
    "    Args:\n",
    "        benchmarks: List of benchmark names\n",
    "        evalsDir: Directory containing evaluation results\n",
    "        metric_column: Column name to plot (default: \"tests compliant\")\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Collect data for each benchmark\n",
    "    benchmark_data = {}\n",
    "    all_main_values = []  # For calculating overall statistics\n",
    "    all_baseline_values = []\n",
    "    all_inverse_values = []\n",
    "    \n",
    "    for benchmark in benchmarks:\n",
    "        main_csv_path = os.path.join(evalsDir, benchmark, benchmark, \"overview.csv\")\n",
    "        baseline_csv_path = os.path.join(evalsDir, benchmark, benchmark, \"overview-baseline.csv\")\n",
    "        \n",
    "        # Skip if either file doesn't exist\n",
    "        if not (os.path.isfile(main_csv_path) and os.path.isfile(baseline_csv_path)):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Read both CSV files\n",
    "            main_df = pd.read_csv(main_csv_path)\n",
    "            baseline_df = pd.read_csv(baseline_csv_path)\n",
    "            \n",
    "            # Strip whitespace from column names\n",
    "            main_df.columns = main_df.columns.str.strip()\n",
    "            baseline_df.columns = baseline_df.columns.str.strip()\n",
    "            \n",
    "            # Check if the metric column exists in both files\n",
    "            if metric_column not in main_df.columns or metric_column not in baseline_df.columns:\n",
    "                print(f\"Warning: '{metric_column}' not found in {benchmark}, skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate average compliance for each dataset, then convert to non-compliance\n",
    "            main_values = [parse_metric(val) for val in main_df[metric_column]]\n",
    "            baseline_values = [parse_metric(val) for val in baseline_df[metric_column]]\n",
    "            \n",
    "            # Get inverse values (tests negative compliant) if available\n",
    "            inverse_compliance_percentages = []\n",
    "            if \"tests negative compliant\" in main_df.columns and \"tests negative\" in main_df.columns:\n",
    "                for _, row in main_df.iterrows():\n",
    "                    neg_compliant = parse_metric(row[\"tests negative compliant\"])\n",
    "                    neg_total = parse_metric(row[\"tests negative\"])\n",
    "                    if neg_total > 0:\n",
    "                        compliance_pct = (neg_compliant / neg_total) * 100.0\n",
    "                    else:\n",
    "                        compliance_pct = 0.0\n",
    "                    inverse_compliance_percentages.append(compliance_pct)\n",
    "            \n",
    "            # Convert compliance to non-compliance (100% - compliance%)\n",
    "            main_non_compliance = [100.0 - v for v in main_values]\n",
    "            baseline_non_compliance = [100.0 - v for v in baseline_values]\n",
    "            inverse_non_compliance = [100.0 - v for v in inverse_compliance_percentages] if inverse_compliance_percentages else []\n",
    "            \n",
    "            # Remove zero/invalid values for average calculation\n",
    "            main_valid = [v for v in main_non_compliance ]\n",
    "            baseline_valid = [v for v in baseline_non_compliance ]\n",
    "            inverse_valid = [v for v in inverse_non_compliance ] if inverse_non_compliance else []\n",
    "            \n",
    "            if main_valid and baseline_valid:\n",
    "                main_avg = np.mean(main_valid)\n",
    "                baseline_avg = np.mean(baseline_valid)\n",
    "                main_std = np.std(main_valid, ddof=1) if len(main_valid) > 1 else 0.0\n",
    "                baseline_std = np.std(baseline_valid, ddof=1) if len(baseline_valid) > 1 else 0.0\n",
    "                \n",
    "                # Calculate inverse stats if available\n",
    "                inverse_avg = np.mean(inverse_valid) if inverse_valid else 0.0\n",
    "                inverse_std = np.std(inverse_valid, ddof=1) if len(inverse_valid) > 1 else 0.0\n",
    "                \n",
    "                benchmark_data[benchmark] = {\n",
    "                    'main': main_avg,\n",
    "                    'baseline': baseline_avg,\n",
    "                    'inverse': inverse_avg,\n",
    "                    'main_std': main_std,\n",
    "                    'baseline_std': baseline_std,\n",
    "                    'inverse_std': inverse_std,\n",
    "                    'main_count': len(main_valid),\n",
    "                    'baseline_count': len(baseline_valid),\n",
    "                    'inverse_count': len(inverse_valid)\n",
    "                }\n",
    "                \n",
    "                # Store individual values for overall statistics\n",
    "                all_main_values.extend(main_valid)\n",
    "                all_baseline_values.extend(baseline_valid)\n",
    "                if inverse_valid:\n",
    "                    all_inverse_values.extend(inverse_valid)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not benchmark_data:\n",
    "        print(\"No benchmark data found for comparison.\")\n",
    "        return\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    benchmark_names = list(benchmark_data.keys())\n",
    "    main_values = [benchmark_data[b]['main'] for b in benchmark_names]\n",
    "    baseline_values = [benchmark_data[b]['baseline'] for b in benchmark_names]\n",
    "    inverse_values = [benchmark_data[b]['inverse'] for b in benchmark_names]\n",
    "    main_errors = [benchmark_data[b]['main_std'] for b in benchmark_names]\n",
    "    baseline_errors = [benchmark_data[b]['baseline_std'] for b in benchmark_names]\n",
    "    inverse_errors = [benchmark_data[b]['inverse_std'] for b in benchmark_names]\n",
    "    \n",
    "    # Calculate overall averages and standard deviations\n",
    "    overall_main_avg = np.mean(all_main_values)\n",
    "    overall_baseline_avg = np.mean(all_baseline_values)\n",
    "    overall_inverse_avg = np.mean(all_inverse_values) if all_inverse_values else 0.0\n",
    "    overall_main_std = np.std(all_main_values, ddof=1) if len(all_main_values) > 1 else 0.0\n",
    "    overall_baseline_std = np.std(all_baseline_values, ddof=1) if len(all_baseline_values) > 1 else 0.0\n",
    "    overall_inverse_std = np.std(all_inverse_values, ddof=1) if len(all_inverse_values) > 1 else 0.0\n",
    "    \n",
    "    # Add averages to the data\n",
    "    all_benchmark_names = benchmark_names + [\"Average\"]\n",
    "    all_main_values_plot = main_values + [overall_main_avg]\n",
    "    all_baseline_values_plot = baseline_values + [overall_baseline_avg]\n",
    "    all_inverse_values_plot = inverse_values + [overall_inverse_avg]\n",
    "    all_main_errors_plot = main_errors + [overall_main_std]\n",
    "    all_baseline_errors_plot = baseline_errors + [overall_baseline_std]\n",
    "    all_inverse_errors_plot = inverse_errors + [overall_inverse_std]\n",
    "    \n",
    "    # Create the grouped bar plot with three bars\n",
    "    x = np.arange(len(all_benchmark_names))\n",
    "    width = 0.25  # Narrower bars for three groups\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(18, 8))\n",
    "    \n",
    "    # Create bars with error bars\n",
    "    bars1 = ax.bar(x - width, all_baseline_values_plot, width, label='Baseline', \n",
    "                   color='lightblue', alpha=0.8, edgecolor='navy', linewidth=0.5,\n",
    "                   yerr=all_baseline_errors_plot, capsize=4, error_kw={'capthick':2, 'alpha':0.8})\n",
    "    bars2 = ax.bar(x, all_main_values_plot, width, label='Promptpex', \n",
    "                   color='lightcoral', alpha=0.8, edgecolor='darkred', linewidth=0.5,\n",
    "                   yerr=all_main_errors_plot, capsize=4, error_kw={'capthick':2, 'alpha':0.8})\n",
    "    bars3 = ax.bar(x + width, all_inverse_values_plot, width, label='Promptpex Inverse', \n",
    "                   color='lightgreen', alpha=0.8, edgecolor='darkgreen', linewidth=0.5,\n",
    "                   yerr=all_inverse_errors_plot, capsize=4, error_kw={'capthick':2, 'alpha':0.8})\n",
    "    \n",
    "    # Highlight the average bars with different styling\n",
    "    if len(all_benchmark_names) > len(benchmark_names):\n",
    "        # Make average bars more prominent\n",
    "        bars1[-1].set_alpha(1.0)\n",
    "        bars1[-1].set_edgecolor('black')\n",
    "        bars1[-1].set_linewidth(2)\n",
    "        bars2[-1].set_alpha(1.0)\n",
    "        bars2[-1].set_edgecolor('black')\n",
    "        bars2[-1].set_linewidth(2)\n",
    "        bars3[-1].set_alpha(1.0)\n",
    "        bars3[-1].set_edgecolor('black')\n",
    "        bars3[-1].set_linewidth(2)\n",
    "    \n",
    "    # Add a vertical separator line before the average group\n",
    "    if len(all_benchmark_names) > 1:\n",
    "        separator_x = len(benchmark_names) - 0.5\n",
    "        ax.axvline(x=separator_x, color='gray', linestyle='--', alpha=0.7, linewidth=1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    def add_value_labels(bars, values, errors):\n",
    "        for bar, value, error in zip(bars, values, errors):\n",
    "            height = bar.get_height()\n",
    "            # Position label above error bar\n",
    "            label_height = height + error + 1.0\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., label_height,\n",
    "                   f'{value:.1f}%', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    add_value_labels(bars1, all_baseline_values_plot, all_baseline_errors_plot)\n",
    "    add_value_labels(bars2, all_main_values_plot, all_main_errors_plot)\n",
    "    add_value_labels(bars3, all_inverse_values_plot, all_inverse_errors_plot)\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_xlabel('Benchmark', fontsize=12)\n",
    "    ax.set_ylabel('Tests Non-compliance %', fontsize=12)\n",
    "    ax.set_title(f'{metric_column.title()} Non-Compliance - Baseline vs Promptpex vs Promptpex Inverse by Benchmark (with Overall Average ± SD)', fontsize=14, pad=20)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(all_benchmark_names, rotation=45, ha='right')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Set y-axis to accommodate error bars\n",
    "    max_val = max(max(np.array(all_baseline_values_plot) + np.array(all_baseline_errors_plot)), \n",
    "                  max(np.array(all_main_values_plot) + np.array(all_main_errors_plot)),\n",
    "                  max(np.array(all_inverse_values_plot) + np.array(all_inverse_errors_plot)))\n",
    "    ax.set_ylim(0, max_val * 1.15)\n",
    "    \n",
    "    # Add text annotation for the average section\n",
    "    if len(all_benchmark_names) > 1:\n",
    "        ax.text(len(benchmark_names), ax.get_ylim()[1] * 0.95, 'Overall\\nAverage ± SD', \n",
    "                ha='center', va='top', fontsize=10, style='italic', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics with standard deviations\n",
    "    print(f\"\\nSummary Statistics for {metric_column} Non-Compliance:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    improvement_main = overall_baseline_avg - overall_main_avg  # For non-compliance, lower is better\n",
    "    improvement_inverse = overall_baseline_avg - overall_inverse_avg\n",
    "    \n",
    "    print(f\"Overall Average Non-Compliance (Baseline): {overall_baseline_avg:.2f}% ± {overall_baseline_std:.2f}%\")\n",
    "    print(f\"Overall Average Non-Compliance (Promptpex): {overall_main_avg:.2f}% ± {overall_main_std:.2f}%\")\n",
    "    print(f\"Overall Average Non-Compliance (Promptpex Inverse): {overall_inverse_avg:.2f}% ± {overall_inverse_std:.2f}%\")\n",
    "    print(f\"Overall Improvement (Promptpex): {improvement_main:+.2f}% ({'improvement' if improvement_main > 0 else 'decline'})\")\n",
    "    print(f\"Overall Improvement (Promptpex Inverse): {improvement_inverse:+.2f}% ({'improvement' if improvement_inverse > 0 else 'decline'})\")\n",
    "    print(f\"Number of benchmarks: {len(benchmark_names)}\")\n",
    "    print(f\"Total data points: {len(all_baseline_values)} baseline, {len(all_main_values)} promptpex, {len(all_inverse_values)} inverse\")\n",
    "    \n",
    "    print(\"\\nPer-benchmark breakdown (Non-Compliance):\")\n",
    "    print(\"-\" * 70)\n",
    "    for benchmark in benchmark_names:\n",
    "        data = benchmark_data[benchmark]\n",
    "        diff_main = data['baseline'] - data['main']  # For non-compliance, lower is better\n",
    "        diff_inverse = data['baseline'] - data['inverse']\n",
    "        print(f\"{benchmark}:\")\n",
    "        print(f\"  Baseline Non-Compliance: {data['baseline']:.1f}% ± {data['baseline_std']:.1f}% (n={data['baseline_count']})\")\n",
    "        print(f\"  Promptpex Non-Compliance: {data['main']:.1f}% ± {data['main_std']:.1f}% (n={data['main_count']})\")\n",
    "        print(f\"  Promptpex Inverse Non-Compliance: {data['inverse']:.1f}% ± {data['inverse_std']:.1f}% (n={data['inverse_count']})\")\n",
    "        print(f\"  Improvement (Promptpex): {diff_main:+.1f}%\")\n",
    "        print(f\"  Improvement (Promptpex Inverse): {diff_inverse:+.1f}%\")\n",
    "        print()\n",
    "\n",
    "# Run the analysis\n",
    "print(\"Creating baseline vs main non-compliance comparison...\")\n",
    "plot_baseline_vs_main_compliance_by_benchmark(benchmarks, evalsDir, \"tests compliant\")\n",
    "plot_baseline_vs_main_compliance_by_benchmark(benchmarks, evalsDir, \"accuracy with azure:o4-mini_2025-04-16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_baseline_vs_main_compliance_by_benchmark_single_model(benchmarks, evalsDir, model_filter, metric_column=\"tests compliant\"):\n",
    "    \"\"\"\n",
    "    Create a grouped bar plot showing % tests non-compliant for baseline vs main across benchmarks for a specific model only.\n",
    "    \n",
    "    Args:\n",
    "        benchmarks: List of benchmark names\n",
    "        evalsDir: Directory containing evaluation results\n",
    "        model_filter: Specific model name to filter for (e.g., \"gpt-oss\")\n",
    "        metric_column: Column name to plot (default: \"tests compliant\")\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Collect data for each benchmark\n",
    "    benchmark_data = {}\n",
    "    all_main_values = []  # For calculating overall statistics\n",
    "    all_baseline_values = []\n",
    "    all_inverse_values = []\n",
    "    \n",
    "    for benchmark in benchmarks:\n",
    "        main_csv_path = os.path.join(evalsDir, benchmark, benchmark, \"overview.csv\")\n",
    "        baseline_csv_path = os.path.join(evalsDir, benchmark, benchmark, \"overview-baseline.csv\")\n",
    "        \n",
    "        # Skip if either file doesn't exist\n",
    "        if not (os.path.isfile(main_csv_path) and os.path.isfile(baseline_csv_path)):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Read both CSV files\n",
    "            main_df = pd.read_csv(main_csv_path)\n",
    "            baseline_df = pd.read_csv(baseline_csv_path)\n",
    "            \n",
    "            # Strip whitespace from column names\n",
    "            main_df.columns = main_df.columns.str.strip()\n",
    "            baseline_df.columns = baseline_df.columns.str.strip()\n",
    "            \n",
    "            # Filter for specific model\n",
    "            main_df_filtered = main_df[main_df['model'] == model_filter]\n",
    "            baseline_df_filtered = baseline_df[baseline_df['model'] == model_filter]\n",
    "            \n",
    "            # Check if the model exists in both datasets and metric column exists\n",
    "            if (len(main_df_filtered) == 0 or len(baseline_df_filtered) == 0 or \n",
    "                metric_column not in main_df.columns or metric_column not in baseline_df.columns):\n",
    "                print(f\"Warning: Model '{model_filter}' or column '{metric_column}' not found in {benchmark}, skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Get values for the specific model, then convert to non-compliance\n",
    "            main_values = [parse_metric(val) for val in main_df_filtered[metric_column]]\n",
    "            baseline_values = [parse_metric(val) for val in baseline_df_filtered[metric_column]]\n",
    "            \n",
    "            # Get inverse values (tests negative compliant) if available for the specific model\n",
    "            inverse_compliance_percentages = []\n",
    "            if \"tests negative compliant\" in main_df.columns and \"tests negative\" in main_df.columns:\n",
    "                for _, row in main_df_filtered.iterrows():\n",
    "                    neg_compliant = parse_metric(row[\"tests negative compliant\"])\n",
    "                    neg_total = parse_metric(row[\"tests negative\"])\n",
    "                    if neg_total > 0:\n",
    "                        compliance_pct = (neg_compliant / neg_total) * 100.0\n",
    "                    else:\n",
    "                        compliance_pct = 0.0\n",
    "                    inverse_compliance_percentages.append(compliance_pct)\n",
    "            \n",
    "            # Convert compliance to non-compliance (100% - compliance%)\n",
    "            main_non_compliance = [100.0 - v for v in main_values]\n",
    "            baseline_non_compliance = [100.0 - v for v in baseline_values]\n",
    "            inverse_non_compliance = [100.0 - v for v in inverse_compliance_percentages] if inverse_compliance_percentages else []\n",
    "            \n",
    "            # Remove zero/invalid values for average calculation (but keep them for completeness)\n",
    "            main_valid = [v for v in main_non_compliance]\n",
    "            baseline_valid = [v for v in baseline_non_compliance]\n",
    "            inverse_valid = [v for v in inverse_non_compliance] if inverse_non_compliance else []\n",
    "            \n",
    "            if main_valid and baseline_valid:\n",
    "                main_avg = np.mean(main_valid)\n",
    "                baseline_avg = np.mean(baseline_valid)\n",
    "                main_std = np.std(main_valid, ddof=1) if len(main_valid) > 1 else 0.0\n",
    "                baseline_std = np.std(baseline_valid, ddof=1) if len(baseline_valid) > 1 else 0.0\n",
    "                \n",
    "                # Calculate inverse stats if available\n",
    "                inverse_avg = np.mean(inverse_valid) if inverse_valid else 0.0\n",
    "                inverse_std = np.std(inverse_valid, ddof=1) if len(inverse_valid) > 1 else 0.0\n",
    "                \n",
    "                benchmark_data[benchmark] = {\n",
    "                    'main': main_avg,\n",
    "                    'baseline': baseline_avg,\n",
    "                    'inverse': inverse_avg,\n",
    "                    'main_std': main_std,\n",
    "                    'baseline_std': baseline_std,\n",
    "                    'inverse_std': inverse_std,\n",
    "                    'main_count': len(main_valid),\n",
    "                    'baseline_count': len(baseline_valid),\n",
    "                    'inverse_count': len(inverse_valid)\n",
    "                }\n",
    "                \n",
    "                # Store individual values for overall statistics\n",
    "                all_main_values.extend(main_valid)\n",
    "                all_baseline_values.extend(baseline_valid)\n",
    "                if inverse_valid:\n",
    "                    all_inverse_values.extend(inverse_valid)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {benchmark}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not benchmark_data:\n",
    "        print(f\"No benchmark data found for model '{model_filter}'.\")\n",
    "        return\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    benchmark_names = list(benchmark_data.keys())\n",
    "    main_values = [benchmark_data[b]['main'] for b in benchmark_names]\n",
    "    baseline_values = [benchmark_data[b]['baseline'] for b in benchmark_names]\n",
    "    inverse_values = [benchmark_data[b]['inverse'] for b in benchmark_names]\n",
    "    main_errors = [benchmark_data[b]['main_std'] for b in benchmark_names]\n",
    "    baseline_errors = [benchmark_data[b]['baseline_std'] for b in benchmark_names]\n",
    "    inverse_errors = [benchmark_data[b]['inverse_std'] for b in benchmark_names]\n",
    "    \n",
    "    # Calculate overall averages and standard deviations\n",
    "    overall_main_avg = np.mean(all_main_values)\n",
    "    overall_baseline_avg = np.mean(all_baseline_values)\n",
    "    overall_inverse_avg = np.mean(all_inverse_values) if all_inverse_values else 0.0\n",
    "    overall_main_std = np.std(all_main_values, ddof=1) if len(all_main_values) > 1 else 0.0\n",
    "    overall_baseline_std = np.std(all_baseline_values, ddof=1) if len(all_baseline_values) > 1 else 0.0\n",
    "    overall_inverse_std = np.std(all_inverse_values, ddof=1) if len(all_inverse_values) > 1 else 0.0\n",
    "    \n",
    "    # Add averages to the data\n",
    "    all_benchmark_names = benchmark_names + [\"Average\"]\n",
    "    all_main_values_plot = main_values + [overall_main_avg]\n",
    "    all_baseline_values_plot = baseline_values + [overall_baseline_avg]\n",
    "    all_inverse_values_plot = inverse_values + [overall_inverse_avg]\n",
    "    all_main_errors_plot = main_errors + [overall_main_std]\n",
    "    all_baseline_errors_plot = baseline_errors + [overall_baseline_std]\n",
    "    all_inverse_errors_plot = inverse_errors + [overall_inverse_std]\n",
    "    \n",
    "    # Create the grouped bar plot with three bars\n",
    "    x = np.arange(len(all_benchmark_names))\n",
    "    width = 0.25  # Narrower bars for three groups\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(18, 8))\n",
    "    \n",
    "    # Create bars with error bars - use different colors for single model view\n",
    "    bars1 = ax.bar(x - width, all_baseline_values_plot, width, label='Baseline', \n",
    "                   color='lightsteelblue', alpha=0.8, edgecolor='darkblue', linewidth=0.8,\n",
    "                   yerr=all_baseline_errors_plot, capsize=4, error_kw={'capthick':2, 'alpha':0.8})\n",
    "    bars2 = ax.bar(x, all_main_values_plot, width, label='Promptpex', \n",
    "                   color='orange', alpha=0.8, edgecolor='darkorange', linewidth=0.8,\n",
    "                   yerr=all_main_errors_plot, capsize=4, error_kw={'capthick':2, 'alpha':0.8})\n",
    "    bars3 = ax.bar(x + width, all_inverse_values_plot, width, label='Promptpex Inverse', \n",
    "                   color='lightgreen', alpha=0.8, edgecolor='darkgreen', linewidth=0.8,\n",
    "                   yerr=all_inverse_errors_plot, capsize=4, error_kw={'capthick':2, 'alpha':0.8})\n",
    "    \n",
    "    # Highlight the average bars with different styling\n",
    "    if len(all_benchmark_names) > len(benchmark_names):\n",
    "        # Make average bars more prominent\n",
    "        bars1[-1].set_alpha(1.0)\n",
    "        bars1[-1].set_edgecolor('black')\n",
    "        bars1[-1].set_linewidth(2)\n",
    "        bars2[-1].set_alpha(1.0)\n",
    "        bars2[-1].set_edgecolor('black')\n",
    "        bars2[-1].set_linewidth(2)\n",
    "        bars3[-1].set_alpha(1.0)\n",
    "        bars3[-1].set_edgecolor('black')\n",
    "        bars3[-1].set_linewidth(2)\n",
    "    \n",
    "    # Add a vertical separator line before the average group\n",
    "    if len(all_benchmark_names) > 1:\n",
    "        separator_x = len(benchmark_names) - 0.5\n",
    "        ax.axvline(x=separator_x, color='gray', linestyle='--', alpha=0.7, linewidth=1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    def add_value_labels(bars, values, errors):\n",
    "        for bar, value, error in zip(bars, values, errors):\n",
    "            height = bar.get_height()\n",
    "            # Position label above error bar\n",
    "            label_height = height + error + 1.0\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., label_height,\n",
    "                   f'{value:.1f}%', ha='center', va='bottom', fontsize=8, weight='bold')\n",
    "    \n",
    "    add_value_labels(bars1, all_baseline_values_plot, all_baseline_errors_plot)\n",
    "    add_value_labels(bars2, all_main_values_plot, all_main_errors_plot)\n",
    "    add_value_labels(bars3, all_inverse_values_plot, all_inverse_errors_plot)\n",
    "    \n",
    "    # Customize the plot\n",
    "    ax.set_xlabel('Benchmark', fontsize=12)\n",
    "    ax.set_ylabel('Tests Non-compliance %', fontsize=12)\n",
    "    ax.set_title(f'{metric_column.title()} Non-Compliance - Baseline vs Promptpex vs Promptpex Inverse for {model_filter} Model (with Overall Average ± SD)', \n",
    "                 fontsize=14, pad=20)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(all_benchmark_names, rotation=45, ha='right')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Set y-axis to accommodate error bars\n",
    "    max_val = max(max(np.array(all_baseline_values_plot) + np.array(all_baseline_errors_plot)), \n",
    "                  max(np.array(all_main_values_plot) + np.array(all_main_errors_plot)),\n",
    "                  max(np.array(all_inverse_values_plot) + np.array(all_inverse_errors_plot)))\n",
    "    ax.set_ylim(0, max_val * 1.15)\n",
    "    \n",
    "    # Add text annotation for the average section\n",
    "    if len(all_benchmark_names) > 1:\n",
    "        ax.text(len(benchmark_names), ax.get_ylim()[1] * 0.95, 'Overall\\nAverage ± SD', \n",
    "                ha='center', va='top', fontsize=10, style='italic', alpha=0.7)\n",
    "    \n",
    "    # Add model name annotation\n",
    "    ax.text(0.02, 0.98, f'Model: {model_filter}', transform=ax.transAxes, fontsize=12, \n",
    "            color='darkblue', alpha=0.8, weight='bold', va='top', ha='left',\n",
    "            bbox=dict(boxstyle='round,pad=0.4', facecolor='lightyellow', alpha=0.7))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics with standard deviations\n",
    "    print(f\"\\nSummary Statistics for {metric_column} Non-Compliance - {model_filter} Model:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    improvement_main = overall_baseline_avg - overall_main_avg  # For non-compliance, lower is better\n",
    "    improvement_inverse = overall_baseline_avg - overall_inverse_avg\n",
    "    \n",
    "    print(f\"Overall Average Non-Compliance (Baseline): {overall_baseline_avg:.2f}% ± {overall_baseline_std:.2f}%\")\n",
    "    print(f\"Overall Average Non-Compliance (Promptpex): {overall_main_avg:.2f}% ± {overall_main_std:.2f}%\")\n",
    "    print(f\"Overall Average Non-Compliance (Promptpex Inverse): {overall_inverse_avg:.2f}% ± {overall_inverse_std:.2f}%\")\n",
    "    print(f\"Overall Improvement (Promptpex): {improvement_main:+.2f}% ({'improvement' if improvement_main > 0 else 'decline'})\")\n",
    "    print(f\"Overall Improvement (Promptpex Inverse): {improvement_inverse:+.2f}% ({'improvement' if improvement_inverse > 0 else 'decline'})\")\n",
    "    print(f\"Number of benchmarks: {len(benchmark_names)}\")\n",
    "    print(f\"Total data points: {len(all_baseline_values)} baseline, {len(all_main_values)} promptpex, {len(all_inverse_values)} inverse\")\n",
    "    \n",
    "    print(f\"\\nPer-benchmark breakdown for {model_filter} (Non-Compliance):\")\n",
    "    print(\"-\" * 80)\n",
    "    for benchmark in benchmark_names:\n",
    "        data = benchmark_data[benchmark]\n",
    "        diff_main = data['baseline'] - data['main']  # For non-compliance, lower is better\n",
    "        diff_inverse = data['baseline'] - data['inverse']\n",
    "        print(f\"{benchmark}:\")\n",
    "        print(f\"  Baseline Non-Compliance: {data['baseline']:.1f}% ± {data['baseline_std']:.1f}% (n={data['baseline_count']})\")\n",
    "        print(f\"  Promptpex Non-Compliance: {data['main']:.1f}% ± {data['main_std']:.1f}% (n={data['main_count']})\")\n",
    "        print(f\"  Promptpex Inverse Non-Compliance: {data['inverse']:.1f}% ± {data['inverse_std']:.1f}% (n={data['inverse_count']})\")\n",
    "        print(f\"  Improvement (Promptpex): {diff_main:+.1f}%\")\n",
    "        print(f\"  Improvement (Promptpex Inverse): {diff_inverse:+.1f}%\")\n",
    "        print()\n",
    "\n",
    "# Run the analysis for gpt-oss model specifically\n",
    "print(\"Creating baseline vs promptpex non-compliance comparison for gpt-oss model only...\")\n",
    "plot_baseline_vs_main_compliance_by_benchmark_single_model(benchmarks, evalsDir, \"gpt-oss\", \"tests compliant\")\n",
    "plot_baseline_vs_main_compliance_by_benchmark_single_model(benchmarks, evalsDir, \"gpt-oss\", \"accuracy with azure:o4-mini_2025-04-16\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
